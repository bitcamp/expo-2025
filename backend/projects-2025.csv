Project Title,Submission Url,Project Status,Judging Status,Highest Step Completed,Project Created At,About The Project,"""Try it out"" Links",Video Demo Link,Opt-In Prizes,Built With,Submitter First Name,Submitter Last Name,Submitter Email,Notes,"What Is The Link To Your Code Repo(Git Hub, Repl.It, Etc)",Bitcamp Sponsored Prize Category #1,Bitcamp Sponsored Prize Category #2,Bitcamp Sponsored Prize Category #3,Alumni Team Prize Category #1,Alumni Team Prize Category #2,Tell Us About Your Technology Experience This Weekend.,"What School Do You Attend? If You Are No Longer In School, What University Did You Attend Most Recently?",Which Of The Following Gen Ai Tools Did You Use This Weekend?,Did You Implement A Generative Ai Model Or Api In Your Hack This Weekend?,"If You Are Submitting To The Best Use Of Gemini Api Prize Category, Please Provide Your Gemini Api Key Project Number.",List All Of The Domain Names Your Team Has Registered With .Tech During This Hackathon,Team Colleges/Universities,Additional Team Member Count,Team Member 1 First Name,Team Member 1 Last Name,Team Member 1 Email,...
QAOA for the kidney exchange problem,"",Draft,Pending,Additional info,04/11/2025 21:00:21,"Inspiration

What it does

The algorithm finds loops of compatible kidney donors and recipients, maximizing the number of transplants.

How we built it

Challenges we ran into

Our initial plan was to use Classiq, however we ran into issues when trying to create accounts.
We encountered obscure bugs with niche quantum software.
We had to do a lot of research for performance analysis on quantum algorithms

Accomplishments that we're proud of

Creating a working quantum algorithm

What we learned

We learned a lot about quantum optimization algorithms regarding applications, implementation and performance analysis.

What's next for QAOA for the kidney exchange problem
","",,Best Hack Promoting Public Health - Bloomberg,"python, pennylane, openqaoa, docplex",Zarif,Gudikal,zgudikal@gmail.com,,https://github.com/9t8/quantum-kidney,Best Advanced Quantum Track Hack,Best First-Time Hack,Best Social Good Hack,"","","Classiq requires manual verification of new accounts
Pennylane worked well",University of Maryland,I did not use GenAI,N/A,N/A,"","University of Maryland - College Park, Henry M. Gunn High School",1,David,Li,dragavor@gmail.com
Untitled,"",Draft,Pending,Manage team,04/11/2025 21:00:29,"","",,"","",nour662,Ali Ahmed,nourali1786@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,0
Keyless Campus,https://bitcamp2025.devpost.com/submissions/642003-keyless-campus,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 21:00:59,"Inspiration

I lost my dorm room keys last week. And I am for sure not the only one. At least 20% of all college students in the US lose their dorm keys each academic year and have to pay $100+ to replace their lock and the keys of their room/apartment mates.

What it does

It is a college dorm door unlocker system for each door where there is an RFID mounted on the outside and a knob unlocker on the other end. Beyond the hardware system, we have a web app that allows users to review and manage door access for each system.

How we built it

Hardware:
Web Application: We used React + Vite on the front end and Java Springboot MVC on the backend with a PostgreSQL database hosted on Render. The React client allows users to sign up a new RFID card into a door system and sign into the door-system-specific dashboard to view the door access history and manage access.

Challenges we ran into

Hardware-side:
Hacking up the embedded software to control the door opener motor and connect to the backend proved to be a non-trivial task. We had to figure out a few quirks and intricacies like circumventing ESP32 wifi band incompatibilities with the local network and setting up the right protocols for TCP communication between the scanner and unlocker chips. 

Software-side:
Setting up the proper database schema/architecture and fully comprehending the user requirements required a few back-and-forths and trial and error. Once we got that figured out, ensuring proper communication with the RFID chip sign-up request posed some trouble but we powered through it.

Demoing:
Since we clearly were not able to bring in a college door and neither a strong enough motor system for it anyway, we have to figure out the most convenient yet realistic way to demonstrate a proof of concept of the whole system working together.

Accomplishments that we're proud of

We are very proud of successfully setting up this hardware + full-stack project within the 36 hours of the hackathon while spending some time enjoying the weekend. 

What we learned

We learned a great deal about the different intricacies when it comes to building a hardware hackathon project as it is our very first hardware project! We got to apply a lot of concepts from class including TCP networks and protocols, database schemas, low-level/embedded systems programming and more!

What's next for Keyless Campus

Scaling Keyless Campus to all dorm doors for all US colleges is the next step!
",https://github.com/Andry-Arthur/KeylessCampus-bitcamp25,,[MLH] Best Use of .Tech,"c, esp32, rfid-rc522, postgresql, render, java, spring, mvc",Andry,Rakotonjanabelo,rakoan02@gettysburg.edu,,https://github.com/Andry-Arthur/KeylessCampus-bitcamp25,Best Hardware Hack,Best Bitcamp Hack,Best UI/UX Hack,Hack That Made You Smile,MOST LIT HACK,"C, ESP32 Chips, RFID Sensors, Java, SpringBoot, React",Gettysburg College,Anthropic,N/A,N/A,"nokeys.tech
andry.tech",Gettysburg College,3,Yashaswe,Amatya,amatyayasu@gmail.com,Prabesh,Bista,prabeshbista_2026@depauw.edu,Brayton,Alkinburgh,braytonalkinburgh@gmail.com
Eyes of Expression,https://bitcamp2025.devpost.com/submissions/642005-eyes-of-expression,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 21:01:27,"Inspiration

Art is one of the most fundamental forms of human expression. Through art we mirror the beauty of the world around us, engage in ages old cultural tradition, and express emotions that words just can't. Unfortunately, people with motor disabilities are often unable to pursue art on their own terms. This is what inspired us to make a tool to help make art more accessible for everyone, because everyone should be free to express themselves as they wish.

What it does

Eyes of Expression allows you to draw using just your eye movements. After a brief calibration phase, you can just look at where you want to move your cursor, and draw without any contact. You can change your pen size/color, toggle drawing mode, use an eraser, clear a drawing, and save past works.

How we built it

We made our website with HTML, CSS, and JavaScript. For checking eye movements, we implemented EyeGestures, a computer vision model built for gaze tracking. We uploaded our code to GitHub, and deployed our website with Vercel.

Challenges we ran into

We came in knowing next to nothing about web development, so it took us a lot of time to learn everything we needed to know about web development, along with how to implement an eye tracking algorithm, and how to deploy a website. We also struggled with improving the accuracy of the eye tracker, and sifted through a variety of models to find the best performing one. We were limited by our hardware, as we didn't have access to any good cameras, which severely hindered eye tracking accuracy.

Accomplishments that we're proud of

We're very proud of deploying an actual end product, despite starting off Bitcamp with no idea how to make a website. We're also proud of the role that Eyes of Expression can play as a proof of concept for what can be accomplished with better hardware.

What we learned

We learned a lot about all the various steps that go into front-end development, how to work with APIs, and about the world of using machine learning for gaze tracking.

What's next for Eyes of Expression

We hope to start off by running our product on better cameras to see how accurate the eye tracking can actually be. Another big step would be to change the user interface to be 100% contactless for maximum accessibility. We also want to implement a database where users can upload their drawings, and view what others users are creating!
",https://bitcamp2025.vercel.app/,,Best Hack Promoting Public Health - Bloomberg,"html, css, javascript, terminal, github, discord, visual-studio-code, camera, vercel",Daksh,Gautam,dakshgautam817@gmail.com,,https://github.com/dakg17/Bitcamp_Yippie,Best First-Time Hack,Best Razzle Dazzle Hack,Best Moonshot Hack,Prettiest Hack,Hack That Made You Smile,"We first started this project by brainstorming potential ideas. We learned more about front-end development with HTML, CSS, and JavaScript. We then spent the next day or so completing the front-end while also implementing the eye tracker. We used a lot of GitHub implementation to keep track of our work and keep a combined workspace for all three members. Overall, we really enjoyed our first hackathon and the process of making a project.",University of Maryland-College Park,OpenAI,N/A,N/A,"",University of Maryland - College Park,2,Zory,Teselko,zoryt8@gmail.com,Aditya,Lahiri,adityalahiri06@gmail.com
Dream Board,"",Draft,Pending,Project overview,04/11/2025 21:02:02,"","",,"","",Maya,Patel,mpatel53@terpmail.umd.edu,,"","","","","","","","","","","","",University of Maryland - College Park,0
Untitled,"",Draft,Pending,Manage team,04/11/2025 21:02:55,"","",,"","",V,S,venkateshshirbhate@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,0
Dino Docs,https://bitcamp2025.devpost.com/submissions/642009-dino-docs,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 21:04:16,"Inspiration

When learning new technologies, the most common advice is RTFM! However, most times the people who write the manual do not make the documentation the most accessible. To truly understand where you need to look, you have to read portions of the docs you are not interested in or even need to know. This is a big time waste and I find it quite frustrating. A common approach is to plug it into an LLM, but that can easily overwhelm their context window.

What it does

This application transforms your technical documentation into an interactive, AI-powered assistant. Simply upload your documentation as a PDF, and you’ll gain a natural-language interface to explore it. 
The system uses Retrieval-Augmented Generation (RAG) to break the docs into meaningful chunks, embed them into a vector database, and deliver fast, accurate, and context-aware responses from a powerful LLM.

You can continue prompting seamlessly — no need to re-upload — until you get the answers you need.

How we built it

We began with some basic starter code that implemented a rudimentary chunker and a simple RAG (Retrieval-Augmented Generation) setup. From there, we developed a lightweight backend using FastAPI, which allowed us to transition our work from a Colab notebook to a hosted server environment. During testing with various documents, we realized that our initial chunking method wasn’t capturing semantically meaningful groupings, which limited the model’s effectiveness. To improve this, we integrated Chonkie — a more sophisticated chunking tool that groups semantically related sentences together. This significantly enhanced the quality of our RAG pipeline, allowing the language model to generate more precise and contextually accurate responses. To add the finishing touches, we added the final flare of a simple, yet elegant react frontend.

Challenges we ran into

It was an unusual experience to work with a AI model,

Accomplishments that we're proud of

What we learned

What's next for Dino Docs

Hopefully adding the project, introducing better chunking, separate namespaces and more granular models for more accurate queries regardless of doc size, and maybe a bit more flare on the front end. Perhaps even a VS code plugin.
",https://github.com/IlyaShch/DinoDocs,,"Best Web Hack Using React - Peraton, [MLH] Best Use of Gemini API","python, pinecone, chonkie, gemini, react",Ilya,Shchelokov,ilya.shchelokov@gmail.com,,https://github.com/IlyaShch/DinoDocs,Best Machine Learning Track Hack,Best First-Time Hack,Best Social Good Hack,Hack That Made You Smile,Prettiest Hack,"We learned about pinecone DB and RAG databases. It was cool to get more specific results, from better targeted queries.",Montgomery College,Gemini,"Yes! Gemini was the LLM processing our contextualized queries! We gave it specified context based off user request, so it could better answer queries!",AIzaSyBE_zCFU-cHglZR9YleYsk4ui2XEE4Hi7Q,"","Montgomery College, University of Maryland - Baltimore County",1,Nishant,Singh,keiran.singh@gmail.com
Bitcamp Submission #1,"",Draft,Pending,Project overview,04/11/2025 21:04:23,"","",,"",api,Nathan,Zhao,nzhao@terpmail.umd.edu,,"","","","","","","","","","","","",University of Maryland - College Park,0
Unhookd,"",Draft,Pending,Project details,04/11/2025 21:04:52,"Inspiration

[addiction isn't bound by borders—it's a universal issue]

Addiction is a global public health crisis. Over 1.25 billion people use tobacco worldwide. Youth electronic cigarette use is skyrocketing, tripling in the UK and doubling in Canada in recent years. The World Health Organization warns that aggressive marketing is hooking children early.

Alcohol use is equally concerning. Nearly 50% of young adults (18–25) globally report drinking in the past month, and 29% binge drink, increasing risks of dependence, depression, and memory loss. 2.6 million alcohol-related deaths per year, stressing that no level of alcohol is ""safe.""

The takeaway? Traditional interventions aren't enough.

Unhookd is our mobile-first solution. It’s designed to meet people where they are—on their phones—with tools for self-tracking, daily check-ins, journaling, and progress reports. It’s discreet, easy to use, and backed by data.

We’re not here to replace therapy, we’re here to make recovery more accessible, especially for young people navigating addiction in silence. With Unhookd, they can take the first step anytime, anywhere.

How we built it

We began by designing the user interface in Figma, focusing on a clean, intuitive experience for users managing their recovery journey. The front end of the app is developed in Swift for iOS, while the back end uses Python to analyze data from user reflections and daily check-ins. This combination allows us to track progress over time and generate personalized insights that support habit change.

Challenges we ran into

We faced several challenges throughout development. Initially, designing a cohesive prototype that truly addressed user needs was difficult. Our early design pages felt clustered and required several iterations to streamline. Translating the prototype into working core functionality also proved complex, especially when ensuring the experience remained intuitive. Additionally, setting up the database to store user check-ins and reflection data securely and efficiently was a technical hurdle we had to overcome.

What we learned

We learned how to transform an abstract idea into a tangible solution through design, collaboration, and technical development. We also gained insight into user-centered design and how challenging—but rewarding—it is to build something that addresses a real human need.

What's next for Unhookd

Our vision for Unhookd goes beyond just tracking habits — we aim to build a secure, intelligent recovery companion. Future plans include encrypted data protection to safeguard user privacy, smart insights powered by machine learning to personalize the recovery journey, and integration with health apps and wearables for a holistic view of progress. We want Unhookd to feel empowering, motivational, and trustworthy — a tool that meets users where they are and grows with them.
","",,Best Hack Promoting Public Health - Bloomberg,"xcode, swift, gemini, python, figma",Telmen,Bayarbaatar,bayarakh@gmail.com,,"",Best App Dev Track Hack,Best First-Time Hack,Best Social Good Hack,Prettiest Hack,Hack That Made You Smile,"We initially attempted to integrate MongoDB and Firebase into our project, but ran into several issues. With MongoDB, the challenge was setting up the correct schemas and managing secure data transfer between the backend and the app in a short amount of time. Firebase was more intuitive, but we struggled with properly configuring authentication and real-time database updates, which slowed us down during development.","Montgomery College, University of Maryland: College Park",OpenAI and Anthropic,N/A,"","","Montgomery College, University of Maryland - College Park",3,Richard,Thomas,richuthomasp@gmail.com,Kristoffer,Velas,ksvelas123@gmail.com,Raushan,Oshan,raushanoshan@gmail.com
NeuraCoach,https://bitcamp2025.devpost.com/submissions/642012-neuracoach,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 21:05:28,"Inspiration

Being a student can be lot of work. A full course load for an undergraduate student is anywhere between 12 - 18 credit hours. Most professors expect at least 3 hours of work per week, per credit hour. That is up to 54 hours a week of work! It can be overwhelming, but NeuraCoach is here to help.

What it does

NeuraCoach is an interactive service that synthesizes a users list of tasks into a clear and concise task plan. This plan is generated to include the items a user needs to complete, as well as offer complementary tasks that can provide meaningful rests and buffers to the workload. Users are also able to log their mood and energy levels as they complete tasks, providing them with a valuable log, and the system with valuable data to adjust its suggestions to. 

How we built it

On the frontend, we utilized the Streamlit platform for user interaction. This allowed us to use Python to create our frontend. On the backend, we used Python scripts to interface with Google Gemini 2.0 for task generation. This way, the task generation can be tuned to the users needs, and current wellbeing. 
","",https://youtu.be/kK1_r896vfQ,"Best Hack Promoting Public Health - Bloomberg, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","python, streamlit, gemini, matplotlib, langchain",Will,Pressler,presslerwill@gmail.com,,https://github.com/ArsalaanK7/bitcamp,Best Machine Learning Track Hack,Best First-Time Hack,Best Social Good Hack,Hack That Made You Smile,Prettiest Hack,"Gemini was a little difficult to use. Setting up the API key took longer than expected, but we were able to figure it out eventually. Streamlit was new and was very easy to get the hang of. Declaring UI elements as if they were variables in Python was easy to understand and made the development process much quicker. GitHub worked well as usual.",North Carolina State University,OpenAI and Gemini,"Yes, we utilized Gemini in our hack to come up with a wellness plan for users based on their input. Users can input their mood and energy levels then Gemini will come up with a plan of actions for them to take to maintain or improve their mood and energy. Users can also input a goal they want to accomplish and Gemini will come up with smaller action steps they can take to accomplish the goal.",1015680249137,"",North Carolina State University,1,Arsalaan,,arsalaankhan022@gmail.com
Untitled,"",Draft,Pending,Manage team,04/11/2025 21:05:56,"","",,"","",Mc,Gamerboi,morysawesome@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,2,Daniel,Petrov,2005danielpetrov@gmail.com,Ryan,Thomas,rthomas0413@gmail.com
AidenAI: Your AI Financial Aid Advisor,https://bitcamp2025.devpost.com/submissions/642014-aidenai-your-ai-financial-aid-advisor,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 21:06:24,"The Gist

Aiden AI harnesses the power of Google Gemini to analyze official scholarship offers and financial documents, in order to help students make a financially smart and informed college decision. The Aiden AI website allows users to upload the deluge of financial communication they receive from colleges, and understand their situation easily through an informal conversation with a helpful AI assistant.

Inspiration

As college students, we have all had to navigate the extensive and cumbersome financial aid process. We know how difficult it can be to read through large legal and financial documents, understand the complex terms, and decide which offer is better. We were inspired by the power of generative AI to provide people with new insights, and so AidenAI aims to make this process easier and more affordable to those who can’t afford a financial analyst through a personalized AI chatbot. 

What it does

The AidenAI website allows users to inform Aiden of their background, finances, and educational goals, and differentiates itself further by allowing users to submit official aid offers, FAFSA documentation, and communications from school financial aid offices. With a generative AI targeted specifically at parsing these complex financial documents, AidenAI helps students to:


Unpack the financespeak by talking with Aiden
Learn the process of negotiating for more financial aid
Ask the what-ifs and hypotheticals that will inform their eventual decision


How we built it

We started by drawing mockups of the various pages and UI elements that would make up the outward-facing website, as well as connecting buttons to the database actions that would need to be taken. We settled on using the FReMP stack to build this application: Flask, React, MongoDB, and Python. We  also used Vite for rapid prototyping of UI as well as bundling the app for Flask hosting. We also used the CapitalOne dataset for providing the AI with mock financial statements as well, and of course Google Gemini for powering Aiden himself.

Challenges we ran into

There were many technical challenges that came with incorporating so many technologies and libraries into a single project — specifically, we struggled to establish an effective development cycle for backend work, which needed to simultaneously access the external MongoDB database as well as our internally-hosted (fully RESTful!) API. With regards to teamwork and integrating our separate pieces of code successfully, we had some issues with branch merging, especially where multiple people were modifying our application’s shared stylesheets. However, we were able to avoid this issue for the most part by using effective version control techniques.

Accomplishments that we're proud of

We are extremely proud to have developed a fully functional, visually pleasing, and (hopefully!) useful application in only 36 hours! Each member was able to apply their own unique skillset, and each team member constantly discussed how their decisions could impact the tasks other members were completing, which helped us avoid grief down the line. Our team had extremely minimal hacking experience, so it was extremely rewarding to end with a well-designed and fully functioning product at the end of our 36-hour marathon!

What we learned

Our team learned a lot about Git and proper version control, as well as the need for great project file layout, in order to prevent merging issues as much as possible. Additionally, everyone greatly expanded their knowledge of React and Flask, and our backend engineers also learned how to use MongoDB Atlas and Cloudflare.

What's next for AidenAI

As we continue to develop the project, we hope to support more file formats for document upload, and also to work on fine-tuning Aiden’s responses so he can better aid students. Focusing on more minute fixes by improving the graphics and general UI/UX will also be an area of improvement. 
",https://github.com/NetThi123/BitCamp2025/,https://youtu.be/waa7uUj9PN8,"Best Financial Hack - Capital One, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","react, mongodb, python, javascript, gemini, terpai",jlw,.,mirandazellnik@gmail.com,,https://github.com/NetThi123/BitCamp2025/,Best Machine Learning Track Hack,Best First-Time Hack,Best Social Good Hack,Prettiest Hack,Hack That Made You Smile,"We had a great time this weekend!

Gemini was honestly an amazing tool, which fit our use case perfectly. Its ability to maintain multiple conversational memories during a single session was crucial to our project functioning, and it is fantastic at parsing the kind of financial images that humans often struggle with. MongoDB Atlas was a fantastic tool for storing all our user records. We used the pymongo library to interact with the database, which made the process of connecting our existing code to the database near-seamless. React sometimes posed issues in getting components to render consistently on different devices, but we found the large selection of components libraries helped substantially with this.","University of Maryland, College Park",OpenAI and Gemini,"We utilized Google Gemini (specifically, the Flash 2.0 model) to serve as the GPT that our users interact with. Each user is assigned a unique session code, which allows our single Gemini API key to manage many users’ conversations simultaneously. The Gemini model is given many instructions before ever interacting with the user, and all files and information the user has entered on various pages is compiled for Gemini to use as reference. The model is then prompted to initiate the conversation with some useful insights and to prompt further discussion — and only at that point is the user connected directly with the Gemini model. We knew we had to implement a generative AI model because the concept of discussing complex financial topics with an experienced mentor was central to the overall project idea — and Gemini ended up meeting our needs exactly.",AIzaSyAxT73qJEy5lLt0hvsy6UN4grH-3eSTqpw,aidenai.tech,"grant high, University of Maryland - College Park",3,Disha,Ramesh,dramesh1@terpmail.umd.edu,tomatoSquirrel,,menameshimuerte@gmail.com,Netra,Thiru,netra.thirumuruhan@gmail.com
Image Steganalysis: Detect Hidden Text with AI,"",Draft,Pending,Additional info,04/11/2025 21:11:42,"Inspiration
In a world of digital communication, information can be hidden anywhere - even in plain sight. Steganography, the art of concealing messages within digital images, poses significant challenges for cybersecurity and digital forensics. Our project was inspired by the growing need to detect these hidden communications, protecting sensitive information and understanding the sophisticated methods of digital concealment.
What it does
Our steganalysis model is a sophisticated image analysis tool that:

Detects hidden messages within digital images
Analyzes pixel-level variations
Provides a probability score of steganographic content
Can process single images or entire directories
Identifies images with potential hidden information with 95% accuracy

How we built it

Data Collection

Used the Alaska2 Image Steganalysis dataset
Collected images with three steganography methods (JMiPOD, JUNIWARD, UERD)

Feature Engineering

Developed advanced feature extraction techniques
Created a 150-dimensional feature vector
Analyzed statistical, gradient, and least significant bit variations

Model Development

Explored multiple machine learning and computer vision algorithms
Implemented Random Forest Classifier
Optimized feature selection and model parameters

Model Validation

Used stratified cross-validation
Implemented rigorous performance metrics
Fine-tuned detection thresholds

Challenges we ran into

High Dimensional Feature Space

Managing complex feature extraction
Reducing computational complexity
longer training hours and high memory usage while exploring pre-trained computer vision models

Accomplishments that we're proud of

Achieved 95% accuracy in detecting hidden messages
Developed a flexible, scalable steganalysis model
Created a comprehensive feature extraction pipeline
Implemented both single and batch image analysis
Demonstrated practical applications in cybersecurity

What we learned

Technical Insights

Advanced image processing techniques
Machine learning model optimization
Feature engineering strategies
",https://github.com/aniruddhg43986683/Bitcamp_Steganalysis/tree/main,https://www.youtube.com/watch?v=tT6rWbZVmx4,Best Digital Forensics Related Hack - Cipher Tech Solutions,"python, flask, machine-learning, deep-learning",Aniruddh,Rajagopal,aniruddhr@gwmail.gwu.edu,,https://github.com/aniruddhg43986683/Bitcamp_Steganalysis/tree/main,Best Machine Learning Track Hack,"","","","",We were mostly working on python and understanding various machine learning and deep learning algorithms for implementation. We also looked into articles and research papers trying to understand stegnography.,George Washington University,OpenAI,no,didn't use,"",George Washington University,1,Siddharth,Saravanan,siddharthsaravanan27@gmail.com
Species Tracker,https://bitcamp2025.devpost.com/submissions/642019-species-tracker,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 21:12:13,"What it does

Species Tracker both compiles and processes various data on animals to both inform the user and predict future population trends of each animal.

How we built it

We built the backend with Python and flask, while the frontend was done in React. The data cleaning and model training was done with Jupyter Notebooks and various Python libraries such as pandas and scikit-learn. Data was gathered from the iNaturalist API.

What we learned

For most of us, it was the first time combining both back and front-end together, as well as using frameworks such as flask and React.

What's next

Future features we'd like to add include:


Discovery feature to randomly generate species for people to learn about, especially those that are considered endangered
Support more species through collection of more data
Optimize by caching prior operations to reduce API calls
List of the currently safe animals at risk of being threatened in the next 10 yearsz

","",,Best Web Hack Using React - Peraton,"react, html, css, javascript, flask, python, jupyter, git, github, inaturalist, bootstrap, pandas, numpy, scikit-learn, matplotlib, scipy, svm, json",kazu,⚡,pikaminecraftlover@gmail.com,,https://github.com/backedman/species_tracker,Best Sustainability Hack,Best Social Good Hack,Best Moonshot Hack,"","",GitHub was really helpful in helping the three of us collaborate. We were able to take on as much as we did through using Git.,University of Maryland College Park,I did not use GenAI,N/A,N/A,"",University of Maryland - College Park,2,Mohit,Bogineni,backedman@gmail.com,Joseph,Yang,razorrotom@gmail.com
StrataGem,https://bitcamp2025.devpost.com/submissions/642020-stratagem,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 21:13:07,"Inspiration

Ever notice how many awesome ideas pop up every day, but only a few become the next big thing? It's not because people aren't creative enough or don't care enough. Often, it's just that they're missing the know-how to grow their idea into something bigger. That's exactly why we created our solution! 
StrataGem has answers for all your business growth challenges. We wanted to help bridge that gap between having a brilliant idea and building a successful business - whether you're an existing business looking to level up or a brand new startup finding your footing.

What it does

We're like that friend who actually listens when you talk about your business dreams! We take the time to understand what makes your business special - your big goals, what you're passionate about, and what matters most to you.

Then, our clever AI digs through hundreds of real-world success and failure stories to cook up strategies that fit YOU like a glove. No more one-size-fits-all advice that feels like it could apply to literally anyone with a business card! And you don't have to worry about hallucination! Our AI model will present you with a published case study for every claim it makes.

Got a small business that's hit a wall? We'll help you climb over it.

Running a fresh-out-of-the-box startup? We'll help you dodge those first-year disasters that nobody warns you about.

Whether you've been grinding away for years or just took the leap last month, StrataGem is here to transform your brilliant idea into something that makes people go ""Wow, how did they get so big so fast?

How we built it


We started off by scraping a ton of high-quality business case studies specific to a few industries. This was harder than expected since, a lot of them are pay-walled.
Next we setup a Postgres DB and cleaned and ingested this data. We then used BART to classify the case studies on certain predefined categories like industry, business size etc.
We then created embeddings using Ollama and stored it in our DB with pgvector extension
Finally we setup a Langchain x Streamlit app with access to Google Gemini models, and used a RAG based system to provide these LLMs with some rich human context,


Challenges we ran into

Data Extraction: One of the biggest hurdles we encountered was sourcing quality data. There are only a limited number of sites that provide access to past business case studies, and even fewer that offer them in a usable format. Extracting the relevant information took significant time and effort. We had to fine-tune our model to filter out noise and capture only the insights that truly mattered for our strategy generation.

Scoping: We were skeptical about completing everything we had in mind to begin with, and features kept being added and removed. Thanks to Lucas from Microsoft/Cloudforce for letting us pick his brain on this. Ultimately we managed to pull through and finish the project.

API Limits: Another significant challenge we faced was working within the constraints of API rate limits. Shoutout to Google for a generous access to the Gemini API which enabled us to use SOTA LLMs for this project

Accomplishments that we're proud of

StrataGem was an amalgamation and mutation of a bunch of ideas that we spent hours brainstorming. We are extremely happy with how it turned out and especially so of the fact that we managed to implement everything from actual human made case studies as our DB, complex RAG and a PPT generator all in one day :)

What we learned

Throughout the development of this project, we were regularly reminded of the massive potential that GenAI has and how we barely scratch the surface in our everyday lives. We definitely look forward to building highly-complex AI orchestrated systems in the future.

What's next for StrataGem

One of the main things we would like to address is the privacy of business sensitive data being shared with  LLM providers and how we could scope this.
We plan to have StrataGem accessible via an live Streamlit app as well as improve the small but mighty repository of business case studies that we currently have. We would also love to integrate/connect the PowerPoint generation part with Microsoft Copilot's existing capability. 
",https://github.com/harshavardhan-patil/stratagem,,"Best Use of GenAI in Business - Cloudforce/Microsoft, [MLH] Best Use of Gemini API","python, langchain, gemini, openai, ollama, genai, streamlit, rag, postgresql, docker",Harshavardhan,Patil,patil.harshavardhanc@gmail.com,,https://github.com/harshavardhan-patil/stratagem,Best Machine Learning Track Hack,Best Bitcamp Hack,Best Social Good Hack,MOST LIT HACK,Prettiest Hack,"Google Gemini: We have a couple of other projects that require GenAI, and we currently were using Ollama's Gemma3 as a local serving LLM. This was the first time we used direct API calls for our GenAI needs and that was only possible due to the generous free tier for Gemini API. Also having a well integrated framework like LangChain helped a lot.","University of Maryland, College Park","OpenAI, Anthropic, Llama, Gemini, and Gemma","We extensively used GenAI models as a backbone for our system. We tapped into the vast knowledge that the LLMs are trained on to provide us with a solid starting point for a business advisor, in the form of API calls. We then enriched and equipped these GenAI models with business case studies that had resemblance to the problem at hand to enable them to make decisions based on what has actually worked in the past.",508923323742,"",University of Maryland - College Park,2,Pragati,Rao,pragatirao1409@gmail.com,Kriti,Shahi,kritishahi66@gmail.com
NOVA,https://bitcamp2025.devpost.com/submissions/642021-nova,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 21:15:31,"Inspiration


We’ve always been intrigued by how artificial intelligence can transform industries, but one area that felt both urgent and underserved was financial management for small business owners. While large enterprises often have entire finance departments and access to expensive software, small and medium-sized businesses are left juggling spreadsheets, manually tracking loans, or logging into clunky dashboards. We recognized this gap and knew there had to be a better, more intelligent way to handle it.



Our goal was to build something more human and intuitive—a voice-powered AI agent that could take on the burden of financial queries, track customer details, monitor loans, and answer questions in real-time. We wanted something that felt less like software and more like a co-pilot. Something that understood context, handled complexity behind the scenes, and provided crystal-clear responses.

As we dove into development, we realized the potential of integrating technologies like Retell for voice interaction, Azure for hosting and scalability, and Capital One's Nessie API for real-time financial data. It opened up a world where a small business owner could say, ""What are my upcoming loan payments?"" or ""Pull up Agastya Mukherjee's customer profile,"" and have the answers instantly—accurate, contextual, and accessible from anywhere.

In conclusion, Nova was born out of a desire to level the playing field. We wanted to equip small business owners with the same level of intelligence and automation that larger organizations benefit from—only better, faster, and easier to use. By blending financial APIs with voice and AI technologies, we’re reshaping how businesses interact with their finances.



What it does

Nova is a smart, voice-powered financial agent built to assist business owners in managing customer data, tracking loans, and answering finance-related queries in real-time. It’s like having a CFO on-call—one that’s fast, efficient, and doesn’t take lunch breaks.

Nova empowers users to:

-Retrieve and manage customer information with just a name or ID.

-Track loan history, upcoming payments, and accounts with one voice command.

-Create new customer profiles securely and efficiently.

-Filter customer data using first and last names, or fetch comprehensive financial summaries for individuals.

-Communicate naturally through voice using Retell’s conversational AI integration, enabling a seamless and intuitive user experience.


Whether you're a business owner trying to locate a customer record or a finance assistant looking for upcoming loan obligations, Nova delivers answers instantly and accurately.

How we built it

Nova combines multiple technologies to provide a seamless voice and web-based financial management experience.

Frontend: Built in React, offering a clean, responsive user interface that connects users with financial insights in real-time.

Backend: Powered by Python (Flask) and securely connected to Capital One’s Nessie API for real-time data on customers, accounts, and loans.

Voice AI: Integrated Retell, a cutting-edge conversational AI framework, to handle voice commands and make interactions more human.

Hosting & DevOps: Deployed using Azure for reliable, cloud-based performance, with support for future scalability.

Security & CORS: Carefully implemented CORS policies and method overrides to ensure the app functions smoothly even behind proxies and Azure frontends.

We also used helper utilities to modularize Nessie API calls and streamline authentication, making the system both robust and maintainable.


Challenges we ran into

One of the major challenges we faced was enabling cross-origin voice interactions through the Azure proxy. POST requests from Retell often didn’t align with standard expectations, which required creating POST-only fallback endpoints for robustness.

Another hurdle was handling voice recognition edge cases. If a user says something outside Nova’s capabilities (e.g., “Tell me a joke”), the agent needed to gently redirect them back to supported tasks, such as customer or loan-related queries.

Finally, integrating Capital One’s Nessie API, which isn’t actively maintained, introduced additional issues with error handling, authentication, and occasional data gaps.

Despite these roadblocks, we tackled each one head-on with creative API design, robust error logging, and flexible endpoint logic.


Accomplishments that we're proud of

✅ Full integration of voice and web interfaces using Retell and React.

✅ Built a modular, secure backend in Python using helper functions for reusability and maintainability.

✅ Developed a name-based customer filtering system, allowing business owners to fetch detailed records without knowing the customer ID.

✅ Created a loan-tracking feature that loops through all user accounts and compiles upcoming loans—something that mimics functionality you'd find in enterprise-level software.

✅ Made the entire platform accessible with minimal friction and voice-friendly error handling for seamless interaction.

What we learned

This project pushed us to grow in multiple areas—from backend architecture to human-AI interaction design. We learned how to:

Build voice-first user flows that feel natural, not robotic.

Work with external APIs (like Nessie) that require extra care in error handling and testing.

Design POST-only endpoints to bypass proxy restrictions, which was a critical learning moment in building production-grade web APIs.

Create modular request handling functions that let us scale features faster with fewer bugs.

Most importantly, we learned the value of making complex financial tools feel simple—especially for non-technical users.


What's next for Voice Agent for Finance

We’re just getting started with Nova. Here's what's on the roadmap:


Smarter NLP interpretation: Make the voice agent understand vague or casual phrases and context-switch more naturally.
Dashboard visualizations: Let users visualize their financial health with graphs, pie charts, and dynamic insights.
Authentication + Multi-user support: Allow businesses with teams to assign roles and access levels.


-Mobile-first experience: Optimize the UI for mobile users who need to check finances on the go.


Wider API integration: Nova can now connect with QuickBooks, Stripe, and Plaid to offer more financial intelligence.


Nova is on a mission to democratize financial management for small businesses—and this is just the beginning.
","https://financewithnova.tech/, https://github.com/rishabhchheda01/bitcamp-2025, https://www.linkedin.com/posts/nova-umd_introducing-nova-activity-7317136305170120705-WopT?utm_source=share&utm_medium=member_desktop&rcm=ACoAAD5PV8kBisQAe8CTnFX7uKHTXERiqNLaHvE",https://www.youtube.com/watch?v=n-Y_rs_MV-s,"Best Financial Hack - Capital One, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech","python, react, python-(flask), retell-ai, gpt-4o, gpt-4o-mini, custom-function-calling, terpai, azure, quickbooks, stripe, flask, requests, cors, html, css, javascript, capital-one-nessie-api",Aryan,Agrawal,aryan527@terpmail.umd.edu,,https://github.com/rishabhchheda01/bitcamp-2025,Best Machine Learning Track Hack,Best Razzle Dazzle Hack,Best Gamification Hack,Hack That Made You Smile,Prettiest Hack,"the team integrated a modern tech stack featuring React for a dynamic front-end, Python (Flask) for the robust backend, and Retell for seamless voice interaction while leveraging Capital One’s Nessie API for real-time financial data and deploying on Microsoft Azure to ensure scalability and reliable cloud hosting.",UMD,OpenAI and Other,"We integrated generative AI into our hack by powering our Retell voice agent API with GPT-4o and GPT-4o-mini, using custom function calling to deliver seamless, context-aware responses to natural language queries. Additionally, we leveraged TerpAI to brainstorm ideas, provide coding assistance, and generate our pitch and presentation, which streamlined development and added significant creative value to the project.",NA,financewithnova.tech,University of Maryland - College Park,3,Aadesh,Kheria,kheriaaadesh@gmail.com,Agastya,Mukherjee,amukhs13@terpmail.umd.edu,Ashay,Srivastava,ashaysriv13@gmail.com
pocketPenny,https://bitcamp2025.devpost.com/submissions/642022-pocketpenny,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 21:15:42,"Inspiration

We were inspired by Kakeibo, the traditional Japanese budgeting system that has been used since 1904.

What it does

A financial goal tracker that not only allows users to keep track of their finances properly but also includes a ""Penny the Pincher"" bot that gives financial advice.

How we built it

We used HTML and CSS for the front end, and used JavaScript for the back end. For our AI bot, we used Gemini-API with React to program our Penny the Pincher bot.

Challenges we ran into

This was both of our first hackathon, and our team also was halved on the second day. Neither of us had experience working with Gemini as well.

Accomplishments that we're proud of

Our UI is very clean and designed, and we have our bot running on Gemini Flash 2.0.

What we learned

We learned how to work when circumstances in life come up, and also how to create our own AI bot.

What's next for pocketPenny

Our next plan is to have the AI be a bit more fine tuned to our liking, including fonts, layout and more. We also want to get the link working to connect it to our github pages. 
",http://pocketpenny.tech,,"Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best Use of Gemini API","html, css, javascript, react, gemini",Daniella,Ghonda,danieog@terpmail.umd.edu,,https://github.com/danieog/kakeibo,Best App Dev Track Hack,Best First-Time Hack,Best Bitcamp Hack,Prettiest Hack,"","This was both of our first times using an API, let alone Gemini-API. The process was very confusing and daunting at first, especially since we both had little experience with it. But by early Sunday morning, we had the program running.  Overall, it was really hard since we didn't have the experience but worth the journey. We also used React to help seamlessly blend our AI bot, along with HTML and CSS.","University of Maryland, College Park",Gemini and Other,"We implemented Gemini's API into our hack, by allowing users to chat in real time with it. We used Gemini over other APIs because of how accessible it was to students (being free), and how well it worked with React and our program.",524277221236,pocketpenny.tech,University of Maryland - College Park,1,Nardos W.,Hailemariam,nhailema@terpmail.umd.edu
Payment Protect,"",Draft,Pending,Additional info,04/11/2025 21:19:20,"Inspiration

On our travels, we booked a trip to Puerto Rico, fully intending to explore the vast natural wonder of the lush El Yunque forest. Yet, our  refused our payment procedures, leaving us stranded with our flights booked, and no forest to explore. We filed a card dispute, with our money never being returned to us, yet it took weeks to process, during which we were forced to cancel the trip altogether, unable to pay for our hotel or flights. And this is due to the 238 million credit card disputes that were filed in 2024, the majority of which are small scale disputes that only serve to distract the card company from taking care of the actual problems. The average dispute also takes $50 to solve, even when the dispute costs less then this, wasting millions of dollars in company assets in disputes that ultimately return less value to the consumer then the card company actually put in, and inevitably, with the 96% return rate of disputes, paying the consumer (justly or not) back anyways. Yet with new technologies, why do we still keep the old ways, of constantly referring back to a human, to constantly having to go through so many layers just to get our justice back, only to waste the money and time of both the consumer and the company?

What it does

There are two major factors to Payment Protect. The first one is the Dispute AI system. In Dispute AI, the customer will directly bring up key information from a list of their recent transactions, by clicking the dispute button next to these transactions, which will then be copied directly into Dispute AI. The ai will then begin the process of automatically filing a dispute, by checking its existing dispute database and realistically submitting a dispute to be compared to this database. If it detects the dispute as valid, and the dispute is under a certain threshold of monetary value, the AI will automatically complete the transfer, saving the bank an average of $45 and 50 days per dispute, which equates to hundreds of millions in value lost every year, often without reason, as most disputes are lower in value then the average cost to file the dispute in the first place. However, if the dispute is invalid, as the machine learning model has been trained to be able to recognize possibly fraudulent disputes and will only improve on this over time, or if the dispute is over a certain monetary threshold, it will be sent to a human appraiser. This is to ensure that no dispute is unfairly worked against, bringing in a human element to have final say. The other function of Payment Protect is an heatmap, which, based off of parameters, provides a score to each merchant that will directly showcase to the consumer merchants they should and should not use. This can be used as a preventative measure, that will ensure that future disputes are lessened, saving the company from having to waste millions on disputes that ultimately could be solved extremely easily originally. The anti-fraud measures undertaken by Payment Protect are second to none.

How we built it

A sql database using Nessi API was created in order to store mock financial transactions, customers, and merchants, and this was fed into an algorithm we created in order to efficiently create more and more of these parameters using the API. After this, we connected a Federal Consumer reports csv, and, after summarizing the data into only relevant points, connected it to the Nessi API database, eventually creating one large database that the AI could use for everything. After this, we assigned a certain score to each merchant from a list of the top 30 Capital One Vendors using a different algorithm, and displayed this in a heatmap. This algorithm used the new big database and therefore could assign a score based upon the details surrounding the disputes, amount of disputes for that vendor, and most recent disputes. 

Challenges we ran into

The AI at first struggled to understand the database it was provided with, yet through training and continued trials we were able to focus it's outputs and effectively convey the purpose the AI originally was meant to conceive. Another major challenge was the creation of an interconnected database, as we first had to change the sql database and copy it into a csv format, which turned out to be harder then expected.

Accomplishments that we're proud of

A major accomplishment we are proud of is the teamwork that our members showed on working together to effectively handle issues posed by the code and streamline its production. Another major accomplishment is the integration of Gemini AI and OpenAI, as both APIs struggled to function under such a heavy workload.

What we learned

We learned how to use APIs effectively together in multiple layers to accomplish complex algorithms that can successfully perform complicated tasks.

What's next for Payment Protect

We hope to integrate statistics into the web app and create far more test users in order to iron out issues and grow support for this new program.
",https://disputedaddy.web.app/,https://youtu.be/kQaWIzZW6VE,"Best Financial Hack - Capital One, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of Gemini API","python, javascript, capital-one, nessieapi, google-cloud, gemini, openai, react, consumer-complaint-database",Aman,Talreja,atalreja@terpmail.umd.edu,,https://github.com/amantalreja/Bitcamp-2025,Best First-Time Hack,Best Bitcamp Hack,Best UI/UX Hack,MOST LIT HACK,Prettiest Hack,"We used Superbase for data collection and for API usage of data, which was extremely effective and capable.","Aman Talreja and Varsha Devireddy attend University of Maryland, Nitesh Kumar attends Georgia State University,  Grish Kinger attends Greenbrier High School","OpenAI, Hugging Face, and Gemini","Yes, we used Gemini API and OpenAI to create an AI system that was used in our web-app, as a major feature of our web-app is an AI system. We used Hugging Face and Kaggle for gathering",967940727656,"","University of Maryland - College Park, Georgia State University, Greenbrier High School, Univerity of Maryland College Park",3,Nitesh,Kumar,nkumar13@student.gsu.edu,Grish,Kinger,grishkinger2008@gmail.com,varshadevired,Varsha Devireddy,vdevired@terpmail.umd.edu
Untitled,"",Draft,Pending,Manage team,04/11/2025 21:22:29,"","",,"","",varshadevired,Varsha Devireddy,vdevired@terpmail.umd.edu,,"","","","","","","","","","","","",Univerity of Maryland College Park,0
CarbonQapture,https://bitcamp2025.devpost.com/submissions/642027-carbonqapture,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 21:22:59,"Inspiration

Quantum Computing promises a new era of computing that uses the quantum nature of a particle to perform computations exponentially faster. With climate change and greenhouse gases like Carbon Dioxide at the forefront of global concerns, carbon capture has emerged as a critical area of research. Metal-organic frameworks (MOFs) are highly porous materials capable of selectively adsorbing CO₂, making them prime candidates for sustainable solutions. However, the vast design space of MOFs makes discovery a challenge — this is where quantum concepts like the Variational Quantum Eigensolver (VQE) comes in.

What it does

This project integrates quantum simulation and machine learning to:


Simulate MOFs using the Variational Quantum Eigensolver (VQE) algorithm.
Estimate ground-state energies to assess CO₂ capture efficiency.
Train a neural network to predict and propose novel MOF structures optimized for carbon capture.
Test out the proposed MOF structures on the VQE algorithm to analyze ground state energy.
Output predictions, highlighting performance metrics like uptake, selectivity, and heat of adsorption.


How we built it


CIF Parsing: Structural data from a real MOF Dataset (in .cif format) is parsed.
Hamiltonian Construction: A simplified 2-qubit molecular Hamiltonian is built for each structure.
Quantum Simulation: Using PennyLane, VQE estimates the ground state energy of each MOF-CO₂ system.
AI Model: A neural network is trained on simulation outputs and pre-existing MOF dataset to propose new MOF structures.
Results: Energies and material properties are saved in .csv format and visualized via plots.


Challenges we ran into


Qiskit Deprecation: Several components of Qiskit, including modules like qiskit.algorithms and deprecated estimators, posed compatibility issues with the latest versions. We had to refactor portions of our codebase and explore alternative quantum computing frameworks like PennyLane to maintain functionality and ensure forward compatibility.
MOF Generation: One of the key challenges we faced was training our model to generate physically viable metal-organic frameworks (MOFs). Initially, the output structures consisted of randomly distributed atoms without any bonding, resulting in non-physical geometries and ground state energies of zero. To address this, we refined the parameters of our training dataset and implemented structural constraints to guide valid MOF generation. Additionally, we adjusted the Variational Quantum Eigensolver (VQE) algorithm by replacing the placeholder coefficients in the Hamiltonian with values more representative of realistic chemical interactions. These changes were crucial in enabling accurate energy evaluation and meaningful structure prediction.


Accomplishments that we're proud of


Successful generation of potential MOFs:
Implementation of the Variational Quantum Eigensolver:
Successful Training of the Neural Network: The new generated MOF's by the Neural network have the following advantages over the currently existing MOF's , 98.3% accuracy in predicted ground state energy, Higher avg CO2/N2 Selectivity, Predicted topology and functional groups building the entire structure and lastly, a better predicted CO2 Uptake that leads to having a more profound impact on the atmosphere.
Implementation of a Professional UI: Building the entire user interface from scratch using React was a significant challenge, but incrementally developing each component helped us manage the complexity effectively. Visualizing the raw data from our trained models posed another difficulty, especially with over 8,000 generated data points. To ensure clarity and performance, we selectively sampled key data points to construct meaningful graphs. For the frontend, we integrated several interactive React libraries to create a responsive and engaging user experience.


What we learned


We learned how to use PennyLane for the first time. This is the first time we have implemented the Variational Quantum Eigensolver.
We learned about metallic-organic frameworks and their application in carbon capture
We learned how to write a neural network, and is our first time writing an AI model that generates new Compounds using a mix of supervised learning and generative AI
We learned how to use react to make dynamic websites, especially interactive graphs.


What's next for CarbonQapture

Our long-term vision is to revolutionize the way innovative materials are discovered and optimized to meet critical environmental challenges. By integrating cutting-edge quantum algorithms with artificial intelligence, we aspire to create a dynamic research platform that continuously evolves and adapts, fostering unprecedented breakthroughs in the field of carbon capture.
We envision a future where our approach not only contributes to a deeper understanding of MOF chemistry and CO₂ sequestration but also spearheads the development of sustainable, high-performance materials. This initiative aspires to bridge the gap between theoretical simulation and practical application, ultimately contributing to global efforts against climate change.
In this vision, the synergy between quantum computing and AI will pave the way for scalable, efficient material design strategies that empower industries and governments worldwide to achieve cleaner, greener technologies.
",https://bitcamp25.vercel.app/#hero,https://www.youtube.com/watch?v=DdIYIRiyi7E,"Best Hack Promoting Public Health - Bloomberg, Best Web Hack Using React - Peraton","python, pennylane, os, pandas, numpy, javascript, react, pytorch, cuda",Sankalp,Dasari,sankalp.dasari@gmail.com,,https://github.com/Sankalp-dasari/bitcamp25.git,Best Advanced Quantum Track Hack,Best Sustainability Hack,Best UI/UX Hack,Prettiest Hack,Hack That Made You Smile,"PennyLane: This was the first time we have used PennyLane. We have experience with Qiskit and have implemented quantum circuits using Qiskit resources. However, implementing PennyLane was a new experience for us.
PyTorch: I have experience using it before but it was my first time using it with a neural network. This is also my first time implementing it for a scientific project.
React: Although we have experience with react, we learned that react has a lot of complex packages. If used correctly, we can make more dynamic and interactive UI",Virginia Tech,OpenAI and Anthropic,N/A,N/A,N/A,"Virginia Tech, Greenwood High International School",3,Aarya,Anup,aarya.k.anup@gmail.com,Aditya,Sunke,adityasunke2004@vt.edu,Sophia,Chukka,sophia05@vt.edu
SayPass,https://bitcamp2025.devpost.com/submissions/642028-saypass,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 21:24:49,"SayPass – Voice is the New Password

Inspiration

Till now, we’ve commonly used patterns, numerical PINs, or fingerprint-based passwords for device and app authentication. 

As an upgrade, SayPass introduces a smarter, more personalized way of authentication — your voice

Unlike many modern smartphones that recognize only voice commands , SayPass goes further by analyzing the unique vocal frequency, pitch, and tone of a person’s voice, something that's hard to imitate and unique to each individual.



What We Built

SayPass is a dual-verification voice authentication system that:


Verifies who is speaking using ECAPA-TDNN speaker embeddings.
Verifies what is spoken using Whisper transcription.
Grants access only when both the voiceprint and phrase match.


Say your passphrase, and SayPass checks:


Is it your unique voice frequency?
Did you say the correct phrase?


Only when both match, access is granted ✓



Use Cases


Cybersecurity defense – Adding a biometric voice layer to protect against unauthorized access
Forensic voice analysis – Matching suspect’s voice with crime scene audio evidence
Hands-free access in vehicles, workstations, or AR/VR setups
Voice-based smart lock access
Authentication for visually impaired users




What We Learned


How to integrate voice biometrics using ECAPA-TDNN
How to use Whisper for accurate transcription, even with accents
The importance of combining voice tone + spoken content for high-security systems




Challenges Faced


Background noise and silent voice trimming
Fine-tuning thresholds to avoid false accept or false reject




The Future

We envision SayPass as a secure, scalable system that can be used in:


Mobile & Web Authentication
Smart Home & IoT Devices
Personalized AI Assistants


Because in the future...


Your voice should be your password.

","",,"Best Digital Forensics Related Hack - Cipher Tech Solutions, [MLH] Best Use of .Tech","tensorflow, ai",Aswin Balaji,Thippa Ramesh,at119@gwmail.gwu.edu,,"",Best Cybersecurity Track Hack,Best First-Time Hack,Best Moonshot Hack,MOST LIT HACK,Prettiest Hack,"Tech Used:
SpeechBrain, OpenAI Whisper, Librosa, SoundDevice, WebrtcVAD, noisereduce, GitHub",The George Washington University,OpenAI,"Yes, we implemented OpenAI’s Whisper, a generative AI model for speech-to-text transcription. It was used to accurately convert spoken passphrases into text, enabling semantic matching as part of our dual-layer voice authentication system.",N/A,"",George Washington University,1,Rahul,Arvind,rahul.arvind@gwu.edu
Finfig,https://bitcamp2025.devpost.com/submissions/642029-finfig,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 21:26:59,"A financial fighter game that can motivate you to make better decisions.
With the use of AI, Finfig lets you compete with your friends while keeping your financial history confidential.

How it works


Every month, you and your friends start with 1000 points.
Transactions are automatically uploaded using Capital One's Nessie API.
The user then has to upload the receipt for that purchase and justify it, allowing AI to see the itemized bill. (You can choose not to upload a receipt for a purchase, but there will be a point deduction to prevent users from hiding unnecessary spending.)
The AI analyzes the receipt and justifications to decide the necessity of the purchase, and deduct points based on how frivolous the purchase was.
The AI can also estimate the amount of carbon emissions needed to produce the item, and the value for  money of that item using data from Walmart and other online price averages.
This means there are three categories upon which you can compare your purchases to your friends, allowing you to make more necessary, more sustainable, and more efficient purchases.
It also stores your receipts locally for your convenience.


Why it works


The competition forces you to review your payments later and think about why you made them, which is a crucial part of controlling frivolous spending.
The competition also adds a social element to the motivation to be more sustainable and make more clever financial decisions.

","",,"Best Financial Hack - Capital One, Best Use of GenAI in Business - Cloudforce/Microsoft","python, flask, gemini, nessie, html5, flutter, dart",Pratheek,Ramakrishna,pratheek.ramakrin@gmail.com,,https://github.com/dwarakeshbaraneetharan/FinFig,Best Gamification Hack,Best First-Time Hack,Best Razzle Dazzle Hack,"","","Python, Flask, Gemini API, Nessie API, HTML, CSS, GitHub, Discord, Slack",University of Maryland,Gemini and Other,We implemented a gemini API in our project to evaluate the necessity for transactions.,n/a,"",University of Maryland - College Park,3,Narein,Vignesh,nareinvignesh27@proton.me,dwarakeshbaraneetharan,Baraneetharan,dwarakeshbaraneetharan@gmail.com,Sanil,Arora,sanarora@terpmail.umd.edu
Gaadi,"",Draft,Pending,Project overview,04/11/2025 21:35:41,"","",,"","",aditya,waikar,asw6688@gmail.com,,"","","","","","","","","","","","","University of Maryland - College Park, University of Maryland - Baltimore County",3,Aaryan,Jadhav,aaryanjadhavo1@gmail.com,psampat04,Sampat,parthsampat2023@gmail.com,Sumedh,Kane,sumedhk1@umbc.edu
hamburger simulator,"",Draft,Pending,Project details,04/11/2025 21:37:54,"Inspiration

whopper

What it does

makes u hamburger

How we built it

with love and the krabby patty secret formula

Challenges we ran into

IDK BRUH

Accomplishments that we're proud of

finishing

What we learned

how to write bad code

What's next for hamburger simulator

taco simulator
","",,"",javascript,James,Espericueta,espericueta2303@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,1,BizzMage,Grammer,dgrammer@terpmail.umd.edu
Tariffix,https://bitcamp2025.devpost.com/submissions/642033-tariffix,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 21:40:29,"Inspiration

With tariffs becoming a popular topic in recent news, we saw a growing gap in consumer understanding of how these hidden costs influence everyday purchases. Tariffix was born out of a desire to make that information accessible, understandable, and easy for the average American shopper.

What it does

Tariffix is a dual-platform tool a Chrome extension and a website designed to help consumers understand the hidden layer of tariffs behind the products they buy.


 Browse smarter on sites like Amazon and Walmart with real-time insights into tariff-influenced pricing powered by AI.
 Look up HTS Codes (Harmonized Tariff Schedule) to understand import taxes on specific goods.
 Estimate how much companies pay in tariffs, and how those costs might be affecting you.


How we built it

Tariffix was developed as a monorepo featuring:  


A Next.js website for tariff education and HTS lookup.
A Vite-powered Chrome extension to overlay data on online stores.
A shared AI backend that scrapes metadata (product category, country of origin, etc.) and calculates estimates.
A ** Pricing algorithm to ensure even with errors you ensure you get an estimate.
A MongoDB database to store and update HTS codes and tariff rates.


Challenges we ran into

Finding accurate, real-time tariff data was harder than expected.
We initially planned to use the U.S. Department of Commerce’s FTA Tariff Rate API, but it hadn’t been updated since early 2024.
To solve this, we created a custom dataset using web scraping and tariff tables, allowing us to estimate tariff costs across various goods.

Accomplishments that we're proud of


Built a seamless integration between extension and site. 
Developed a flexible backend for live tariff estimation.
Made a complex economic concept easier for the average consumer.


What we learned


Tariff data is fragmented and hard to access, centralizing it is valuable.
Building for both web and extensions takes planning and modular design. 
UX matters a lot when explaining dry, complex topics like international trade.


What's next for Tariffix


Integrate real-time news on trade policy and tariff changes.
Expand the database to include country-specific tariff rules.
Partner with financial organizations and companies.
Add visual graphs and price comparison tools for better insight.

",https://github.com/Adr1an04/Tariffix,https://youtu.be/OPF4dyn6LrM?si=jiRWWvwZLqZHudB_,"Best Financial Hack - Capital One, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","next.js, vite, typescript, react, tailwind, mongodb, cheerio, chrome-extension-types, chrome-extension-api, gemini",Sam,Borges,samuelxborges@gmail.com,,https://github.com/Adr1an04/Tariffix,Best App Dev Track Hack,Best UI/UX Hack,Best First-Time Hack,Prettiest Hack,Hack That Made You Smile,"This weekend was our first time working with web scraping, MongoDB Atlas, and Gemini AI, and we learned a lot. Using Cheerio to scrape product data from Amazon and Walmart directly in the browser was super interesting—it gave us a hands-on understanding of how to extract and process data from real web pages. MongoDB Atlas was also new to us, and we were surprised by how easy it was to set up and manage. The Compass tool was especially helpful for testing queries and visualizing our data. We also tried out Gemini AI for the first time to generate summaries of tariff info, which helped make the data easier to understand. Overall, it was a fun experience getting to work with tools we hadn’t used before, and we gained a lot of practical knowledge.",University of Central Florida,Gemini,"Yes. We utilized AI and API technologies across multiple layers of our stack to enhance trade data accessibility. Our Chrome extension, built with Vite and Chrome Extension APIs, uses Cheerio for real-time DOM parsing to extract HTSNO-related content directly from e-commerce sites. Once identified, the extension queries a MongoDB Atlas backend—a distributed NoSQL data store optimized for low-latency retrieval of tariff rates tied to HTSNO codes. To make this data more interpretable, we integrated Gemini AI, a powerful NLP model, which processes the queried results to generate concise, human-readable summaries of applicable trade duties, leveraging its contextual understanding to assist users in making informed import decisions within their existing browsing experience.",AIzaSyAsSkoizAwBbLATPlQRTsQN7GSrFyYyPPk,"",University of Central Florida,3,Kai,Sprunger,kaisprunger@gmail.com,Elizabeth Pretto-Sotelo,⊹.✮₊⋆,elizabethprettosotelo@gmail.com,Adrian,Osorio Blanchard,osorioadrian04@gmail.com
SmartSave,"",Draft,Pending,Project overview,04/11/2025 22:03:09,"","",,"","",Jared-Hernandez,Hernandez,hernandezjared386@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,2,ichethan01,Chethan,ichethan@terpmail.umd.edu,YS,Develop,0codedevelop@gmail.com
Untitled,"",Draft,Pending,Manage team,04/11/2025 22:05:21,"","",,"","",Christian,Lee,christianmlee0206@gmail.com,,"","","","","","","","","","","","","",0
Untitled,"",Draft,Pending,Manage team,04/11/2025 22:08:39,"","",,"","",Christian,Lee,itsbabyshark11@gmail.com,,"","","","","","","","","","","","",Wootton,0
Buddy,https://bitcamp2025.devpost.com/submissions/642038-buddy,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 22:10:55,"Inspiration

Over 3 million students rely on Shuttle-UM buses at the University of Maryland each year. While the Transit App offers reliable real-time tracking, I noticed a gap: it felt too cluttered.
 Buddy makes navigating campus buses and Metrobus as simple as saying, “Find me a bus to Regents Drive Garage?”

What it Does?


Listen to natural speech
Understand bus-related queries like:


""Find me a bus to Eppley Recreation Center.”
""Find me a bus from Graduate Hills to Regents Drive Garage.”

“Show me the bus route”:
This would create a photorealistic 3D map animation for all the bus stops.
""Where am I right now?"":
Automatically fetch your location using GPS coordinates and display it.
Retrieve and summarize the bus transit itineraries.


How its built ?


Flutter + Dart: Built the mobile interface for speech interaction, message rendering, etc.
Speech-to-Text: Used speech_to_text package to convert voice to text in real-time.
Flask Backend: Acts as a middleware between the app and third-party APIs (Dialogflow, TransitApp, Maps, Gemini).
Dialogflow: Handles NLP for understanding intents.
Geolocation: Fetches user coordinates and place names using the Geolocator package and Google Maps Geocoding API.
Photorealistic 3D Tiles API: Used to show animation and bus stops.
Transit Routing: Calls TransitApp’s OTP planning API for real-time itinerary results.
Google Gemini AI: Parses JSON route data and generates clean, human-friendly summaries with coordinates and dynamic Google Maps links.


Challenges:


Geolocation accuracy: Translating location intents to real GPS data was tricky, especially with ambiguous addresses.
3D Maps: Figuring out the animation with each bus stop for the selected bus itinerary was difficult because this feature is in ALPHA by Google.
Processing .json file having bus routes.


Accomplishments that I'm proud of:


Generated interactive route summaries with Gemini AI that users can explore via photorealistic 3D maps.


What I learned

Combining Flutter , flask and a bunch of API’s and create a fully working prototype

What's next for Buddy?

More extensive summaries for Bus routes with Gemini API 
Add real-time bus tracking.
Lot of error handling needed for edge cases.
Integrate Database and multi-users.
",https://gitlab.com/ronnie28/buddy/,https://www.youtube.com/watch?v=UJ3YZi_M3V0,[MLH] Best Use of Gemini API,"flask, flutter, javascript, gemini-api, transit-api, json, ngrok, google-cloud, dialogflow, google-maps, google-ai-studio, python",Ronnie,J,ronniejoseph.work@gmail.com,,https://gitlab.com/ronnie28/buddy,Best App Dev Track Hack,Best First-Time Hack,Best Bitcamp Hack,"","",I found Google AI studio very interesting for my usecase. It allowed me to get a ready-made code after I tested via system instructions to get proper answers in the studio dashboard.,"University of Maryland, College Park",OpenAI and Gemini,Yes I used Google Gemini API to auto summarise the .json file containing bus itineraries and return it back to the flutter app frontend.,AIzaSyCt4RSTijA3GK-y5JsuT1jJopQK54emN0s,"",University of Maryland - College Park,0
Untitled,"",Draft,Pending,Manage team,04/11/2025 22:16:04,"","",,"","",Alex,Slokvenko,aslokvenko@gmail.com,,"","","","","","","","","","","","","",0
The Dino Game,https://bitcamp2025.devpost.com/submissions/642042-the-dino-game,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 22:18:34,"Inspiration

Google's classic dinosaur game.

What it does

You play as a dinosaur that is running through the wilderness, in either the story mode or endless mode. In either one, the only movement you control is ducking and jumping. You have to dodge dangerous obstacles in order to make it to the end in story mode or to a new highscore in endless mode.

How we built it

Using Godot 4.3 as our game engine, we were able to put everything together quite easily. We collaborated using GitHub, made art with Aseprite, and got the font through searching online. The music was made through samples online and then those samples were mixed.

Challenges we ran into

We ended up having a major issue where a commit was pushed to main that cause the entire game to break. This resulted in us losing about and hour's worth of progress and having to rebuild what we had lost. 

Accomplishments that we're proud of

We were able to get the core gameplay of both routes of the game done even though some of the features such as the pause menu and main menu are a little bare bones.

What we learned

The dinosaur game is actually pretty hard to make. Also, tweening kinda sucks.

What's next for The Dino Game

There isn't much planned for the future of The Dino Game, but if we were to do more, the visuals and less developed parts of the game would definitely be our focus.
",https://drive.google.com/drive/folders/1bwfIBA-EQyCTvBNnPqe38FjK8weC0tsk?dmr=1&ec=wgc-drive-globalnav-goto,,"","godot, aseprite",Winton,Jones,oogabooga1271@gmail.com,,https://github.com/PlantvZombie/Dino,Best Gamification Hack,Best Bitcamp Hack,"",MOST LIT HACK,Prettiest Hack,"In the process of using GitHub, there were some major issues when multiple merges happened in short succession from different people. This was a little frustrating and caused some setbacks, but GitHub worked great otherwise.",Baltimore Polytechnic Institute,I did not use GenAI,N/A,N/A,"","Baltimore Polytechnic Institute, baltimore poly, Baltimore polytechnic, Balitmore Polytechnic Institute",3,NoahGBaker,Baker,palifonix@gmail.com,Walter,Sibrian,walterchats1864@gmail.com,Am,Juyin,juyinam@gmail.com
Sudoku Mobile App Solver,https://bitcamp2025.devpost.com/submissions/642043-sudoku-mobile-app-solver,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 22:20:20,"Inspiration

I love solving sudoku puzzles and wanted to learn how to integrate image recognition and build an app to solve puzzles.

What it does

This is a mobile app that allows the user to either upload or take a photo of a sudoku board to then solve it!

How we built it

We built it using Android Studio with languages such as Java and Kotlin. As well as dependencies like CameraX and Google ML kit. 

Challenges we ran into

One challenge we ran into was making Google ML properly read the image and extract the correct numbers. 

Accomplishments that we're proud of

We were really proud of the fact that this was our first hackathon, but we still chose something way outside of our comfort zone. Meeting one another for the first time also tested our ability to adapt, which I think both of us did a great job at. Through proper communication, persistence, and drive we made it further than we originally thought we could. 

What we learned

We learned that there needs to be a lot more research done regarding what languages, or even the IDE we will dependencies to use. Starting to code immediately might have been the biggest drawback since there were so many features we did not fully understand until later on. 

What's next for Sudoku Mobile App Solver

We would like to see this app be more flexible in terms of the types of images it can take or have even more puzzle solvers integrated into it. 
",https://github.com/ElijahAmir08/SudokuApp.git,,"","kotlin, java, google-ml, android-studio, camerax",Elijah-Amir R,Hodges,nuy7wc@virginia.edu,,https://github.com/ElijahAmir08/SudokuApp.git,Best First-Time Hack,Best UI/UX Hack,"",Hack That Made You Smile,Prettiest Hack,We utilized GitHub the most and this was an excellent tool to collaborate as a team with.,University of Virginia,OpenAI,N/A,N/A,N/A,"University of Virginia, University of Maryland - College Park",1,Sean,Burk,sburk2@terpmail.umd.edu
Unravel,https://bitcamp2025.devpost.com/submissions/642050-unravel,Submitted (Gallery/Visible),Pending,Submit,04/11/2025 23:23:00,"Inspiration

News consumption today is fragmented. Most readers rely on a narrow set of sources, reinforcing personal biases and creating filter bubbles. We wanted to break that by helping users visualize how stories shift...not just in topic, but in tone, timing, and framing. What if we could model that drift and volatility across platforms?

What it does

Unravel is a browser extension that enhances news reading by offering multiple perspectives. When you're reading a news article, Unravel:


Analyzes the current article's topic
Finds related articles from other major publishers across the political spectrum
Performs sentiment analysis to show how each source frames the same story
Visualizes publication timelines to reveal how coverage unfolds over the past 12 days


Readers get immediate, contextualized insight into how narratives are shape, without needing to search manually.

How we built it


Chrome extension: JS content scripts, popup UI, and background worker
FastAPI backend (Python) for NLP processing
HuggingFace transformers for real-time sentiment analysis
Chart.js to render dynamic sentiment-over-time graphs
NewsAPI to source articles across mainstream media
Experimental integration with the Bluesky API to track real-time social volatility


Challenges we ran into


Content Security Policy (CSP) restrictions in Chrome extensions made it difficult to use external libraries like Chart.js and D3.js
Needed sentiment analysis that was both lightweight and context-aware
Managing cross-script communication (content, popup, background) was tricky
Pretrained LLMs struggled with nuanced cross-source comparisons
Bluesky’s rate limits unexpectedly hit us mid-testing 🥲


Accomplishments that we're proud of


Built an end-to-end system that integrates seamlessly into the reader’s workflow
Created visualizations that reveal media framing and sentiment shifts
Clean architectural split between frontend and backend components
Designed a reusable modular backend that extends beyond just news
Integrated decentralized social data into our sentiment pipeline


What we learned


Chrome extension architecture and inter-script messaging
HuggingFace pipelines for zero-shot classification and sentiment scoring
Web scraping, rate limiting, and content filtering on decentralized platforms like Bluesky
How to model narrative drift as a volatility signal


What's next for Unravel


Support more news languages and international domains
Integrate source credibility scores and historical sentiment baselines
Add an interpretability layer explaining why a sentiment was labeled
Expand volatility tracking to Reddit, X (Twitter), and emerging social platforms
Release a research-grade API for analysts, journalists, and traders

",https://github.com/bkalaaa/unravel.git,,"","fastapi, javascript, chartjs, chrome, python, hugging-face-transformers, zero-shot-classification, newsapi, service-worker",Jessica,He,jessicahe004@gmail.com,,https://github.com/bkalaaa/unravel.git,Best Machine Learning Track Hack,Best Social Good Hack,Best Bitcamp Hack,"","","We primarily used HuggingFace Transformers, FastAPI, NewsAPI, Chart.js, and experimented with Bluesky's API.

HuggingFace was smooth and powerful for real-time sentiment analysis—even for zero-shot classification. It allowed us to quickly prototype and adapt our NLP pipeline without fine-tuning a full model.

FastAPI was a great choice for handling async requests and modular backend development. It made it easy to integrate multiple endpoints like sentiment scoring and social data ingestion.

Chart.js worked well for visualizations, though integrating it inside a Chrome extension popup required workarounds due to Content Security Policy (CSP) limitations.

Our biggest challenge was with Bluesky’s AP, its rate limits and limited documentation made experimentation difficult, though we still managed to extract sentiment signals from social posts.","University of Maryland, Georgia State University, Penn State University,",Hugging Face,"Yes, but not to generate, to trace change. Our pipeline takes a news article and maps how its narrative shifts across publishers and social platforms, using HuggingFace’s zero-shot models to track sentiment, framing, and timing.

This isn’t just NLP, it’s a new lens for the internet. As digital noise grows, what matters is who’s talking and how they’re reacting. We’re pushing toward volatility-aware systems that capture how narratives drift, and why that matters.",N/A,"","Georgia State University, Pennsylvania State University",2,William,,wl63025@yahoo.com,B,K,kalakheti_@outlook.com
Untitled,"",Draft,Pending,Manage team,04/11/2025 23:47:18,"","",,"","",Matthew,Tsi,matthewofoc@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,2,Rita,Quach,ritaquach2004@gmail.com,Tony,Zheng,tonyzh21@gmail.com
Mapify,"",Draft,Pending,Additional info,04/12/2025 00:09:34,"Inspiration

Studying with visuals cater to various learning styles, simplifies complex information, and enhances memory and retention.

What it does

It takes in a transcript and provides a summary and a mind map. The tool also has a chatbot and flashcard builder for additional study tools.

How we built it

Front end = React, Backend = Node/express, Python, used deepseek and Gemini AI models

Challenges we ran into

installing libraries, debugging small issues regarding testing independently and getting same results when applying to app

Accomplishments that we're proud of

developed an app with more features, understood how to use AI APIs and variety of Python libraries

What we learned

Using AI APIs, Python libraries and formatting JSON for creating the map

What's next for Mapify

better mind map that digests the summary even further
",https://github.com/RohanT17/bitcamp25,,"Best Web Hack Using React - Peraton, [MLH] Best Use of Gemini API","react, node.js, python, deepseek, gemini",Rohan,Tadisetty,rtadi99@terpmail.umd.edu,,https://github.com/RohanT17/bitcamp25,Best Machine Learning Track Hack,Best Social Good Hack,Best First-Time Hack,Hack That Made You Smile,MOST LIT HACK,"Had fun learning how to use different Python libraries, though it took time to pick the right ones that didn't cause installation issues",University of Maryland,OpenAI and Other,I used Gemini's and Deepseek's AI API for creating the summaries from transcripts and for the chatbot given some prompting context.,816481971663,"",University of Maryland - College Park,2,Thushan,Ranasinghe,thushanr5@gmail.com,abhijay,rao mallya,abhirao@terpmail.umd.edu
Untitled,"",Draft,Pending,Manage team,04/12/2025 00:09:38,"","",,"","",Sion,Marak,www.ssmsion@gmail.com,,"","","","","","","","","","","","",Northern Virginia Community College,0
i am struggle,"",Draft,Pending,Manage team,04/12/2025 00:24:55,"","",,"","",Carolyn,Lee,ckl62015@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,3,Dilan,Kaza,dkaza0001@gmail.com,Bryan,Gao,brybryboi215@gmail.com,Michelle,Chung,michellehychung@gmail.com
StudyRight,"",Draft,Pending,Project overview,04/12/2025 02:56:22,"## Inspiration
Let’s be honest — we’ve all been there. Mindlessly doom-scrolling through short-form videos for hours. In fact, the average TikTok user spends around 95 minutes a day on the app — and for many college students, that number climbs to 3–4 hours daily.

And as software engineers or college students, we’ve all had that classic moment: sitting in class thinking, ‘I’ll just study this later.’ Then later comes… and the regret sets in.

That’s where StudyRight comes in. We’ve built a tool that turns your study notes into engaging, AI-generated short videos — tailored for your short attention span. Instead of fighting your content addiction, we’re using it to your advantage.

Learn smarter, not harder — with StudyRight.

## What it does
StudyRight takes lecture notes or study materials provided by the user and uses AI to generate short, visually engaging explainer videos. These videos are designed in the style of popular short-form content, making it easier for students to retain information in a format they’re already addicted to. Users can input their notes, choose a style or topic focus, and get back a digestible, entertaining video that helps them study on the go — whether they’re commuting, procrastinating, or just in need of a quick review.

##How we built it?

## Challenges we ran into
One of the biggest challenges we faced was integrating multiple technologies — from frontend video input to backend AI processing — into a seamless user experience. Converting raw lecture notes into coherent, engaging video scripts required fine-tuning our AI prompt design to get consistent, high-quality outputs. We also ran into issues with video generation speed and file handling, especially when dealing with large media assets and cloud storage through AWS S3. Debugging compatibility between services and ensuring everything ran smoothly across different devices and environments definitely pushed us to think creatively and troubleshoot efficiently.

## Accomplishments that we're proud of

What we learned**

## What's next for StudyRight
We envision expanding StudyRight into a mobile app, allowing students to learn anywhere, anytime — whether on a commute or in between classes. We also plan to add collaboration and sharing features so study groups can upload notes together and learn from each other’s videos. Additionally, personalized learning paths will use AI to analyze users' habits and content to recommend tailored study videos, helping students focus on what they need most.
","",,"Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of Gemini API","",Aditya,Dwivedi,aditya.vd2002@gmail.com,,https://github.com/arnavdeepaware/StudyRight,Best App Dev Track Hack,Best Bitcamp Hack,Best Moonshot Hack,Prettiest Hack,Hack That Made You Smile,"",City University of New York,"","","","","CUNY Queens College, Queens College of the City University of New York, City College of New York of the City University of New York, The",2,Ankit,Kumar,kumarankit512@gmail.com,Arnav,Deepaware,arnav.deepaware@gmail.com
OutTheGC,https://bitcamp2025.devpost.com/submissions/642106-outthegc,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 03:28:11,"Inspiration

We built OutTheGC to solve a problem almost every friend group runs into: making plans in the group chat that never actually happen. Messages get buried, decisions drag on, and good ideas fade away. OutTheGC turns those scattered conversations into real plans by giving friends a simple, organized way to coordinate events together.

What it does

OutTheGC is a collaborative event planning platform that makes organizing with friends easy and actionable. Users can create or join friend groups through invitation links, suggest and plan events, and build out details like location, time, and budget. Events sync directly with Google Calendar, and upcoming plans are displayed within each group for better visibility and follow-through.

How we built it

We built OutTheGC using Next.js for the frontend and Firebase for authentication and real-time data storage. Our UI was designed in Figma with a focus on simplicity and ease of use. We integrated the Google API for seamless calendar syncing and used the Gemini API to help generate activity ideas and planning suggestions.

Challenges we ran into

One of the biggest challenges was integrating with the Google Calendar API, which involved navigating a complex system of permissions, scopes, and OAuth configurations. We also spent time refining the user interface to strike the right balance between functionality and simplicity, ensuring it supported collaboration without overwhelming the user.

Accomplishments that we're proud of

We're proud to have built a complete first version of OutTheGC, bringing together collaborative planning tools, calendar syncing, and smart suggestions in one cohesive platform. Successfully integrating multiple APIs into a smooth, unified experience was a major milestone. Most importantly, we’re proud that the platform is something we genuinely want to use with our own friends.

What we learned

Throughout this project, we gained experience working with Firebase and third-party APIs in a collaborative dev environment. We also learned how to approach product design with a user-first mindset, and how to keep our feature set focused to avoid unnecessary complexity while still solving the core problem.

What's next for OutTheGC

We’re excited to keep growing OutTheGC. Next steps include integrating payment platforms like Venmo or PayPal to simplify cost splitting, adding push notifications to keep users informed, and developing a mobile version so planning can happen on the go.
","",,[MLH] Best Use of Gemini API,"typescript, google, firebase, figma, gemini-api, next.js",Celestine,Sumah,csumah1@umbc.edu,,https://github.com/csumah/pjarcs-bitcamp-2025,Best Bitcamp Hack,Best UI/UX Hack,Best Social Good Hack,MOST LIT HACK,Prettiest Hack,"This weekend, we worked primarily with Firebase, the Google Calendar API, and the Gemini API. Firebase was smooth for authentication and data handling, but integrating Google Calendar was a bit tricky due to OAuth setup and permission scopes.","University of Maryland, Baltimore County",Gemini,"Yes, I implemented the Gemini API for AI suggestions on trips and cost breakdowns.",561798945255,"",University of Maryland - Baltimore County,2,Ashley,Rabino,ashleyjanelle.rabino@yahoo.com,Bella,Goltser,igoltse1@umbc.edu
thinking,"",Draft,Pending,Project details,04/12/2025 03:43:44,"Inspiration

Money management apps often feel passive — they tell you what you’ve spent but don’t actively protect or coach you. We wanted to create a smart, proactive companion that doesn’t just track your spending but shields you from it — flagging risky habits, ghost expenses, and giving actionable financial coaching through GenAI.

That’s how FinGuard was born: your AI-powered spending shield.

What it does

Connects to Capital One’s Nessie API to simulate bank accounts, bills, and transactions
Detects ghost expenses like idle subscriptions or duplicate charges
Uses Gemini API + Cloudflare Workers AI to offer personalized savings tips and financial advice
Visualizes spend patterns, upcoming bills, and progress toward savings goals
Lets users set smart budget challenges and gamifies their financial journey

How we built it

Challenges we ran into

Thinking......

Accomplishments that we're proud of

Learning so much. Always grateful!!

What we learned

What's next for FinGuard
","",,"",react,Nidhi,Kamath,kamathnidhi98@gmail.com,,"","","","","","","","","","","","",Virginia Tech,0
LifexAI,https://bitcamp2025.devpost.com/submissions/642116-lifexai,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 04:31:40,"Inspiration

In today's complex financial landscape, many people struggle to make sense of their spending habits and financial health. Traditional banking apps often present raw data without meaningful insights. We were inspired to create LifexAI after realizing that people need more than just transaction lists, they need a financial detective that can analyze patterns, identify anomalies, and provide actionable recommendations to improve their financial well-being.

What it does

LifexAI transforms raw financial data into meaningful insights through:

Financial Dashboard: Provides a comprehensive overview of all accounts and balances
Transaction Tracking: Categorizes and displays transactions in an intuitive interface
Spending Analysis: Visualizes spending by category with interactive charts
Anomaly Detection: Identifies unusual transactions that may require attention
Financial Health Scoring: Calculates a personalized score with tailored recommendations
Recurring Expense Identification: Automatically detects subscriptions and regular payments
Spending Trend Visualization: Shows spending patterns over time to identify trends

How we built it

We built LifexAI using a modern tech stack:

Next.js & React: For a responsive, component-based frontend
TypeScript: For type safety and improved developer experience
Chart.js & React-ChartJS-2: For interactive data visualizations
Framer Motion: For smooth animations and transitions
Tailwind CSS: For efficient, responsive styling
MongoDB: For data storage and retrieval
Capital One Nessie API: For banking data integration
React Query: For efficient data fetching and state management

Challenges we ran into

Financial Data Integration: Working with the Nessie API required understanding complex financial data structures
Anomaly Detection Algorithm: Developing an effective algorithm to identify unusual transactions without false positives
Performance Optimization: Ensuring smooth performance with real-time data processing and visualizations
Responsive Design: Creating a dashboard that works seamlessly across all device sizes
User Experience: Balancing comprehensive data with an intuitive, non-overwhelming interface
Hosting: Deployment errors caused a lot of problems as we were unable to build it successfully on the server. But we got through it at the very last minute.

Accomplishments that we're proud of

Intuitive Financial Dashboard: Created a clean, information-rich interface that makes complex financial data accessible
Effective Anomaly Detection: Implemented a statistical approach that successfully identifies unusual spending patterns
Comprehensive Financial Health Score: Developed a holistic scoring system that considers multiple financial factors
Seamless Dark/Light Mode: Implemented a fully responsive theme system for user preference
Polished Animations: Added subtle animations that enhance the user experience without being distracting

What we learned

Financial Data Analysis: Gained expertise in processing and analyzing financial transaction data
Statistical Methods: Learned to apply statistical approaches for pattern recognition and anomaly detection
UX Design for Financial Tools: Discovered effective ways to present complex financial information
API Integration: Improved our skills in working with external financial APIs
Performance Optimization: Learned techniques for optimizing data-heavy applications
Hosting: Learned the issues with building on platforms like Vercel and adding CName, SSL from different sites and connecting it all together.

What's next for LifexAI

AI-Powered Insights: Implementing actual machine learning models for more sophisticated financial analysis and predictions
Predictive Spending Forecasts: Adding capabilities to predict future expenses based on historical patterns
Health Monitoring Integration: Expanding to track and analyze health metrics alongside financial data
Digital Safety Features: Adding tools to monitor and protect users' digital footprint
Natural Language Processing: Implementing a conversational interface for users to ask questions about their finances
Expanded Financial Planning Tools: Adding budget creation, goal setting, and investment tracking features
",https://www.lifexai.tech,,"Best Hack Promoting Public Health - Bloomberg, Best Financial Hack - Capital One, Best Digital Forensics Related Hack - Cipher Tech Solutions, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best AI Application Built with Cloudflare, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","react, terp, cloudflare, mongodb, mlh, gemini, .tech, vercel, tailwind, css3, nessie, github, nextjs",Nidhi,Kamath,kamathnidhi98@gmail.com,,https://github.com/nidhikamath2102/lifexai,Best App Dev Track Hack,Best UI/UX Hack,Best Social Good Hack,Prettiest Hack,Hack That Made You Smile,"Explored and implemented Cloudflare, Gemini api",Virginia Tech,"OpenAI, Anthropic, and Gemini","No, we used the Capital One API which was already provided to us!",AIzaSyDt54Fz9uf_pQFk3CVRalygp4yxhJ97Rj0,https://www.lifexai.tech/,Virginia Tech,2,Rajat,Nagar,rajatnagarr@gmail.com,Kunal,Mahato,kunalmahato@vt.edu
FinePrintasaurus,https://bitcamp2025.devpost.com/submissions/642156-fineprintasaurus,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 07:43:18,"Inspiration

According to The Economic Policy Institute, over 60 million U.S. workers are bound by forced arbitration, where they are unable to settle disputes outside of the company, with non-unionized, low-wage, and minority workers being the most affected. With this in mind, we developed FinePrintasaurus: a user-friendly app that helps users understand vaguely-worded contracts. 

As college students beginning to enter a corporate world, we knew we’d soon be signing papers with conditions we didn’t understand. We hope that FinePrintasaurus can protect us, our peers, and our community from vague legal terms and deceptive fine print. 

What it does

Users are able to upload pdf files of legal documents, contracts, and/or terms of services onto the app and extract the crucial information in a quick, digestible manner that requires no legal expertise. FinePrintasaurus summarizes the involved parties, the most important clauses that the user should know, and the consequences of terminating/breaching the contract. 

There also is a login feature where users can save the different contracts they have uploaded and have all of them recorded in a table on the home page. If they wish to get rid of it, they can simply delete it by pressing the X button. 

Throughout the entire process, we prioritized making our user interface as minimalistic and intuitive as possible for increased accessibility. 

How we built it

Technologies for frontend: React, TypeScript, Next.js. Technologies for backend: MongoDB, GeminiAPI,Node.js, Expressjs. For deployment Vercel was used for the frontend and Render was used for the backend.

Challenges we ran into

There were multiple issues that we ran into over the course of the project. One issue that we ran across was CORS errors because the back-end server did not like the front-end pulling from it. There were also limitations with the Gemini API that we were using which made it difficult to scrap a whole file, needing it to be parsed. The original js-based parser that we were using also did not work properly so we had to pivot to a python based one.

Accomplishments that we're proud of

During this hackathon, we were able to implement a deployable website on Vercel (front-end) and render (back-end) that utilizes a login feature that saves user progress onto MongoDB. Before Bitcamp, we didn’t have much hackathon experience and had never used MongoDB or Gemini API. Despite this, we were able to use our own personal strengths to contribute to different aspects of the project. From the little victories to the late-night conversations, we’re proud to say that we will have walked away from this event knowing that we gave it everything we had.

What we learned

We learned how to use MongoDB atlas to store our information. We learned how to use GEMINI API, and how to create user authentication with JWT. We learned how to debug and deploy web applications. We also learned how to distribute tasks and communicate with a team.

What's next for FinePrintasaurus

With Generative AI being a relatively new development, our parsing isn’t the most accurate. But as time passes, Generative AI will continue to evolve, as will the accuracy of our legal summaries. 

As for future developments of FinePrintasaurus, we understand the significance of the language barrier, and so in future cases, we aim to reach a wider-audience by implementing linguistic APIs that can fluently translate these terms. 

Though it’s only been 36 hours, we’re extremely proud of what we’ve done. And we hope that this app will be able to support many users in making informed decisions on contractual agreements. 

Thank you, and this has been FinePrintasarus! 
",https://bitcamp-2025-frontend.vercel.app/login,https://youtu.be/CqvCT7MCk1Y,"Best Web Hack Using React - Peraton, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","react, typescript, gemini, mongodb, python, node.js, next.js, express.js",Amber,Li,amberl18@terpmail.umd.edu,,https://docs.google.com/document/d/14Rp3xlyiz5mHdYO27-GiK1RRypC_o4FVqxVkvjoMzP4/edit?usp=sharing,Best App Dev Track Hack,Best Social Good Hack,Best First-Time Hack,Prettiest Hack,Hack That Made You Smile,"Having not used MongoDB before, we thought it was incredibly easy to pick up and the UI was easy to understand. We liked that we were using GitHub since everyone on the team was familiar with the platform and it allowed for us to test before we actually committed any changes to our code, it also makes it easy to revert the code back.","University of Maryland, College Park",Gemini,"Yes, we implemented the Gemini API in our hack. We used the API because we needed something that would be able to read in the contract pdfs and extrapolate information from them. We used Gemini to be able to get a summary and the key information into a more easy to understand format compared to the contract.",373659128837,"",University of Maryland - College Park,3,Rachel,Li,rli8416@terpmail.umd.edu,Madison,Lawson,mlawson4@terpmail.umd.edu,Neal,Harricharan,nealharricharan@gmail.com
FoodShare,https://bitcamp2025.devpost.com/submissions/642189-foodshare,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 10:04:48,"Inspiration

Every year, millions of Americans struggle with food insecurity, especially those in low-income urban areas. With our group being from the Baltimore area, we've seen the causes and effects of urban food insecurity firsthand. Although Baltimore is home to a plethora of food pantries and food-based non-profits, residents struggle to find free or affordable food options. Because of poor funding, small non-profits struggle to get the word out about what they do. As a result, residents are left hungry and organizations aren't serving as much people as they could, leaving them with underwhelming numbers on grant applications. The system is broken and we're here to fix it. 

What it does

We answer the fundamental question: Where can I go to get free food right now?
Our user-friendly interface displays all of the food pantries that are open nearby and provides filters that curate the results to the needs of the user. Users can easily filter for locations that are in walking distance, locations that offer delivery, and locations that offer a specific food type such as meat or produce. 

How we built it

The front-end of the web application was built with React.JS. The back-end was built with Express.JS. Our dataset was retrieved and stored in Python with Pandas. We utilized the Google Maps API for map-related features and Firebase for user authentication. We also developed a prototype of a mobile version of the app in Figma.

Challenges we ran into

Incorporating the various aspects of the project from the front-end, to the back-end, to the data was difficult. Implementing the Google Maps API was especially challenging, as was user authentication with Firebase.

Accomplishments that we're proud of

We're proud of our UI design in both the web application and the mobile prototype. We're also proud of the practical use of our project and the promise that it has as a potential business venture in the future. Overall, we're very pleased with what we were able to accomplish in a weekend.

What we learned

We learned that the process of developing an application like ours is exhaustive and requires much troubleshooting and collaboration. We also learned that through the use of AI technologies like ChatGPT, Gemini, and Repl.it, the development process is significantly smoother. 

What's next for FoodShare

In the future, we'd like to add an admin panel where non-profit owners could directly modify the information that's displayed about their non-profit on the app. We want to fully implement the app's social ecosystem, allowing users to leave ratings and reviews on food pantries. Hopefully, this will motivate the pantries to improve their food and service quality as a result. Lastly, we'd like to develop the mobile application to resemble the application displayed in our prototype. 
",https://www.figma.com/proto/lh0Op2HVyF6LXSRkk0xBF2/App-Mockup?node-id=1-4&t=GQZeS5OG0Cqj3qqS-1&starting-point-node-id=1%3A4,https://www.youtube.com/watch?v=KUpmGvsCIpU,Best Web Hack Using React - Peraton,"react.js, figma, express.js, pandas, google-maps, firebase",Brandon,Isbell,brandonisbell@gmail.com,,https://github.com/isbellbr/FreeFoodFinder,Best Social Good Hack,Best UI/UX Hack,Best Bitcamp Hack,Hack That Made You Smile,Prettiest Hack,We enjoyed developing our prototype in Figma and collaborating with each other through GitHub.,"University of Maryland, College Park",Gemini,N/A,N/A,foodshare.tech,"University of Maryland - College Park, Baltimore Polytechnic Institute",3,Luis,Valerio,lvalerio@terpmail.umd.edu,k8816,,kzkz8816@gmail.com,Josh,Tagle,joshtag05@gmail.com
Quaran.Tech,https://bitcamp2025.devpost.com/submissions/642194-quaran-tech,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 10:08:37,"Inspiration

During major public health events, the public is often not aware of the duties of public health workers such as contact tracers, decreasing the efficiency of procedures such as public health mandates and ""flattening the curve."" This game aims to educate the public on various public health risks and how they are dealt with. 

In addition, Efthimios Papadopoulos is an avid gamer and it was noticed that current games may become boring after the storyline has been exhausted. This game is an experiment in the use of large language models to enhance game experiences through the generation of alternate storylines.

What it does

Quaran.Tech is a cross-platform HTML5 game that allows players to experience the life of a public health specialist in a fast-paced environment. When days are shortened to in-game seconds, the more tedious parts of this occupation are eliminated, and we are left with the most intense and nerve-wracking decision-making.

How we built it


HTML/CSS/JS/jQuery was used to develop the game frontend which is hosted on Cloudflare Pages
The backend runs on Cloudflare Workers ensuring scalability for the long run.
Gemini API is used to generate unlimited new scenarios for players to experience.
MongoDB stores existing scenarios.
The .TECH domain name was utilized.


Challenges we ran into

Making a game without a game engine was more difficult than we thought. Without the abstractions of scenes and sprites, mundane commands such as showing and removing icons sometimes led to strange, difficult, and random errors.

Time was a serious constraint. We initially did not think of the LLM integration, but when we started writing the game storyline file, a new appreciation for game development was gained as we realized that writing storylines was extremely time-consuming.Therefore, we decided to integrate the AI story writing into our game, which accelerated development of the storyline, but slowed us down due to the added complexity.

...and finally, CORS issues. A lot of CORS issues.

Accomplishments that we're proud of

We're proud that we built a game in the short timeframe provided. Efthimios had never written a game before of any sort, and Heng had only experimented with some small games in easy-to-use game engines. In addition, it was our first time integrating Google Gemini into an application. 

What we learned

Efthimios learned a lot of HTML and CSS techniques. From zero to hero type ah learning 😤😤😤.

Heng Ye learned the intricacies of developing game software. In addition, he learned how to utilize Gemini API, Cloudflare Pages, and Cloudflare Workers.

What's next for QuaranTech


Voice chat with AI characters (doctors, patients, government officials)
More reliable LLM storyline generation.

",http://quaran.tech,,"Best Hack Promoting Public Health - Bloomberg, [MLH] Best Use of .Tech, [MLH] Best AI Application Built with Cloudflare, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","javascript, cloudflare, mongodb, gemini, html5, css3",Heng,Ye,heng@yeheng.org,,https://gitlab.com/HengYeDev/bitcamp2025,Best Gamification Hack,Best Razzle Dazzle Hack,Best Bitcamp Hack,Hack That Made You Smile,Prettiest Hack,"We used MongoDB, GitLab, Google Gemini,  Cloudflare Pages, and Cloudflare Workers.",Smithtown High School West,Gemini,"Yes, we used Gemini to allow users to generate new storylines in the game.",147455385305,Quaran.Tech,Smithtown High School West,1,Timmy,Papadopoulos,timmypapadopoulos20@gmail.com
Untitled,"",Draft,Pending,Manage team,04/12/2025 10:09:51,"","",,"","",Tong,Zhan,tongzhan706@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,0
Untitled,"",Draft,Pending,Manage team,04/12/2025 10:13:02,"","",,"","",Nuna,Klu,nunaklu1@gmail.com,,"","","","","","","","","","","","",University of Maryland - Baltimore County,0
SmartSplit,"",Draft,Pending,Project details,04/12/2025 10:37:49,"Inspiration

We created SmartSplit because we noticed that college students often struggle with budgeting, tracking expenses, and saving money in an easy, motivating way. We wanted to build a tool that could help make managing finances simple, social, and even a little fun.

What it does

This was our first hackathon and the first time any of us worked with React and Git in a team setting. Throughout the project, we also learned how to set up a real-time database using Firebase, and how to connect user input to live data. Seeing the app update instantly as users entered their goals or expenses was a huge milestone for us.

How we built it

We built the project using React for the frontend and Firebase for the backend database. We used Together AI to generate smart financial advice based on user spending patterns. The app features goal tracking, budget tracking, an accountability circle with friends, and an interactive dashboard to make managing money approachable.

Challenges we ran into

A lot of the tools and technologies were new to us. Setting up Firebase and getting it to talk properly with our React components took time. We also had to figure out how to collaborate efficiently with Git, manage merges, and combine everyone’s work into one project. Debugging React state changes and making sure the data flowed correctly between components was another big challenge, but we learned a lot by troubleshooting together. 

Accomplishments that we're proud of

This is our first hackathon as a team and we are proud to complete and submit a project for judging. We are proud of successfully integrated react.js framework which is one we have little experience with. We are proud of effectively collaborating on github learning many of the key functions on the fly. Also, learning version-control with Git was challenging but will be extremely useful for our future careers.

What we learned

We learned how to effectively integrate AI into our application using API keys and were able to setup a database using firebase. Although we knew a bit of react, we expanded our knowledge of the framework over the past 36 hours. We also were able to expand our knowledge on GitHub features, such as pushing, pulling and merging while working in a team. Finally, we learned the importance of helping each other debug problems and work together when things weren't working as planned. 

What's next for SmartSplit

In the future, we can create a mobile app that allows users to input their daily expenses and spending goals on the go. We would also like to host the website on a server so that everyone can view and access our product online. Finally, we could also train our AI integration to give more personalized budgeting advice based on user habits. 
",https://github.com/15ANURAG34/SmartSplit,,"","react, firebase, html, css, javascript, api, json",Anurag,Pokala,anuragp34@vt.edu,,"","","","","","","","","","","","",Virginia Tech,3,Ritvik,Navil,rknavil@gmail.com,Nikhil,Netto,nikhil.c.netto@gmail.com,Parth,Mehta,parthpm06@gmail.com
zombs.tech,https://bitcamp2025.devpost.com/submissions/642217-zombs-tech,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 10:45:53,"🧟‍♂️ zombs.tech – A First-Person Zombie Survival Web Game

🎮 Inspiration

When I was a kid, I was obsessed with browser-based first-person shooters like ShellShockers and Krunker.io. They weren’t just games—they were friendship factories, moments of joy, and core memories. I've always dreamed of creating my own FPS web game that could give someone else that same thrill. Throw in some Doom inspiration, and boom—zombs.tech was born.

⚔️ What It Does

zombs.tech is a fast-paced, first-person shooter where you take down waves of zombies and slimes, leveling up as you go and chasing that high score. Every wave gets harder. You’ll dash, updraft, and shoot your way through chaos, racking up kills and dodging enemy fire like a boss.

🛠️ How I Built It

I built this beast using JavaScript with Three.js to handle all the 3D magic. It’s running on a web-based HTML framework and features a custom UI/UX design that brings the whole thing to life. Every model, mechanic, and texture was hand-crafted, tested, and refined for smooth gameplay.

😵‍💫 Challenges I Ran Into

BRO. THERE WERE SO. MANY. BUGS.


Texturing? Took ages to figure out how to make textures attach.
Projectiles? Had to learn raycasting just to figure out if a bullet hit an enemy or me.
Walls? My character kept phasing through them for TWO HOURS until I coded collision physics from scratch.
Map creation? Every wall had to be placed by hand with specific coordinates—ugh.
UI/UX? Total overhaul midway through because the original was trash.
Testing? I wrote actual unit tests for a web game just to track down sneaky bugs.


But I pushed through, and I’m so freaking proud of where it landed.

🏆 Accomplishments I'm Proud Of

I packed in a TON of features in a short time, and now I feel like I’ve truly mastered Three.js. Designing and building this game is second nature to me now. Here's what’s in the game:

Core Gameplay:


🔫 FPS mechanics with WASD + mouse look
🧟‍♂️ Wave-based survival
⚔️ Two enemy types: melee zombies & ranged slime shooters (from wave 3)
❤️ Health system (100 HP)
🧮 Score and wave progression system


Player Abilities:


Dash (E) – 6s cooldown
Updraft (Q) – 8s cooldown


Enemies:


Zombies chase and hit you up close
Ranged enemies shoot green projectiles, can't shoot through walls, have smarter AI (150 HP!)


Environment:


3D map with full collision detection
Projectile collisions
Minimap and spawn point systems


UI Elements:


🚀 Start screen with title, instructions, credits, etc.
In-game HUD:


Health bar
Score tracker
Wave progress
Cooldown indicators
Crosshair

🪦 Game over screen
⚙️ Settings menu:


Mouse sensitivity
Crosshair style
Music & SFX volume sliders
Mute all option



Visual Effects:


Gun animations, glowing bullets, enemy death animations
Hit markers, score pop-ups, and health bar color changes
Damage screen flash for that extra drama


Audio System:


Background music
Laser shots, zombie groans, dash and updraft sounds
Full volume control and mute options


📚 What I Learned

Three.js & 3D Graphics:


Scene setup, camera movement, lighting, materials
Texture mapping and UVs
Raycasting and hit detection


Advanced JavaScript:


Game loop logic and timing
OOP with game classes
State management & event systems
Vector math and 3D calculations


Browser APIs:


requestAnimationFrame for performance
Pointer Lock API for smooth FPS controls
Web Audio API for all the juicy sounds
Canvas rendering for the minimap


UI/UX Development:


Built fully responsive menus and in-game HUDs
Added animations, transitions, and visual feedback for all game states


Game Dev Concepts:


Wave-based progression and enemy balancing
Cooldown-based abilities
AI behaviors with pathfinding and attack logic


Physics & Math:


Collision systems for players, enemies, and bullets
Movement and camera rotation in 3D space


Development Best Practices:


Well-structured code with clear game states
Performance tuning and resource management
Debugging and writing unit tests for game logic


🚀 What’s Next for zombs.tech

Oh man, I’ve got big plans:


Multiplayer battles (1v1s, team fights, co-op survival)
Boss fights with crazy mechanics
New enemies, new maps, and new game modes
Even smoother movement and combat mechanics
Ability to choose your own music and sound packs
Graphics upgrade (thinking shaders & better models)
Leaderboards, account systems, and more polish all around




Thanks for checking out zombs.tech! If you love browser shooters or just want to blast zombies for fun, I hope this game brings you the same joy that inspired me to build it 💥🎯
",https://zombs.tech/,https://youtu.be/Eerwxn2F-fs,[MLH] Best Use of .Tech,"three.js, javascript, html5, css3, requestanimationframe, canvasapi, pointerlockapi, domapi",Josh,zheng,joshzcamp2018@gmail.com,,https://github.com/JoshuaZheng0/zombs.tech,Best First-Time Hack,Best UI/UX Hack,Best Razzle Dazzle Hack,Hack That Made You Smile,MOST LIT HACK,"I worked with Three.js during the hackathon, and overall, it's a great tool for creating browser-based 3D games. It offers an easy setup for scenes, cameras, and renderers, and performs well even with multiple moving objects. Features like PointerLockControls and raycasting made it easy to implement FPS mechanics and hit detection. However, there is a learning curve with 3D math and debugging, and mobile performance can be inconsistent. While Three.js lacks a built-in physics engine, the library's robust documentation and community support helped overcome challenges. Overall, Three.js proved to be a powerful tool for creating engaging 3D game experiences in the browser.",University of Maryland - College Park,OpenAI,"Yes, I used a music api and some others.",N/A,"https://zombs.tech/

THIS IS COOL! PLEASE LET MY WEBSITE REMAIN ON THE WEBFOREVER!","",0
 TerpMatch: LLM-Powered Research Connector,https://bitcamp2025.devpost.com/submissions/642231-terpmatch-llm-powered-research-connector,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 11:11:28,"Inspiration

At the University of Maryland, opportunities for undergraduate and graduate students to work with professors on research or academic roles like TA and grader are everywhere — but the process of finding the right match is fragmented, informal, and often based on word-of-mouth. We wanted to solve this using AI.

What if we could help students intelligently discover professors whose research and open roles truly align with their interests and skills? That’s how TerpMatch was born — a matchmaking platform tailored for UMD that uses cutting-edge LLMs to bridge the gap between students and faculty.



What it does

TerpMatch matches students with professors based on:


Desired role (RA, TA, Grader)
Research interests
Resume-based technical skills
Gemini LLM-inferred skill requirements based on each professor’s research areas


Students fill out a quick profile and upload their resume. TerpMatch intelligently recommends professors with high compatibility scores. Professors, on the other side, can see students who applied to them, ranked by match score.

It’s like LinkedIn meets AI — but built specifically for Terps 🎯



How we built it


Frontend: React.js with Axios for REST API integration and resume upload
Backend: Flask (Python) handles user data, resume management, and scoring logic
Database: MongoDB stores student and professor profiles
LLM Integration: Google Gemini API was used to infer technical skills from research interest strings
Matching Logic: Compatibility is calculated based on role match, research overlap, and skill alignment (LLM-enhanced)
Resume Handling: Uploaded resumes were stored and converted to clean .txt and .pdf formats dynamically




Challenges we ran into


Setting up and integrating the Gemini API for large-scale skill inference was tricky due to rate limits and output parsing.
Designing a fair and balanced compatibility scoring system that made sense for both students and professors.
Making sure resume uploads and LLM-powered inference remained performant and didn't overload the backend.
Merging frontend/backend cleanly while keeping the app lightweight and responsive.




Accomplishments that we're proud of


Built a full-stack AI-powered matchmaking engine from scratch within Bitcamp
Successfully integrated Gemini LLM to infer skills from raw research interests
Designed a dual-view system for both students and professors
Created polished resume generation, scoring, and filtering capabilities — all hosted and demo-ready
Delivered something that could genuinely benefit the UMD academic community




What we learned


How to structure a full-stack application using React + Flask + MongoDB
How to integrate LLM APIs into traditional software systems
How to handle resume parsing and formatting at scale
The power of semantic matching vs. keyword-based filtering
The potential of AI in making academia more accessible and collaborative




What's next for TerpMatch: LLM-Powered Research Connector


Add student-facing ""Top Matches"" dashboard with live updates
Let professors customize skill requirements and use Gemini for custom role analysis
Deploy to a public cloud platform (e.g., Streamlit Cloud, Render, or Railway)
Integrate UMD authentication (CAS or SSO)
Add a Gemini-powered chatbot to help students ask questions like “Which profs do ML in healthcare?”


We're excited to see where TerpMatch goes — and how it can help make UMD's vibrant academic network even stronger. 🐢
",https://github.com/Rishabh000/Bitcamp-2025,https://youtu.be/HsGi8SH34CM,"Best Hack Promoting Public Health - Bloomberg, Best Financial Hack - Capital One, Best Digital Forensics Related Hack - Cipher Tech Solutions, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best AI Application Built with Cloudflare, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","react.js, flask, mongodb, google, gemini, geminiapi, pymongo, flask-cors, tqdm, python, javascript, css, html, llm",Rishabh,Ranka,rranka@umd.edu,,https://github.com/Rishabh000/Bitcamp-2025,Best Machine Learning Track Hack,Best First-Time Hack,Best Bitcamp Hack,Hack That Made You Smile,Prettiest Hack,"This weekend was an incredible deep dive into the full-stack and AI ecosystem. We primarily worked with MongoDB, Google Gemini API, React, Flask, and GitHub. MongoDB was smooth to integrate with Flask using PyMongo, and its flexible schema made it easy to iterate on our student-professor matching models. The real highlight was exploring Google's Gemini LLM — integrating natural language understanding into our app using Gemini’s API to infer skills from unstructured research topics was a game-changer. The response quality was impressive, and the API was well-documented, although rate limiting did require us to batch intelligently. Using GitHub for version control across frontend and backend teams helped us collaborate seamlessly. Overall, every tech stack had its learning curve, but they worked together beautifully to bring our vision for TerpMatch to life.",University of Maryland,"OpenAI, Gemini, and Other","We implemented a generative AI API in our hack this weekend using Google's Gemini API. We used it to infer relevant technical skills based on professors’ research interests. Instead of relying on manually entered skill requirements, we sent natural language prompts like:

“Based on the following research interests: Smart Cities, Cybersecurity, list 5 technical or programming skills a student should ideally have...”

Gemini responded with contextually appropriate skills (e.g., Python, IoT, SQL, Machine Learning), which we used to calculate a more intelligent compatibility score between students and professors. This allowed us to go beyond keyword matching and add true semantic understanding to our matching system.

Using Gemini added a powerful LLM-driven layer to our project and made it feel truly ""intelligent.""",615878040010,"","University of Maryland - College Park, Indian Institute of Information Technology, Vadodara",2,Rishika,Thakre,rishikathakre07@gmail.com,ANISHA,KATIYAR,202051027@iiitvadodara.ac.in
DebateGuard,"",Draft,Pending,Additional info,04/12/2025 11:47:39,"Inspiration

In the age of misinformation and bad-faith debates, many individuals struggle to recognize what a respectful and constructive debate looks like. This misunderstanding often results in the dismissal of meaningful discourse, limiting critical thinking and learning opportunities. DebateGuard offers an intuitive platform for users to engage in moderated debates, promoting clear communication and informed argumentation.

What it does

DebateGuard provides users with a one-on-one video debate platform enhanced by real-time AI moderation. During each debate session, participants receive live transcription and immediate AI-driven analysis, highlighting logical fallacies and encouraging more constructive interactions. Post-debate, participants can review detailed analytics including speaking times, fallacies identified, and comprehensive summaries of their arguments.

How we built it

We built DebateGuard using a robust full-stack architecture centered around Next.js, React, and Node.js to provide real-time functionality and a seamless user experience. For the frontend, we utilized Next.js and Tailwind CSS, creating a sleek, modern, and responsive user interface, enhanced by animations from Framer Motion. React facilitated dynamic, reusable components for clarity and ease of use.

Our backend leverages Node.js along with WebSockets and WebRTC for seamless real-time video call functionality. Real-time transcription is handled using the Web Speech API, while logical fallacy detection and moderation feedback are powered by GPT-4 through strategically crafted prompts. We also integrated Stream React SDK to manage user interactions and streamline communication features. We chose PostgreSQL for reliable data management, integrated seamlessly with Prisma ORM.

Challenges we ran into

A significant challenge was ensuring real-time transcription and AI-driven fallacy detection operated seamlessly within live video calls without noticeable latency. Initially, integrating backend APIs, AI services, and frontend components was complex, but through careful debugging, iterative testing, and WebSocket optimization, we substantially improved responsiveness and overall user experience.

Accomplishments that we're proud of

We're proud of successfully implementing real-time AI moderation that provides immediate, relevant feedback during debates. Additionally, our modern, intuitive user interface significantly enhances user experience. Seamlessly integrating advanced technologies like GPT-4, WebRTC video streaming, and real-time analytics into a cohesive platform is another achievement we take pride in.

What we learned

We gained valuable experience in real-time audio and video streaming, AI integration, and natural language processing. Working with WebSockets taught us efficient real-time data handling, while combining frontend and backend technologies deepened our understanding of full-stack development. Additionally, using GPT-4 and the Web Speech API enhanced our skills in prompt engineering and real-time transcription.

What's next for DebateGuard

Moving forward, we plan to incorporate advanced visual behavior analysis such as gesture and emotion detection to offer deeper insights and enhanced moderation accuracy. We're also exploring a spectator mode, enabling third-party users to observe debates with live AI commentary. Personalization of AI moderation tones and development of user-specific analytics dashboards are further enhancements planned to enrich user experience.
","",,"Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech","node.js, next.js, react, tailwind, framermotion, webrtc, websockets, postgresql, prisma, webspeechapi, openai, streamreactsdk, vercel",Tom,Shimoni,tom.shimoni123@gmail.com,,https://github.com/amanuelabiy/DebateGuard,Best App Dev Track Hack,Best Social Good Hack,Best First-Time Hack,Hack That Made You Smile,Prettiest Hack,GitHub was very useful and helpful for our collaboration on the project.,University of Maryland-College Park,OpenAI,"We used OpenAI's GPT-4 API to serve as a real-time debate moderator. During live video calls, GPT-4 analyzes transcribed speech to detect logical fallacies and poor debate tactics. It then generates context-aware prompts, such as reminders to avoid personal attacks or suggestions to acknowledge the opponent’s argument before responding.",N/A,"","University of Maryland - College Park, George Mason University",3,Thomas,Schlinke,tjschlinke@gmail.com,Amanuel,Abiy,amanuelabiy41@gmail.com,Markose,Tsegaye,markbes493@gmail.com
Mealbot,https://bitcamp2025.devpost.com/submissions/642331-mealbot,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 13:57:50,"Inspiration

Oftentimes college students who live in apartments aren't aware what foods they can make or have access to and as such have poor diets, we sought to help remedy this issue in an easily accessible manner. This is where Mealbot comes in!

What it does

It is a chatbot that answers all questions about meal prepping. 

How we built it

We built the frontend using HTML, CSS and JS and the used Gemini's API to answer the questions.

Challenges we ran into

Trying to integrate the API into the webpage. Figuring out how API's work!

Accomplishments that we're proud of

A working project. 

What we learned

How to work with API's and how to code a chatbot!

What's next for Mealbot

Adding in better functionality with more features.
","",,[MLH] Best Use of Gemini API,"html5, css3, javascript, geminiapi, python, flask",Sweta,Dantu,dsweta@umd.edu,,https://github.com/Iuumi/tasktracker.github.io/tree/website_html,Best Machine Learning Track Hack,Best First-Time Hack,Best UI/UX Hack,Prettiest Hack,"",Had a lot of fun figuring out how to work with API's and integrating it with the front end of our chatbot.,"University of Maryland, College Park",Gemini,Yes! We used Gemini API for making our mealprep chatbot.,646700030367,"",University of Maryland - College Park,2,Megan,Lu,mlu12349@terpmail.umd.edu,Joshua,Akoda,ijakoda2@gmail.com
AlphaMutate,https://bitcamp2025.devpost.com/submissions/642359-alphamutate,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 14:58:07,"Inspiration

We wanted to get more involved with the Drug Discovery research field and thought that this hackathon would be a good chance to try out developing a project to aid that process. The members of the group all thought the Alphafold library was interesting and were interested in implementing an actual service using them.

What it does

Given a protein sequence, our application will direct the user to a website where they can see an interactive 3D model of the closest matching protein that is in the Alphafold database, allowing for incomplete sequence inputs. The application also hosts a Gemini based chatbot that will answer any questions about the protein that was chosen from Alphafold as well as models of some mutations of the protein that may be useful for other applications.

How we built it

We built this application using a Svelte based website and a backend that consists of Gemini and Alphafold API calls.

Challenges we ran into

We were unable to implement many of the things we had planned out, such as using RF-diffusion to implement binder design or MD simulations with the proteins. This was largely due to a lack of clarity in dependencies, along with certain services required to implement those features being very complex or lengthy to implement.

Accomplishments that we're proud of

We are proud of our user interface and the design / structure of the website and the workflow we have created. We are also proud of creating something that could actually be incredibly helpful to various fields of protein research, based on our discussions with experienced researchers in this field.

What we learned

We learned a new style of front-end web development with Svelte, as well as new technologies like AlphaFold and Gen-AI Integration with an interactive element on a website.

What's next for AlphaMutate

Some potential expansions for our project include implementing RF-diffusion to help find ""Binders"" where proteins can be linked to other proteins. We also want to expand our use cases to implement molecular dynamics simulations. Currently our project is catered towards chemistry applications, however using simulation softwares like OpenMD and GROMACS, we can also cater towards different fields of research, like materials science.
",https://github.com/pkuppa11/alphafold-bitcamp,https://youtu.be/rwlSOP8wbDU,"Best Hack Promoting Public Health - Bloomberg, Best Use of GenAI in Business - Cloudforce/Microsoft, [MLH] Best Use of Gemini API","python, javascript, svelte, html5, gromacs, gemini, alphafold",Pranav,Kuppa,pkuppa1@terpmail.umd.edu,,https://github.com/pkuppa11/alphafold-bitcamp,Best App Dev Track Hack,Best Bitcamp Hack,Best Moonshot Hack,Prettiest Hack,Hack That Made You Smile,"GitHub worked as expected
Gemini was very helpful",University of Maryland,Gemini,"Yes, we used it to generate more information about the protein being analyzed and answer any questions the user may ask.",993257364412,"",University of Maryland - College Park,3,Akshat,Alok,akshat.alok2018@gmail.com,Pratham,Ashar,pashar@terpmail.umd.edu,Austin,Thomas,austin08thomas@gmail.com
Sustainable Earth (Eco-Finance),https://bitcamp2025.devpost.com/submissions/642366-sustainable-earth-eco-finance,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 15:12:16,"Inspiration

From climate change and extreme weather to environmental pollution to ecosystem degradation, we wanted to make a difference on the environment using finance. Taking inspiration from Acorn, an app which invests spare change, we wanted to invest small portions of money into eco-friendly companies for clients while also encouraging clients to spend green.

What it does

Sustainable Earth is a banking application which tracks the client’s transactions, either through manually inputting a csv file or connecting with the client’s primary banking app with Plaid, and provides a eco-score for the user as well as other eco-friendly metrics. The user is then provided with recommended steps to take to decrease their carbon footprint and make more sustainable purchases. The user can then search for any product and the application will return the most sustainable option for the user to purchase as well as an eco-score for the product. The next part of the application is rewards. Based on a multiplier system, the user can purchase products based on their rewards credit through companies partnered with Sustainable Earth. In addition, users can invest their rewards money in eco-friendly companies. In the investments page, the user is asked to answer questions for the application to obtain a grasp on the user’s financial situation and risk preference.

How we built it

We first thought of an idea that we all liked. Then we shared a repository and began coding. We used multiple technologies to make our functionalities work. We then created the front end using latest tech like tailwind and react. Then we created and tested the backend code and our prompts for the LLM APIs. After, we connected the frontend and backend using tech like flask and express. Finally we had our friends attempt to break the “product” and fix as many bugs as possible!

Challenges we ran into

Figuring out how to obtain user transaction data was difficult, so we ended up using Plaid. We also had trouble with developing an effective visualization for the user’s active investments, ended up implementing yfinance api to fetch live stock data. Had trouble parsing web-scraped data specifically for application usage, fixed by using Gemini API which did parsing for us instead.

Accomplishments that we're proud of

We're proud of being able to integrate Gemini into our search engine for environmentally product options and green stocks, allowing users to help the environment through everyday tasks like shopping and investing. We are also proud of our UI and the use of Plaid API to retrieve user banking data. Finally, we are proud of our team bonding exercises and our accountability in finishing each part of the project on time.

What we learned

Learned how to use Gemini API for multiple aspects of our project. Learned how to connect Plaid API with application using Plaid’s mock account.

What's next for Sustainable Earth (Eco-Finance)

The next step that we plan to take is to work with banks to sponsor real-money cashback rewards for customers who have environmentally friendly spending habits. We also plan to gamify this experience through achievements, streaks, and a leaderboard encouraging player competition. Combined, these additions will increase player engagement and make a larger impact towards the environment.
","https://sustainable-earthy-24kgorla.replit.app/auth, https://github.com/KaushalGorla/SustainableEarthy",,"Best Financial Hack - Capital One, [MLH] Best Use of Gemini API","python, typescript, react, nltk, tailwind-css, shalcn-ui, nivocharts, tanstackquery, node.js, express.js, flask, pandas, yfinance, chart.js, numpy, plaid, gemini, in-memory-storage, postgresql, beautiful-soup",Bhavesh,Mantrabuddi,bmantrab@terpmail.umd.edu,,https://github.com/KaushalGorla/SustainableEarthy,Best Sustainability Hack,Best First-Time Hack,Best Social Good Hack,Hack That Made You Smile,MOST LIT HACK,"This weekend's hackathon was a fantastic opportunity to work with a variety of cutting-edge technologies. I enjoyed refining a full-stack application using Node.js and Express on the backend and React with TypeScript on the frontend. I particularly appreciated how Flask integrated seamlessly for the stock tracking service, leveraging Python libraries like yfinance and Pandas to deliver real-time data insights. The use of Tailwind CSS and Shadcn UI made styling and component management a breeze, while Nivo Charts and Chart.js provided impressive data visualizations. Overall, the smooth integration of these diverse tools not only enhanced our development workflow but also deepened my understanding of modern tech stacks.","University of Maryland, College Park",Gemini,"We used the Gemini API to rate purchases for environmental friendliness, carbon footprint, and water usage to compute an overall ESG score for users. We also used Gemini in our search engines to find the most sustainable versions of every product so that users are able to create better shopping habits and raise their ESG score. Finally, we used Gemini to find the best green stocks for the user to invest in based on a risk assessment survey.",AIzaSyDP5xs9wfpIj1xqh8ABXDzaarwXMapZIDA,N/A,University of Maryland - College Park,3,Kaushal,Gorla,kgorla@terpmail.umd.edu,Yiyang,Liu,yiyangliu254@gmail.com,Suchith,Rao,suchith@terpmail.umd.edu
Untitled,"",Draft,Pending,Manage team,04/12/2025 15:26:43,"","",,"","",Cassie,Li,starrybright066@gmail.com,,"","","","","","","","","","","","","",0
NeuroScan.AI,https://bitcamp2025.devpost.com/submissions/642379-neuroscan-ai,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 15:33:12,"Try out NeuroScan.AI now on: link

Inspiration

Our journey began with a deep fascination for both neurology and artificial intelligence. We were inspired by the potential to bridge these two fields to create something that could make a real difference in healthcare. The idea of using AI to assist in early brain tumor detection, a critical area where early diagnosis can significantly impact patient outcomes, drove us to develop NeuroScan.AI. We wanted to create a tool that could support medical professionals while making advanced diagnostic capabilities more accessible.

What it does

NeuroScan.AI is an advanced brain tumor classification system that:
Analyzes MRI scans using deep learning
Classifies different types of brain tumors with 92% accuracy
Generates comprehensive medical reports
Provides personalized treatment recommendations using Google's Gemini AI
Creates detailed PDF reports with patient information, analysis results, and treatment plans
Includes a section for doctor's review and approval

How we built it

We built NeuroScan.AI using a sophisticated tech stack:
Deep Learning: TensorFlow 2.15.0 with MobileNetV2 architecture
Web Interface: Streamlit 1.32.0 for the user-friendly web application
Image Processing: OpenCV 4.9.0.80 and Pillow 10.2.0
AI Integration: Google's Gemini AI for treatment recommendations
Report Generation: FPDF2 for creating professional medical reports
Data Processing: NumPy 1.24.3 and scikit-learn 1.4.1
The system was trained on a comprehensive dataset of brain MRI scans, fine-tuning the MobileNetV2 model to achieve optimal performance.

Challenges we ran into

One of our biggest challenges was optimizing the model for accurate tumor detection. We had to:
Fine-tune the MobileNetV2 model on our specific dataset
Process and normalize hundreds of MRI scans
Optimize TensorFlow processing for better accuracy
Balance model complexity with processing speed
Ensure the system could handle various MRI scan formats and qualities
Integrate multiple AI components (MobileNetV2 and Gemini) seamlessly

Accomplishments that we're proud of

We're particularly proud of:
Achieving 92% accuracy in tumor classification
Creating a user-friendly interface that medical professionals can easily use
Successfully integrating multiple AI technologies
Developing a comprehensive reporting system
Building a scalable solution that can be deployed in medical settings
Creating a system that could potentially save lives through early detection

What we learned

Throughout this project, we learned:
The importance of model optimization and fine-tuning
How to effectively process medical imaging data
The value of user feedback in medical technology
The challenges of integrating multiple AI systems
The significance of accuracy in medical applications
How to balance technical complexity with practical usability

What's next for NeuroScan.AI

We envision several exciting developments:
Longitudinal Analysis: Adding a new AI model to track tumor changes over time
Personalized Reports: Further customization of reports based on patient history
Enhanced Accuracy: Continuous improvement of our classification model
Integration: Connecting with hospital systems and electronic health records
Mobile App: Developing a mobile version for easier access
Real-time Processing: Implementing real-time MRI scan analysis
Global Deployment: Making the system available to medical facilities worldwide
Research Collaboration: Partnering with medical institutions for further validation
Our ultimate goal is to make NeuroScan.AI a vital tool in the fight against brain tumors, helping doctors make faster, more accurate diagnoses and ultimately saving lives through early detection and personalized treatment plans.
","https://github.com/anshulg614/NeuroScan.AI, https://neuroscan-ai.onrender.com/",,"","python, tensorflow, google, gemini, ai, google-gemini, opencv, streamlit, numpy, mobilenetv2, scikit-learn",anshulg614,Ganumpally,aganumpally@gmail.com,,https://github.com/anshulg614/NeuroScan.AI,Best First-Time Hack,Best Social Good Hack,Best Sustainability Hack,Hack That Made You Smile,MOST LIT HACK,"Technology Experience Review
TensorFlow & Deep Learning: TensorFlow 2.15.0's robust ecosystem enabled effective implementation of MobileNetV2 for MRI analysis. Pre-trained models and GPU acceleration were invaluable, though model optimization presented challenges.
Google's Gemini AI: Gemini AI's NLP capabilities transformed our project, enabling personalized treatment recommendations. Clear documentation and straightforward integration made it a powerful addition to our tech stack.
Streamlit: Streamlit 1.32.0's Python-first approach allowed rapid development of a professional medical interface with real-time updates, though large file handling required careful management.
OpenCV & Pillow: These libraries provided essential image processing capabilities for MRI preprocessing, with strong documentation and community support facilitating complex implementations.
GitHub: Critical for version control and collaboration, GitHub's features helped maintain code quality and secure handling of sensitive medical data.
FPDF2: Enabled professional medical report generation with flexible formatting and comprehensive documentation capabilities.
Overall: Our tech stack created a sophisticated medical AI application. While integration challenges existed, the components worked well together. Future improvements could include more specialized medical imaging libraries for deeper learning integration. This experience reinforced our belief in AI's potential in healthcare.",University of Maryland - College Park,OpenAI and Gemini,"Yes, we implemented Google's Gemini AI API in our project. We used it to generate personalized treatment recommendations based on our MobileNetV2 model's brain tumor classification results.
Implementation: Integrated Gemini AI into our treatment recommendations module to process tumor classifications and patient data, generating medically-relevant treatment suggestions.
Why Gemini AI:
Advanced medical terminology understanding
Context-aware recommendation generation
Seamless Python integration
Reliable API with strong documentation
Personalized medical advice capabilities
Results: Enhanced our diagnostic tool with professional-grade treatment recommendations and comprehensive medical reports, adding an intelligent layer to complement our tumor detection system.",1077526998862,"",University of Maryland - College Park,3,Rahul,Anantuni,anantuni.rahul@gmail.com,Rishi,Modugu,rmodugu@terpmail.umd.edu,Sahil,Nilkanthache,nilkanthache@gmail.com
Untitled,"",Draft,Pending,Manage team,04/12/2025 15:36:21,"","",,"","",Jeffery,Liu,jeffery.z.liu@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,0
BiasGPT,https://bitcamp2025.devpost.com/submissions/642384-biasgpt,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 15:40:29,"BiasGPT: An Automated Demographic Bias Detection System

Inspiration

The idea for BiasGPT was born out of a desire to explore and mitigate demographic bias in AI-generated content. As language models become more integrated into products and systems, it’s essential to understand how subtle shifts in race, gender, or other identity traits can influence the tone or content of generated responses. I wanted to build a tool that would not only detect these shifts but also learn from them over time.

What I Built

BiasGPT is an end-to-end system that:


Takes a prompt and generates a demographically-swapped version using spaCy and a custom dictionary.
Scores the ""tone shift"" between the original and swapped prompt using OpenAI’s GPT-3.5.
Labels responses based on the shift and stores them in a dataset.
Retrains a DistilBERT classifier nightly to improve its bias detection capabilities.
Includes a React frontend and a FastAPI backend for live interaction.


Tech Stack:


Frontend: React + Tailwind CSS (Next.js)
Backend: FastAPI
ML/NLP: Hugging Face Transformers, spaCy, OpenAI API
Automation: Cron (macOS LaunchAgent fallback), bash scripts
Model training: PyTorch, Scikit-learn
Data handling: pandas, CSV pipelines


What I Learned


How to automate nightly ML pipelines with cron and bash scripting
How to tokenize, train, and save Hugging Face transformer models
Managing environment variables securely for API access
Creating modular, debuggable code for complex pipelines
Using launchctl as a fallback to cron on macOS for scheduled jobs


Challenges I Faced


Setting up cron jobs on macOS was more complex than expected due to SIP (System Integrity Protection)
Loading large spaCy models and dealing with slow runtime at scale
Ensuring .env variables loaded correctly in every context (cron, shell, etc.)
Resolving issues with model serialization (e.g., mismatched keys in state_dict)
Detecting subtle differences in tone and formalizing that into a reliable metric
Avoiding submodule conflicts in GitHub and ensuring a clean repo structure



Current Status & Room to Grow
BiasGPT is intentionally in its infancy: the classifier is trained on a very small, synthetic dataset, so its predictions are still rudimentary. What matters most at this stage is the automated pipeline—every night the system generates new swapped‑prompt pairs, scores them, merges them into the master dataset, and retrains the model. As that dataset grows from a few dozen examples to thousands, the classifier’s F1 score will steadily improve and the system will evolve from a proof‑of‑concept into a genuinely smart bias‑detection engine.


What’s Next


Improve demographic swaps to be more context-aware
Use more advanced classifiers for tone and sentiment
Support batch uploads and human feedback loops in the UI
Extend support to other biases (e.g., age, socioeconomic status)

",https://github.com/rzarka1298/BiasGPT/tree/main,https://youtu.be/FEIzE_bVogc,Best Web Hack Using React - Peraton,"python, fastapi, react, tailwindcss, pytorch, huggingfacetransformers, spacy, openaiapi, pandas, numpy, cron, dotenv, github",Rugved,Zarkar,rzarkar1@terpmail.umd.edu,,https://github.com/rzarka1298/BiasGPT/tree/main,Best Machine Learning Track Hack,Best First-Time Hack,Best Social Good Hack,Hack That Made You Smile,Prettiest Hack,"This weekend, I had an incredible time using Vite with React, FastAPI, and Hugging Face Transformers to rapidly prototype an AI-driven project at the hackathon. The integration was smooth, and the streamlined development workflow allowed me to quickly iterate on my ideas, especially with Tailwind CSS making the frontend modern and responsive. Additionally, GitHub proved indispensable for version control and ensuring my codebase stayed organized and efficient throughout the event.",University of Maryland - College Park,OpenAI and Hugging Face,"Yes, I implemented a generative AI model using OpenAI's GPT-3.5 API along with Hugging Face Transformers. I utilized GPT-3.5 to generate numerical bias scores by comparing the tone of original versus demographically swapped texts, which then informed my model training. This approach helped me dynamically assess and refine the system's ability to detect subtle language biases throughout the project.",N/A,N/A,University of Maryland - College Park,0
Switch lane ,https://bitcamp2025.devpost.com/submissions/642387-switch-lane,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 15:42:39,"Inspiration

Pac-Man, flappy bird 

What it does

There are 9 sprites coming towards the player. One  or two of them are blue while the rest are black. The player has to maneuver to go through the blue sprite. If the user touches the black sprite they lose. The sprites change blue randomly 

How I built it

Using snap 

Challenges I ran into

I struggled finding a way to get data to all the sprites at once. I used the broadcast function 

Accomplishments that we're proud of

Broadcasting the data. I always struggled having multiple sprites communicating 

What we learned

A little bit more about snap

What's next for Switch lane

Maybe branch out using a different language
",http://snap.berkely.edu/snap/snap.html,https://youtu.be/niic0jwWdr4?si=8HNJpCC8uvp0hd3U,"",snap,Donya,Johnson,donyajohnson43@gmail.com,,http://snap.berkely.edu/snap/snap.html,Best First-Time Hack,Best First-Time Hack,Best First-Time Hack,"","",I looked at the Bloomberg.,Baltimore polytechnic institute,I did not use GenAI,N/A,N/A,N/A,Baltimore Polytechnic Institute,0
Schrödinger's Catch: TROUT of This World!,"",Draft,Pending,Project details,04/12/2025 16:05:17,"Inspiration

What it does

We created a VR game using Godot that allows users to walk around an island and fish that uses quantum computing to generate a random fish. 

How we built it

We created our project using the Godot game engine using Godot XR Tools for VR support, as well as creating a seperate web page with Python Flask for the random number generator. 

Challenges we ran into

We ran into many challenges during the creation of our project, the first being a lack of resources and documentation on Godot XR Tools and Godot VR. Another major challenge we had was that all of us are beginners in both VR as well as quantum computing, so we didn't know how to do a lot of things at first. Finally, one of our biggest issues was how to spawn in the fish after casting the line, ""Nishchay say something here"". 

Accomplishments that we're proud of

We are proud to have

What we learned

We learned how to export a Godot VR game to the Meta Quest 2 VR Headset to test and play, how to set up a player scene for VR in Godot using the Godot XR tools, and how to get random number generation using quantum computing.

What's next for Schrödinger's Catch: TROUT of This World!

The next steps for Schrödingers Catch
","",,[MLH] Best Use of .Tech,"meta-quest, godot, python, flask",Kush,Desai,kdd4203@gmail.com,,https://github.com/NA980593/schrodingers-catch#,Best Advanced Quantum Track Hack,Best Gamification Hack,Best Razzle Dazzle Hack,Hack That Made You Smile,Prettiest Hack,We are reviewing Godot.,West Chester University,I did not use GenAI,N/A,"","",West Chester University of Pennsylvania,2,Nishchay,Arjyal,na980593@wcupa.edu,NS977255,Santone,ns977255@wcupa.edu
Sprout It!,https://bitcamp2025.devpost.com/submissions/642407-sprout-it,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 16:09:21,"Inspiration

Safiyah forgot to water her plant.

What it does

Tracks the plants you have and sets a schedule for plant care.

How we built it

Code

Challenges we ran into

A lot

Accomplishments that we're proud of

Logo

What we learned

Plant

What's next for Sprout It!

Hopefully it will actually start working
",https://github.com/rhebadeeba/sprout_it,,"","react, javascript, node.js",Rhea,Choudhury,rhea.choudhury57@gmail.com,,https://github.com/rhebadeeba/sprout_it,Best First-Time Hack,"","","","",i liked the dino statue,University of Maryland,OpenAI,We tried,n/a,"",University of Maryland - College Park,1,Laurel,Sun,laurel.sun.06@gmail.com
Quanta,https://bitcamp2025.devpost.com/submissions/642408-quanta,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 16:11:05,"Inspiration

We've all worked with Fusion360 before and been frustrated at how long it can take to render certain items and the fact that its limited in scope - it can't model nanomaterials. Predicting how materials behave under stress is critical for many different industries, so we decided to make a solution. 

Introducing Quanta: Quantum-Assisted Nano-Structural Analysis

What it does

Our algorithm queries the properties of a material through the Materials Project API. Then, it represents the material as a flat mesh grid with a corresponding stiffness matrix, K. It can also take in an .obj file and put the material onto that. We used Finite Element Analysis (FEA) principles to approximate the behavior of mesh/thin sheets as a system of linear equations. We solve for u in the equation Ku = f, where f is force vector and u is the displacement of each point on the mesh. 

In our quantum implementation, we did this by using the Harrow–Hassidim–Lloyd (HHL) algorithm to determine the potential of quantum to quickly solve for sparse linear systems of equations. 

We can then render a 3D model based on the displacement matrix, which is presented to the user.

How we built it

We used Python (Flask and NumPy) on the back-end. For the quantum implementation, we used the Classiq Python SDK. On the front-end, we used React and Typescript. To create the .obj files, we used Blender.

Challenges we ran into

This was our first time coding up a simulation like this, and none of us had a lot of experience in materials science, so we had a lot of background research to do. In addition, we had to do a lot of math to derive the code for the algorithm, which took a long time. We spent a lot of time deliberating on if the project was even feasible before we began coding.

Also, none of us had actually integrated quantum computing into a ""real"" application, so we were nervous about if it would even work. We quickly realized that the current state of quantum hardware constrains us to smaller matrices, so we decided to pivot and create both a classical and quantum algorithm. The quantum implementation serves as a proof of concept and building block for future iterations of Quanta, while the classical implementation shows what is capable with current technologies and what areas for improvement are.

On the front-end, we faced challenges as well. This was the first time our team created a site using React Router, and it was also our first experience with Three.js for rendering 3D models. Learning these technologies and integrating them into our project was a steep learning curve, but it was rewarding to see the a physical visualization of our product.

Accomplishments that we're proud of

We're proud that we were able to make a functioning project! We also are proud that we are able to do meaningful work on both the backend and the frontend. 

What we learned

We all learned about quantum algorithms and how quantum computing can be used in the real world. We also learned a lot about materials science and different considerations that engineers use when deciding what materials to use.

What's next for Quanta

We want to see how far we can push Quanta and how big our mesh can be for the quantum implementation. We also want to add more variables to our calculation for the stiffness matrix and make our model even more accurate.
",http://quantano.tech,,"Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech","python, react, classiq, three.js, flask, typescript, tailwind, vite",Jessica,Su,aes6qb@virginia.edu,,https://github.com/chemystery09/bitcamp-25,Best Advanced Quantum Track Hack,Best First-Time Hack,Best Bitcamp Hack,Prettiest Hack,Hack That Made You Smile,"GitHub: It was really useful for us to use GitHub together as a team, because all of us were working on different parts, but they were all interrelated. We used GitHub in tandem with VSCode Live Share to minimize code conflicts, which seemed like the best use case for our needs.","University of Maryland College Park, University of Virginia",I did not use GenAI,N/A,N/A,quantano.tech,"University of Virginia, Thomas Jefferson High School for Science and Technology",3,Ishara,Shanmugasundaram,isha9shan@gmail.com,Diana,Lin,lindiana1206@gmail.com,Michelle,Che,michelle.che05@gmail.com
Moody Ring,https://bitcamp2025.devpost.com/submissions/642415-moody-ring,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 16:25:40,"Inspiration

Thinking about mood rings, one of the most quintessential tourist town souvenir shop items you'll find out there, I have always had an interest in how colors have been attributed to the way we feel. I decided to try and make a ""mood ring"" themed website that allows users to get in-touch with their emotions and learn more about why we attribute certain colors to certain emotions, and how things like seeing certain colors can bring about different thoughts and feelings.

What it does

Although I couldn't get the full functionality out of the way for the website, Moody Ring would be a five-question mood quiz that dynamically displays the color you're feeling as you progress through each question. 

How I built it

I already have experience with HTML/CSS/JS, but I wanted to finally get my feet wet with some brand-new skills: React and Node.js. I have never used the framework before, so to start, I looked up a few tutorials to follow and some sample code to get started with setting up a website framework. 

Challenges I ran into

Since I was COMPLETELY unfamiliar with React and Node.js, I had a lot to learn as I progressed as much as I could with this project. A good chunk of my project time was dedicated to sitting down with tutorial videos and guides and learning how to implement TypeScript. I also had never worked with custom domains before, and setting up my .tech domain was a bit tricky, so I'm not sure if I set up correctly.

Accomplishments that I'm proud of

I'm proud of what I learned about frontend frameworks as I was working through this project. Thinking about everything I've done without using React, TypeScript, and Node, I never realized how React bridges together my web development skills into one easy bundle. I'm proud that I can walk away from Bitcamp with an entirely new skill under my belt that can prove useful as I move forwards professionally.

What's next for Moody Ring

I am currently taking an HTML/CSS/JS course here at the University of Maryland, so I do want to build on my knowledge by working through completing MoodyRing as a personal project. I really enjoyed working with Node.Js and React, and I want to keep on developing my skills beyond college as well, so personal projects like these will definitely help to keep my skills sharp. Seeing as I have the MoodyRing.net domain name free for one year, I might as well make the most out of it.
",https://github.com/devwilkes/moody-ring,,"Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech","react, typescript, html5, css, git, github, .tech, copilot",Devon,Wilkes,devonowilkes@hotmail.com,,https://github.com/devwilkes/moody-ring,Best Razzle Dazzle Hack,Best Moonshot Hack,Best UI/UX Hack,"","","Not much, I'm pretty used to GitHub, and learning React wasn't too difficult after I went through a few tutorials.",University of Maryland,Other,"I did not implement any generative AI model, but I did use a bit of GitHub Copilot to identify bugs or syntax errors in my code as I worked.",N/A,https://moodyring.tech,"",0
LaunchMate,https://bitcamp2025.devpost.com/submissions/642423-launchmate,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 16:45:11,"LaunchMate Overview

🌟 Inspiration

LaunchMate was inspired by the vision of democratizing entrepreneurship. We recognized that many aspiring founders are overwhelmed by the complexities of starting a business. Our mission is to simplify this journey, providing the tools and guidance needed to turn innovative ideas into successful ventures.

🚀 What it Does

LaunchMate is an AI-powered platform designed to support entrepreneurs at every stage of their startup journey. From idea validation to market launch, it offers personalized templates, strategic insights, and a vibrant community, ensuring that users receive the guidance and resources they need to succeed.

🛠️ How We Built It

We developed LaunchMate using a robust tech stack that ensures scalability and efficiency:


*TypeScript *: The core language for building the platform's backend and frontend, providing type safety and modern JavaScript features.
*PLpgSQL *: Utilized for advanced database functions and procedures, enhancing data management capabilities.
*JavaScript *: Employed for client-side scripting and enhancing interactive user experiences.
*CSS *: Used for styling and designing a user-friendly interface.
HTML: The backbone for structuring web content and ensuring accessibility.


⚙️ Challenges We Ran Into

Throughout the development process, we faced several challenges, including:


Integrating real-time market tracking features while maintaining system performance.
Ensuring data privacy and security for users' sensitive business information.
Creating a seamless user experience that caters to both novice and seasoned entrepreneurs.


🏆 Accomplishments That We're Proud Of

We're proud of several key accomplishments:


Developing a unique AI-driven recommendation system that adapts to each user's startup journey.
Receiving positive feedback from beta testers, who reported a significant increase in successful project launches.


📚 What We Learned

The development of LaunchMate taught us the importance of user-centric design and iterative development. We learned to prioritize features based on user feedback and to remain agile in adapting to new challenges and opportunities.

🔮 What's Next for LaunchMate

Looking ahead, we plan to:


Enhance our AI capabilities to provide even more personalized and actionable insights.
Expand our platform's reach by forming additional partnerships and increasing our marketing efforts.
Continuously improve the user experience based on ongoing feedback and technological advancements.

","http://launchmate.tech, https://github.com/AdhyyanKumar/launchmate",,"Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","typescript, plpgsql, javascript, html, css",Yashovardhan,Saraswat,yash2006@terpmail.umd.edu,,https://github.com/AdhyyanKumar/launchmate,Best Bitcamp Hack,Best UI/UX Hack,Best First-Time Hack,"","","This weekend was an incredible learning experience. My team and I built an AI-powered startup assistant platform using a combination of front-end and back-end tools, integrated with generative AI to assist aspiring entrepreneurs. We explored a full development cycle — from ideation and planning, to prototyping and testing. Collaborating in a time-constrained environment really pushed us to focus on the MVP and use the best tech for speed and scalability.","University of Maryland , College Park",Gemini,"We worked with UMD'S TerpAI(OpenAI's GPT-4), GitHub, and Google Gemini. TerpAI was instrumental in helping us generate elevator pitches, milestone planning templates, and simulate market research summaries. GitHub made team collaboration seamless through version control and pull requests. Gemini was useful in generating structured UI copy and business logic in natural language format. Overall, these technologies were highly responsive and reliable. We’d love to see more real-time collaboration features in Gemini’s API documentation and easier fine-tuning in OpenAI’s playground.",1064350020039,www.launchmate.tech only,University of Maryland - College Park,3,Ansh,Mathur,anshmathur616@gmail.com,Adhyyan,Kumar,adhyyan@gmail.com,Mikhil,Chohda,mikhilchohda@gmail.com
NeoVisr,https://bitcamp2025.devpost.com/submissions/642424-neovisr,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 16:45:29,"Inspiration

We were inspired by the challenge University of Maryland Computer Science students face in managing degree requirements, course prerequisites, and advising processes. As students ourselves, we've found it hard to obtain clear, current information from UMD's website or wait weeks for meetings with advisors. We envisioned NeoVisr, a mobile app that acts as a personal academic advisor, giving responses instantly and automating tasks like degree audits. The idea of integrating AI into daily use tools to simplify student life inspired us to design a solution that is innovative and feasible. The name comes from the idea of being a ""new"" and modernized advisor.

What it does

NeoVisr is a mobile app and AI-powered backend designed to assist University of Maryland (UMD) Computer Science students with academic advising. The React Native app offers a chat interface for querying course details, while the backend processes PDF data and automates degree audits via UMD's uAchieve system.

How we built it

We split the project into backend and frontend using a mix of robust technologies:

Backend (agent.py, tools.py): 
We used Google ADK to create an LlmAgent cs_advisor, which runs queries with the Gemini 2.0 Flash Generative AI model. We created custom tools ourselves:
pdf_tool and search_pdfs to extract and query advising PDFs' text.
degree_audit_tool to automate uAchieve login and audit downloads programmatically using Playwright.
interpengage_tool for appointment scheduling.
advisor_email_tool to generate Gmail links with filled-out templates.
PDFs were indexed from inside a pdfs/ directory, sorted by classes like 400-level courses.
PDF text was extracted by pdfplumber, and Playwright spanned web navigation with CAS login and Duo 2FA.

Frontend (index.tsx): 
We made a React Native mobile app with a chat UI. Key pieces were:
AdvisorAI for chat UI, utilizing FlatList for messages and KeyboardAvoidingView for input.
ProfileScreen for managing user information (name, major, dark mode).
AsyncStorage to locally cache chat profiles and chat sessions.
A theme provider for light/dark themes and a sidebar for chat navigation.
The app interacts with the backend using Google ADK's API, sending queries and displaying responses.
We worked together in our Git environment, pair-coded for tough parts like Playwright automation, and locally tested using test PDFs and mock credentials.

Challenges we ran into

Playwright and Duo 2FA: Automating logins into uAchieve and TerpEngage was challenging because Duo's 2FA page wasn't always loading consistently in headless mode. We spent hours fiddling with selectors and timeouts, debugging via screenshots.
PDF Search Accuracy: Some PDFs had dense tables or scanned text, so pdfplumber struggled to extract useful data. We had to verify results manually and wished we could have added OCR integration if we had more time.
Backend API Stability: The Google ADK API at times provided indeterminate errors, making it difficult to debug session creation and tool responses.
Time Squeeze: Given only a weekend, it was challenging to put feature development up against testing. We added core features but weren't able to complete optimizing the UI or to add error recovery.

Accomplishments that we're proud of

We're thrilled with what we've done:

Functional AI Agent: Our cs_advisor answers real questions like ""What are CMSC412's prerequisites?"" with accurate PDF-sourced answers or helpful email templates.
Automation Victory: Getting Playwright to work through uAchieve's degree audit process, including Duo 2FA, was a win.
Polished UI: The React Native app looks clean and intuitive, with a chat interface that impressed our demo audience.
Collaboration: We collaborated in harmony, dividing the work (backend tools, frontend UI, PDF indexing) and merging code with few conflicts.
Demo Success: Showing a live degree audit download and chat session answering course questions wowed the judges and was a very rewarding experience.

What we learned

The hackathon was a gold mine of learning:

Google ADK: We realized how to structure AI agents based on tools and guidelines, although we learned to double verify API error logs in advance.
Playwright: We worked firsthand with browser automation, learned about selectors and state handling, but felt the limitations of headless mode when dealing with dynamic authentication.
React Native: We improved in mobile UI development, especially state and local storage management, but discovered environment setup takes foresight.
PDF Processing: Extracting text from research PDFs uncovered us to document structure flaws and the need for robust parsing tools.
Collaboration: Division of tasks and effective communication kept us on track, even under pressure. We also learned the skill of prioritizing MVPs over perfection at a hackathon.

What's next for Neovisr

Neovisr has huge potential, and we’re excited to keep building:

Enhanced PDF Parsing: Integrate OCR (e.g., pytesseract) for scanned PDFs and add metadata tagging for faster searches.
Secure Authentication: Replace plaintext credentials with encrypted tokens or OAuth for uAchieve/TerpEngage access.
UI Upgrades: Add PDF previews for degree audits, rich text formatting for AI responses, and smoother sidebar animations.
New Features: Include a course scheduler, calendar integration for appointments, and support for majors other than CS.
Scalability: Scale the backend to support multiple users, cache high-frequency queries, and host on a cloud server for real-world testing.
User Testing: Partner with UMD students to test the app based on feedback, ensuring it solves real needs.

Neovisr started as a hackathon spark, but we’re committed to making it a go-to tool for UMD students, blending AI smarts with practical automation to simplify academic life.
",https://github.com/samikwangneo/bitcamp2025,,"Best Use of GenAI in Business - Cloudforce/Microsoft, [MLH] Best Use of Gemini API","reactnative, typescript, asyncstorage, python, googleadkagents, playwright, pdfplumber, gemini, genai, llm, api",Aymaan,Hussain,ahussai8@terpmail.umd.edu,,https://github.com/samikwangneo/bitcamp2025,Best App Dev Track Hack,Best Moonshot Hack,Best Razzle Dazzle Hack,Hack That Made You Smile,Prettiest Hack,"This weekend participating at Bitcamp we interacted with a lot of different technologies. The main technology we used for the backend would be Google ADK which would handle the student queries. Setting up this agent was pretty straightforward as we used the tutorial provided by Google in their Agent Development Kit. Specifically using ADK we did not run into a lot of issues and everything went pretty smoothly. 

The following tool we used was Playwright for the degree audit and TerpEngage portion of our app. Having the screenshots when creating this process was a lifesaver as it helped us pinpoint at which point in the process we were having trouble and made debugging a whole lot simpler.

We used React Native for the frontend portion of our app. We are all pretty familiar with React Native but using technologies such as FlatList and AsyncStorage was a first for some of us. These new technologies however were pretty easy to understand and overall no complaints as it let us create our UI in a very polished manner.",University of Maryland: College Park,Gemini,"Yes during Bitcamp we did implement a generative AI model or API in our hack. We used the Gemini API for generating chat responses in our app through Google's Agent Development Kit (ADK) . We trained the model with advising pdfs as context. We then used this to create an LLMAgent called cs_advisor which will process all of the queries inputted about topics such as courses, ULCs, and more. The AI will then search the PDFs, create email templates, and help users book meetings with advisors through TerpEngage and Degree Audits.",732172765310,samik.tech,University of Maryland - College Park,3,Rivan,Parikh,rivan.parikh@gmail.com,Eswar,Karavadi,ekaravad@terpmail.umd.edu,Samik,Wangneo,wangneosamik@gmail.com
Meal Planner App,"",Draft,Pending,Project details,04/12/2025 16:50:46,"Inspiration - Being gym bros

What it does - Helps people plan their meals to allow them eat healthy and reach their desired goals as well as helping to keep a steady diet.

How we built it - Used a multitude of coding languages to create an app that can be used on your personal smart phone

Challenges we ran into - Accessing a database of nutrition facts for foods,

Accomplishments that we're proud of -

What we learned -

What's next for NutriFit -
","",,"","react, javascript, css, tailwindcss",Brandon,Faison,mrbrandonfaison@gmail.com,,"",Best App Dev Track Hack,Best Bitcamp Hack,Best Social Good Hack,MOST LIT HACK,Hack That Made You Smile,"Well, trying to make the code in itself was hard because the computer kept crashing",Baltimore Polytechnic Institute,Other,"Yes, we used it to create the recipes needed to make the meals and also get the macros of each ingredient. Without it, this project would not have enough recipes and it would overall be limited.",we arent.,"",Baltimore Polytechnic Institute,2,Nsonoofc,Stevenson,nikostevenson@gmail.com,Omar,Elfernani,omarelfernani1@gmail.com
DinoLearn,https://bitcamp2025.devpost.com/submissions/642431-dinolearn,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 17:08:48,"Inspiration

Studies show that people with ADHD are 3x more likely to drop out of traditional online learning due to long-form, rigid content. And yet, over 60% of all learners prefer bite-sized microlearning formats for better retention and focus.

We wanted to create something that caters to short attention spans, maximizes engagement, and adapts to any topic on the fly. That’s how DinoLearn was born—an AI-powered mobile-first platform that brings the joy of learning back, one prehistoric-sized chunk at a time.

It’s learning reimagined—with dinosaurs, because who wouldn’t want a dino as their study buddy? 🦕

What it does

DinoLearn generates a personalized learning roadmap based on any topic the user enters. Each day has:


A lesson outline
Summary content
A daily quiz


It’s built for learners who want to stay consistent and focused, one dino-sized step at a time.

How we built it

Frontend: Built using React Native with Expo Go.

Backend: Python FastAPI handles lesson generation and data requests.

AI Models: Gemini API creates the learning roadmap; ChatGPT powers lesson details and quizzes.

Database: MongoDB stores user progress and content dynamically.

Hosting: Full deployment on learnivore.tech with a custom .tech domain.

Extras: We implemented dynamic content loading—lessons and quizzes are only generated when users click on a specific day, speeding up performance.

Challenges we ran into

Gemini’s response time was slow (~90s); we optimized by switching to partial, on-demand generation.

Vercel domain verification was tricky with the .tech domain.

MongoDB’s ObjectId serialization initially broke our FastAPI responses—fixed with proper Pydantic handling.

Accomplishments that we're proud of

Successfully built and deployed a full-stack AI product in under 36 hours.

Live on a clever .tech domain: learnivore.tech as well as have an Expo Go App💡

Smooth integration between multiple APIs and databases with lazy loading optimization.

Design and UX are both educational and engaging—think Duolingo but with dinos!

What we learned

How to balance performance and AI response time with smart API call strategies.

Full-stack deployment practices with custom domains.

Better understanding of async handling in FastAPI and API rate limitations.

What's next for DinoLearn

Integrate ElevenLabs API for voice-based daily lessons.

Add gamification elements like badges.

Support multiple languages and accessibility features.
","https://learnivore.tech/, https://github.com/k-kochhar/DinoLearn",,"Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","next, react-native, python, fastapi, mongodb, gemini, openai, expo.io",Kshitij,Kochhar,kkochhar2004@gmail.com,,https://github.com/k-kochhar/DinoLearn,Best App Dev Track Hack,Best Bitcamp Hack,Best UI/UX Hack,Prettiest Hack,Hack That Made You Smile,"We learned how to efficiently manage and optimize API usage—especially when working with rate-limited generative AI models like Gemini and ChatGPT. Balancing speed with cost, we explored strategies for lazy loading, batching requests, and dynamic content generation. Also, MongoDB, GitHub, and Expo Go were absolute lifesavers for fast prototyping and deployment.","University of Maryland, University of Maryland Baltimore County, Northeastern University",OpenAI and Gemini,"Yes, our entire project is built around generative AI.
We used:

Gemini API to generate personalized 14-day roadmaps based on user-entered topics.

ChatGPT API to create lesson content and daily quizzes dynamically. This allowed us to build an intelligent learning assistant that adapts to any topic a user wants to explore.",1086161766207,learnivore.tech,"University of Maryland - College Park, University of Maryland - Baltimore County",3,Tirth,Patel,tirthppatelofficial@gmail.com,Josh,Attota,attotajva@gmail.com,Eric,Chen,echen9870@gmail.com
DMPN: My Tech Aura,https://bitcamp2025.devpost.com/submissions/642443-dmpn-my-tech-aura,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 17:37:50,"Inspiration

We’ve all had those moments while coding where we think we’re being productive… but somehow two hours pass and all we’ve done was renamed a few variables and watched 50 Instagram reels. We wanted to build a tool that keeps us mindful of our actual productivity, without micromanaging. Great focus and great code quality makes great progress in software development. ⏭️⏭️⏭️

That’s where DMPN – Developer Monitoring & Productivity Nexus – was born: an aura tracker that provides live feedback on whether you’re in the zone 🔎 or zoning out ✖️. Not only is it an aura tracker, we also provided 🎮gamification🎮 where we can face off against other programmers in ranked aura battles to show who is the most 10x engineer in the planet!

What it does

The core of DMPN is a fine-tuned model through a webcam image that shows the developer's emotions and, and a screenshot of the current screen of the developer's monitor. These images are analyzed in real time using 🦾 OpenAI's vision models 🦾 to generate productivity scores. From there, DMPN gauges a developer’s tech aura – essentially, how focused and productive they are during a coding session. This score is visualized in two✨ clean UIs ✨ to give users a sense of how they’re doing, gently nudging them back on track if needed.

In addition, we also spearheaded ⚔️ Aura Battles ⚔️, a online matchmaking tool that pitch you against other aspiring developers, in order to see who have the best productivity, style, and aura!

How we built it

We built DMPN as a monorepo with a modern tech stack:


Frontend: Built with Next.js and styled using TailwindCSS. We used WebSockets to push real-time productivity scores to the UI. 🌐
Backend: Powered by Flask. It handles image capture, preprocessing, and communication with the OpenAI Vision API. 📸


Our design is proud to be UI-first in order to win the UI track create the best first-time experience for users. In addition, we also prioritize real-time interactivity with the user, optimizing prompting and backend pipeline to provide low latency.

The system captures a screenshot and webcam photo at regular intervals, sending them to the backend for analysis. We designed prompts for OpenAI’s model to interpret visual indicators of productivity or distraction from both face and screen. Productivity scores are calculated and streamed live to the frontend via sockets.

Challenges we ran into


Setting up real-time image capture from both the webcam and screen in a smooth, system-agnostic way. 😎
Prompt engineering for OpenAI to interpret productivity meaningfully from facial expressions and screen content. 🎭
Handling WebSocket communication and ensuring consistent, lag-free updates.📶
Balancing user insights with privacy — making sure we don’t overstep while still delivering value.🔐


Accomplishments that we're proud of


A fully functional MVP with end-to-end integration — from image capture to live productivity scoring.
Tuned prompts that produced surprisingly accurate productivity assessments. Embrace the exponents!
A clean, minimalistic UI that feels futuristic and responsive.
Successfully used a monorepo architecture, streamlining development across multiple parts of the stack.


What we learned


Vision AI can be surprisingly insightful, but it needs thoughtful prompting and tuning.
Real-time interactivity is powerful, especially when paired with WebSockets.
Modular development and clear collaboration made working across the frontend and backend seamless.
It's possible to create meaningful productivity tools without being invasive — subtle feedback can go a long way.


What's next for DMPN: My Tech Aura


Refined scoring: Add more nuanced categories like deep focus, distraction, passive browsing, etc.
Personalization: Let users train a model on their own habits for more tailored feedback.
Integrations: Build browser extensions and IDE plugins for smoother workflow monitoring.
Privacy-first options: Offer local processing modes with no cloud storage, to ensure full data control for users.

",http://myaura.tech,,"Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech","nextjs, tailwindcss, websocket, openai, flask",Vuong,Ho,hochivuong2002@gmail.com,,https://github.com/VietRocHack/DMPN,Best Gamification Hack,Best Razzle Dazzle Hack,Best Bitcamp Hack,Prettiest Hack,Hack That Made You Smile,We had a great time setting up a Flask backend and handling stuff with OpenAI and other ML models we use for our purposes. We also started working with AI-assisted code tools for the frontend and we must say the experience is surprisingly pleasant.,University of Rochester,"OpenAI, Anthropic, and Other","Yes, we used OpenAI's flagship GPT models and Claude 3.7 Sonnet.",N/A,"dmpn.tech
myaura.tech",University of Rochester,3,Hoang,Le,hoangledo2092004@gmail.com,Lam,Pham,ptlam61004@gmail.com,Duc,Vu,ducvuminh6983@gmail.com
Seismic Sense,https://bitcamp2025.devpost.com/submissions/642454-seismic-sense,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 17:56:44,"Inspiration

We were inspired to build the Seismic Sense device by visually impaired characters that are able to sense through other means, such as Daredevil or Toph Beifong, with the idea of giving visually impaired people another option to navigate their surroundings.

What it does

The Seismic Sense device relays information about how far objects in the environment are to the user through vibrations in the handle. The 3 Ultrasonic Distance sensors allow for a field of view of 90 degrees. The 3 vibration motors are placed on the left, center, and right of the handle to give the user a sense of direction when using the device. The the distance sensor on the right is placed 45 degrees from the center and it corresponds with the vibration motor on the right side of the handle to give accurate feedback.

How we built it

We recorded distance sensor data with an Arduino and mapped its signal to control vibration motors, then we 3D-modeled/3D-printed a housing for it to sit inside.

Challenges we ran into

We had trouble with the ultrasonic distance sensors' signals interfering with each other, we were able to fix it by spacing out the times they would send their signals. We also had some trouble with knowing how much feedback to give at certain distances. Optimizing print times for 3D-Printers, in order to have more time for testing.

Accomplishments that we're proud of

We were able to use our device to successfully navigate around people and detect moving obstacles while not being able to see.

What we learned

We learned that running multiple ultrasonic sensors next to each other can cause problems, and that small vibration motors can be controlled through pulse width modulation for easy haptic feedback.

What's next for Seismic Sense

Adding more sensors to increase the amount of information we are able to receive and then transmit to the user. Creating a haptic glove for more efficient vibration transmission. Adding ground feedback to relay changes in slope when walking.
",https://github.com/Brickbuilder0658/SeismicSense,https://vimeo.com/1075017634?share=copy,Best Hack Promoting Public Health - Bloomberg,"arduino, c++",Aidan,Fleischer,afleisc1@umbc.edu,,https://github.com/Brickbuilder0658/SeismicSense,Best Hardware Hack,Best Social Good Hack,Best First-Time Hack,Prettiest Hack,Hack That Made You Smile,"Our project was mostly focused on interfacing a couple of HC-SR04 ultrasonic distance sensors with an Arduino. We had an easy time with Arduino because we have used it before, and it is very consistent. We did have a few hiccups with the distance sensors' waves interacting with each other, but we solved them fairly quickly by spacing out the times they would transmit.","University of Maryland, College Park and University of Maryland, Baltimore County",I did not use GenAI,N/A,N/A,seismicsense.tech,"University of Maryland - Baltimore County, University of Maryland - College Park",3,Charles,Epple,cepple@terpmail.umd.edu,Eric,Cheng,ericheng@terpmail.umd.edu,Abhinav,Patel,apatel30@umbc.edu
PhishNet AI,https://bitcamp2025.devpost.com/submissions/642478-phishnet-ai,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 18:20:22,"PhishNet

Everyday, millions fall victim to phishing attempts, scams, and identity theft. PhishNet AI catches these malicious websites and explains why it was dangerous using AI and anti-virus programs.

Inspiration

Many of the people who fall victim to tech support scams, phishing attempts, malware attacks, and identity theft are people who might not be familiar with internet safety. These people might click on shady links, be targeted by scam advertisements, or download malware claiming to be legitimate software. Because of this, billions of dollars of victims' money is stolen every single year.

We wanted to find a solution that prevents these scams and phishing attempts, while also providing people internet safety tips to prevent scams in the future.

What PhishNet AI does

PhishNet AI is a pun of the word phishing and the word net.

Phishing refers to any attempt from a website, email, or phone call to trick someone into providing an attacker personal information and/or money.

PhishNet AI serves to ""catch"" these phishing attempts in its metaphorical ""net"" and prevent the user from getting scammed, and prevent them from losing money and/or their personal information.

As the user browses the web, PhishNet AI is always vigilant about what websites the user clicks on or gets redirected to. Everyday websites like Google or Facebook won't trip any red flags, but if a user stumbles across a malicious website seeking to steal their information, PhishNet AI will quickly block the user from accessing the website, and provide the user with useful information (powered by Gemini AI) about why the website was blocked, and what threats were prevented.

Additionally, a well-known tactic used by scammers is to have the user download remote control software, allowing the scammer to take control of their computer. However, these kinds of software are not inherently malicious, and so instead of blocking the website, we instead alert the user about what the website is, and allow them to proceed forward if they understand.

How we built PhishNet AI

PhishNet AI is a Google Chrome web extension that sits on top of the browser at all times. The extension constantly monitors the web URL that the user is currently on, calls upon multiple URL checking bots to gauge the threat level of the current website, and generates a report on what the URL checkers found.

If the URL checkers report that a threat has been found, PhishNet AI will redirect the user to a safe page, and notify them that the malicious website has been blocked. Gemini AI generates a short summary of what kind of threat the website posed, and what kind of threat PhishNet AI prevented. All of this is shown to the user in a simple and concise manner, so that they can quickly get back to what they were doing. In addition to this, our program checks the URL with a database of known remote control websites, and if it is a known remote control website, then it alerts the user of the risks and ensures they know what they're doing.

Challenges we ran into while developing PhishNet AI

There were many different methods we considered when it came to gauging a website's legitimacy (whether a website was a phishing scam, contained malware, or was completely benign).


Using pre-trained models to gauge the legitimacy of a website was accurate for most malicious websites, but caused a lot of false positives for benign websites. This solution was quickly ruled out. Additionally, it added the component of needing intense processing power for the model to run, which we didn't have the budget for.
About six different URL checking APIs were considered, but didn't provide the heuristic we needed to generate an AI summary of the threat. The idea was to utilize a hybrid analysis API that was both accurate and provided enough heuristics about the website to tell the user what the threat was in a detailed way.
We considered caching websites that were already seen, to reduce API calls and avoid re-processing the same website more than once. We considered a database of some sort to store seen websites, along with threat information about that website, but with the way Chrome web extensions work, it was difficult to link a traditional database to the application. We opted to use VirusTotal's vast database of malicious URLs as a comparison point instead.


Accomplishments that we're proud of

We're proud, simply, of how coordinated we were throughout the whole project. It was amazing working with people who were able to bounce ideas back and forth with a lot of overlap between ideas. It felt like everyone was on the same wavelength at all times, and this enabled us to finish our hack impressively quickly.

Additionally, the speed at which we solved our biggest hurdles (detailed above) while on two hours of sleep, as well as empty stomachs is quite the achievement and definitely something to be proud of.

What we learned

Chrome web extensions have tight constraints. Working and coding around those constraints allowed us to formulate workarounds for things we thought would work, but didn't work due to the limitations posed by Chrome web extensions.

For instance, calling certain types of APIs requires a ""man in the middle"" to route these API calls securely. We learned that the underlying technology behind this limitation, Manifest V3, is intensely restrictive about securely sending requests to certain API endpoints. Researching this limitation was a huge pain and took a long time to crack, but of course it was worth it in the end.

What's next for PhishNet AI

Being able to scan newly downloaded files for malware, and warning the user about the infected file is a feature we're extremely ambitious about. Sometimes, browser security protocols and antivirus tools can miss malicious files. However, by utilizing the same bots that analyze web URLs, and modifying the behavior to work for downloaded files, the user can quickly and effectively be alerted about malicious files on their device.
",https://github.com/Pouncetail/PhishNet,https://youtu.be/V-MHhHGHbxc?si=_z9g4UnuYWODhAfn,[MLH] Best Use of Gemini API,"html, javascript, virustotal, gemini",Cedric,Yeo,pouncetail@gmail.com,,https://github.com/Pouncetail/PhishNet,Best Cybersecurity Track Hack,Best Social Good Hack,Best UI/UX Hack,MOST LIT HACK,Prettiest Hack,"Learning about MongoDB, we got a good understanding of how it works. However, we were unable to properly use it due to restrictions in our project. This is due to google extensions not being allowed to access MongoDB because of security issues, and the process to attempt to connect the google extension would take too long for a Bitcamp project.",University of Maryland College Park,Gemini,"If the URL checkers report that a threat has been found, PhishNet AI will redirect the user to a safe page, and notify them that the malicious website has been blocked. Gemini AI generates a short summary of what kind of threat the website posed, and what kind of threat PhishNet AI prevented. We used gemini to give the user a better understanding of what malicious websites can hold, and hopefully, will better inform them to avoid sites like these. All of this is shown to the user in a simple and concise manner, so that they can quickly get back to what they were doing.",976516090130,"","University of Maryland - College Park, University of Maryland - University College",3,Mayitreya,Pasumarthy,mayitreya.pasumarthy@gmail.com,Sujithkumar,Manoharan,sujith26@terpmail.umd.edu,Rahul,Yarlagadda,rahul123yarlagadda@gmail.com
MoneyTrail,https://bitcamp2025.devpost.com/submissions/642479-moneytrail,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 18:21:23,"Inspiration

We were inspired by the real-world challenges faced by financial crime investigators. With fraud becoming more digital and harder to trace, we wanted to build a tool that not only flags suspicious behavior but also helps analysts visually trace the money trail. Our goal was to make fraud detection more intuitive, interactive, and explainable—especially for analysts who need to act quickly and with confidence.

What it does

MoneyTrail is a real-time investigative platform that:


Displays high-risk accounts with interactive risk score gauges
Maps transactions and suspicious behaviors via dynamic network graphs
Visualizes triggered alerts with severity labels
Offers an interactive merchant view showing linked users and suspicious patterns
Includes a smart search bar that links to accounts, merchants, and alerts
Provides a score distribution bar to break down low, medium, and high-risk thresholds

How we built it
Frontend: React with Vite + custom CSS for layout and interactivity
Data Visualizations: D3.js for network graphs and score charts
Backend: Python Flask API serving mock banking data
Data Sharing: React Context API to provide live data across the entire app
Routing: React Router for navigating between dashboard, account detail, merchant insights, and alerts

Challenges we ran into

Connecting the frontend and backend — properly handling cross-origin requests (CORS), API endpoints, and synchronizing live data in React


Designing modular components that could handle dynamic data across tabs and views

Integrating D3.js into React without breaking layout responsiveness

Making the search experience fast, smart, and visually intuitive

Managing conditional rendering based on selected accounts, merchants, and user input

Accomplishments that we're proud of

Built an end-to-end financial investigation tool in under 36 hours

Successfully connected React frontend with Flask backend and passed live data through Context

Visualized complex fraud patterns in a way that's easy to understand

Created a clean and scalable frontend that mimics real-world compliance tools

Made the experience fun and informative — like Palantir-lite for fraud

What we learned

How to build a full-stack app with frontend–backend communication

How to use React Context for clean global state and live data access

How to implement custom network and bar visualizations with D3.js

How to translate fraud patterns into meaningful frontend UI/UX

That connecting React + Flask isn't plug-and-play — but incredibly rewarding once solved!

What's next for MoneyTrail

Add user authentication and role-based access (analyst vs admin)

Integrate an ML model to generate real-time fraud scores

Enable timeline filtering for historical fraud patterns

Connect to live banking APIs for real-time simulation

Package it for compliance teams as a plug-and-play fraud dashboard
",https://github.com/jainansh16/Money-Trail,,"Best Financial Hack - Capital One, Best Web Hack Using React - Peraton, [MLH] Best Use of MongoDB Atlas","react, vite, css, d3.js, python, javascript",Dhruvi,Kadhiwala,kadhiwala.dhruvi@gmail.com,,https://github.com/jainansh16/Money-Trail,Best Bitcamp Hack,Best UI/UX Hack,Best Moonshot Hack,"","","This weekend, we worked with React, Flask, D3.js, and GitHub. Connecting the frontend and backend was tricky at first due to CORS issues, but once resolved, Flask worked smoothly for serving API data. Using D3.js inside React was challenging but rewarding—it gave us complete control over our fraud network visualizations. Vite was a game-changer for fast frontend development, and GitHub helped us collaborate efficiently. Overall, the tech stack was powerful and flexible once everything was wired up.",University of Maryland,I did not use GenAI,We used Capital One's API called Nessie.,NA,"",University of Maryland - College Park,3,Ansh,Jain,ajain316@terpmail.umd.edu,Vismay,Igur,igur.vismay@gmail.com,Elvin,Sellappan,elsellan@gmail.com
Icebreaker Game Website,"",Draft,Pending,Project overview,04/12/2025 18:21:41,"","",,"","",Joseph,Wu,josephbwu@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,0
MealMap,https://bitcamp2025.devpost.com/submissions/642492-mealmap,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 18:30:29,"Inspiration

As I am moving into an apartment next year, I wanted to stay organized and efficient as I transition into cooking my own meals. I realized how much time and mental energy goes into planning meals, figuring out what groceries to buy, and making sure I’m not forgetting about the cheese in the back of my fridge for 4 months. That is why we created MealMap, a tool to make meal planning, grocery shopping, and inventory management easy.

What it does,

It allows you to plan out your meals for the week, track the ingredients you have in your fridge, and gives you a shopping list based on the recipes you choose. Once your meal plan is set, it automatically generates a shopping list based on the ingredients used in your selected recipes. You can explore recipes created by other users or build your own from scratch. It’s designed to reduce food waste, save time, and help users maintain consistent eating habits.

How we built it

We built the frontend using React with TypeScript. The backend uses Firebase, which handles authentication, database storage, and hosting. For the ingredient and nutrition database, we integrated the USDA FoodData Central API, which provides macro and micronutrient information for thousands of food items. This allowed us to automatically display nutritional info for each recipe and calculate totals per meal or day.

Challenges we ran into

One challenge we ran into is not having enough time to add all the features I wanted to add to make this a more useful tool. Some features we would've loved to add include a fridge inventory integration and dynamic expiration tracking. These needed more backend logic that we didn't have the time to implement. Another challenge was figuring out how to structure recipe data in a way that is flexible yet searchable was more complicated than expected.

Accomplishments that we're proud of

We’re proud of building a fully functional meal planning app with recipe browsing, custom meal creation, and auto-generated shopping lists. I didn't expect the  integration of the USDA API with our UI to be so seamless, and getting the authentication, database syncing, and UI components to work together smoothly was a rewarding challenge. Most importantly, the app solves a real personal need, and we know it could help others too.

What we learned

Do NOT put your api keys publicly on the git repository. :D We also used TypeScript for the first time and realized how easy it makes the web development process with debugging compared to Javascript. Using Firebase is a lot easier than we expected.

What's next for MealMap

We would love to create a more complete website and scale the website for public use. Here are some features we did not have the time to implement during bitcamp:


suggested shopping days
improved food expiration tracking
better track ingredients based on what meals you eat and buy
suggest meal suggestions based on current fridge


Special Thanks

Hariketh Kailad <3
","https://amealmap.vercel.app/, https://github.com/carrotlemon",https://youtu.be/bYUCivqwOs0,"Best Hack Promoting Public Health - Bloomberg, Best Web Hack Using React - Peraton","react, typescript, firebase, tailwind",Trevor,Kim,trevorkim3141@gmail.com,,https://github.com/carrotlemon/mealmap,Best First-Time Hack,Best Social Good Hack,Best UI/UX Hack,Prettiest Hack,Hack That Made You Smile,"I used React, TypeScript, Firebase, Git, Github, Premiere Pro, OBS, Microsoft Copilot and my brain.",University of Maryland,Other,N/A,N/A,"","Solon High School, University of Maryland - College Park",1,Ryan,Lee,ryan.y.lee0509@gmail.com
Gesture Based Music Controller,"",Draft,Pending,Project overview,04/12/2025 19:02:28,"Inspiration

Imogen Heap is one of my favorite producers because of her unique sound and innovative way she approaches music. I recently fell down a rabbit hole learning about the MiMU Gloves, a wearable technology she developed that allows musicians to control sound and effects through hand gestures. As a musician myself, I was inspired to build a more accessible version using a laptop webcam to produce a similar outcome.

What it does

Using the laptop’s webcam, I implemented real time hand tracking to detect specific gestures and map them to musical effects. Each gesture acts as a control signal, allowing me to manipulate sound dynamically as I perform. For example, holding an open palm begins recording a loop, which continues until I close my fist to stop it. Raising one finger adds reverb, while two fingers shift the pitch up by a couple of semitones. These hand movements transform the audio in real time without any physical hardware needed.

How we built it

The system uses a laptop webcam and OpenCV to capture video frames, which are analyzed using MediaPipe's Hands solution. MediaPipe provides 21 3D hand landmarks, and I extract their 2D positions to identify the configuration of fingers. A custom function compares the y-coordinates of finger tip landmarks (IDs 8, 12, 16, 20) with their intermediate joints to determine whether each finger is extended or curled. This simple heuristic allows me to detect key hand gestures like an open palm, a closed fist, or specific fingers raised.

Each gesture maps to a musical action. For example, when an open palm is detected, a separate thread initiates a 5-second audio recording using the sounddevice library, storing the waveform in a global buffer. When the fist gesture is recognized, the recorded loop is played back through the laptop’s speakers. To avoid blocking the main gesture detection loop, threading is used for all audio functions, ensuring smooth interaction without frame lag.

The audio processing side is handled by the Pedalboard library from Spotify. It enables real-time signal manipulation through virtual audio effects like gain, compressor, chorus, reverb, and pitch shift. Although only the recording and playback gestures are currently linked to functionality, the framework is set up to extend easily. For instance, the “1 finger up” gesture could toggle a reverb effect, and “2 fingers up” could apply pitch shifting by a few semitones.

The entire system runs in real time, with the OpenCV window providing visual feedback by overlaying the detected gesture label directly onto the video feed. The webcam resolution is set to 640x480 for a balance between performance and accuracy. The application continuously runs until the user presses 'q', at which point the webcam feed is released and the program exits cleanly.

In essence, this project brings together computer vision, audio programming, and gesture-based control to prototype an accessible version of a performance tool like Imogen Heap’s MiMU gloves—using only software and a laptop webcam.

Challenges we ran into

Accomplishments that we're proud of

What we learned

What's next for Gesture Based Music Controller
","",,"","python, opencv, mediapipe, sounddevice, pedalboard",Tanvi,Gupta,tgupta17@terpmail.umd.edu,,"","","","","","","","","","","","",University of Maryland - College Park,0
Cipherzoic,https://bitcamp2025.devpost.com/submissions/642517-cipherzoic,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 19:07:24,"Inspiration

We wanted to build something that combined encryption, humor, and creativity. Cipherzoic took inspiration from Bitcamp's theme to put a fun twist on substitution ciphers. The process of hiding secret messages in images by first turning the text into caveman-style “grunts” and ASCII cave paintings was full of enjoyment and laughter.

What it does

Cipherzoic lets users upload an image and a secret message. It uses least significant bit (LSB) steganography to embed the message into the image without visibly altering it. When decrypted, the hidden message is transformed into “caveman language"" via a custom-built ""prehistoric cipher"" that replaces letters with primitive-sounding words and ASCII cave paintings. 

How we built it

We used HTML and CSS to create a simple and intuitive front end where users can upload images and enter secret messages. On the back end, we used Flask to handle message processing, encryption, image manipulation, and decryption. The website runs on our .tech domain in a DigitalOcean container on an Nginx web server.

The core of the project uses LSB (Least Significant Bit) steganography to embed the message directly into the image without visibly altering it. Once decrypted, the message is transformed back into plaintext using our custom cipher.

Our cipher is fully reversible, giving users a fun and functional way to hide and then reveal messages with a prehistoric twist.

Challenges we ran into


Developing the cipher: We wanted the symbols to be complex enough to obscure the original message but still reversible for accurate translation.
File format issues: Lossy formats like JPEG often corrupted the embedded message, so we focused on lossless formats like PNG.
Unicode support: Creating prehistoric symbols for modern characters such as emojis was a challenge—supporting more characters remains a future goal.


Accomplishments that we're proud of


Designed a fully custom and reversible cipher
Embedded messages using steganography with zero visual distortion
Responsive and polished frontend that perfectly fits the theme of our project
Made our website accessible via our .tech domain using DigitalOcean, nginx, and get.tech. 


What we learned


How to develop a custom cipher system
The mechanics of substitution ciphers and LSB steganography
Limitations and quirks of different file formats and character encodings


What's next for Cipherzoic


Support for video or audio steganography
A caveman-speak chatbot
More comprehensive character support and symbols for code embedding

","https://github.com/joe-magg/Bitcamp-2025, https://cipherzoic.tech",https://youtu.be/iL699EtljEA,"Best Digital Forensics Related Hack - Cipher Tech Solutions, [MLH] Best Use of .Tech","html, css, python, flask, digitalocean, nginx",Brian,Ngo,bn00017@mix.wvu.edu,,https://github.com/joe-magg/Bitcamp-2025,Best Cybersecurity Track Hack,Best UI/UX Hack,Best Bitcamp Hack,Hack That Made You Smile,Prettiest Hack,N/A,West Virginia University (Morgantown),"OpenAI, Anthropic, and Other",N/A,N/A,cipherzoic.tech,"West Virginia University, West Virginia State University",3,josiahb10,Brown,jdb00076@mix.wvu.edu,Visnu S,Pandian,visnupandian1@gmail.com,Joseph,Maggio,joemaggio13@gmail.com
PosePilot,https://bitcamp2025.devpost.com/submissions/642528-posepilot,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 19:17:30,"PosePilot: Real-Time Posture Monitoring Assistant

Team Members: Sai Jagadeesh Muralikrishnan, Chan, Pure, Vivek
BitCamp 2025 Submission

Inspiration

The inspiration for PosePilot came from a shared struggle that many of us face—spending long hours in front of our computers with poor posture. As students, gamers, and developers, we often found ourselves slouching unknowingly for extended periods, leading to neck strain, back pain, and fatigue. We wanted to build something practical and helpful—a lightweight tool that could nudge us toward better posture in real time, without needing any wearables or specialized hardware.

What We Built

PosePilot is a real-time AI-powered posture monitoring system that uses your webcam to detect your posture, evaluate key metrics like neck and back angles, and provide visual/audio feedback if you’ve been misaligned for too long. The system also gives you a posture score, deviation alerts, and motivational badges to gamify progress and encourage good habits.

What We Learned


We deepened our understanding of pose estimation using YOLOv8-Pose.
We applied vector math and trigonometry (like dot products and angle calculations) to convert raw keypoints into posture insights.
We learned how to build and deploy Flask APIs, integrate real-time camera feeds, and render feedback through a fully responsive frontend built with Tailwind CSS and HTML.
We explored how to deliver high FPS performance in Python while balancing accuracy with efficiency.
We also discovered how important user experience is in an AI tool—so we designed a clean, intuitive interface to make posture tracking simple and enjoyable.


How It Works


Your webcam captures real-time video frames.
The backend uses YOLOv8-Pose to extract keypoints from each frame.
Using vector angles between your shoulders, hips, and neck, we calculate:


Neck tilt
Spine curvature

If the neck/back angle exceeds a threshold (typically ~20°), we trigger posture alerts.
A real-time posture score updates as you maintain better posture over time.


We also included a baseline posture capture, deviation duration tracker, and earned badges like ""Posture Master"" and ""Fast Tracker"" to help users stay motivated.

Built With


Python – Backend processing and detection logic
YOLOv8-Pose – Real-time pose estimation
Flask – Lightweight API backend
HTML5 + Tailwind CSS – Modern frontend
Chart.js – Posture history graph
Pyttsx3 & Audio Alerts – For voice feedback and beeps
Three.js & GSAP – For 3D visuals and scroll animations
(Upcoming) Google Gemini AI – For posture reports and tips (in progress)


🌐 Try It Out


💻 GitHub Repository: github.com/cravotics/Pose-pilot
Follow the instructions in the README to replicate the system locally.
📸 Note: The demo requires running the Python backend (main.py) and accessing the HTML interface locally.


Challenges We Faced


Calibrating neck and back angles reliably from webcam input was tricky due to real-world camera distortions and body variations.
Managing real-time performance with high-resolution video while maintaining inference speed required careful optimization.
Designing a UI that felt natural and motivating without overwhelming the user took several iterations.


What's Next?


We're integrating Gemini AI to generate daily posture summaries and personalized feedback.
We're also prototyping a vibration wristband that gives a gentle buzz when you slouch—turning PosePilot into a full hardware+software assistant.

",https://saijagadeesh.com/Pose-pilot/,https://youtu.be/UlEeWPRlD64,Best Hack Promoting Public Health - Bloomberg,"yolov8-pose, flask, pyqt5, tailwind-css, chart.js, three.js, gsap, html5, javascript, pyttsx3, built-with-python, gemini, ai, python, yolo",Vivek,Patel,vivekschpatel@gmail.com,,https://github.com/cravotics/Pose-pilot,Best Machine Learning Track Hack,Best First-Time Hack,Best Gamification Hack,Prettiest Hack,Hack That Made You Smile,"We had a great time diving deep into computer vision and real-time web apps. We mainly worked with YOLOv8-Pose, Flask, and Tailwind CSS. Integrating everything smoothly and optimizing the model for posture tracking was a fun challenge. We also explored embedding Gemini for future use. Overall, a super hands-on and rewarding experience!",University of Maryland College Park,"OpenAI, Gemini, and DeepSeek","Yes, while we didn’t integrate it fully into the live demo, we designed our system to work with Gemini for generating personalized posture reports and real-time voice-based coaching in future versions.",1046917462205 and AIzaSyA71g53Gw1831fzaZWPZUcjXTShbmWx7fQ,None,"University of Maryland - College Park, Whitman College",3,Sai Jagadeesh,Muralikrishnan,jagadeesh.offcl2k@gmail.com,Chan Myae,Aung(Charles),charles2002mdy@gmail.com,buahomy,Buahom,yuttanaweebuahom@gmail.com
CircuLens,https://bitcamp2025.devpost.com/submissions/642542-circulens,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 19:38:28,"🔌 CircuLens – Understand Circuits, Visually
About the Project
CircuLens is an AI-powered tool that converts real-world images of breadboard circuits into clean, digital circuit diagrams and intuitive explanations.

Inspired by tools like Google Lens, we wanted to bring similar visual intelligence to the world of electronics. Many students and hobbyists struggle to understand how their physical circuits relate to schematics — CircuLens bridges that gap with just one upload.

Inspiration
We were driven by a common frustration:
“I built a circuit… but I don’t know how to explain it.”

In hardware workshops and beginner electronics classes, students often create circuits on breadboards but lack the clarity to document or describe what they've built. Our goal was to make understanding circuits as easy as taking a photo.

What We Learned
How to build a clean, responsive frontend using React, Vite, Shadcn UI, and Tailwind CSS

How to simulate an AI pipeline with mocked output for circuit analysis

How to handle UX challenges like image previews, drag-and-drop upload, and loading states

Real-world Git collaboration, including resolving complex merge conflicts

How We Built It
Frontend: Built entirely with React using Vite and TypeScript

UI Framework: Styled with Tailwind CSS and components from Shadcn UI

Image Upload: Supports both file selection and drag-and-drop

AI Layer (Simulated): Currently mocked to return:

An ASCII-based circuit diagram

A natural language explanation of how the circuit works

Polished UX: Emphasis on mobile responsiveness, clear layout, and ease of use

Challenges We Faced
Git merge conflicts between frontend team members

Ensuring responsive design across screen sizes

Managing image state and simulated analysis output without a real backend

Keeping the interface simple and beginner-friendly

What's Next
Integrate real-time circuit recognition using Google Gemini or OpenCV

Add live camera input for mobile users

Support exporting diagrams to tools like KiCad, LTSpice, or Fritzing

Build collaborative features like shared annotations and editable diagrams
",https://github.com/eionpaulos/bitcamp2025,,"Best Web Hack Using React - Peraton, [MLH] Best Use of Gemini API","react, python, opencv, gemini, fast, api, gemini2.0, uvicorn, ai, ml, computervision",Ibrahim Ahmed,Mohammed,ibrahim.iftaquar@gmail.com,,https://github.com/eionpaulos/bitcamp2025,Best Machine Learning Track Hack,Best First-Time Hack,Best Bitcamp Hack,Hack That Made You Smile,MOST LIT HACK,"This weekend, we explored a wide range of technologies while building an AI-powered circuit analyzer. We used FastAPI and Uvicorn for the backend, React with Vite and Shadcn UI for the frontend, and OpenCV for image processing. We also integrated Google Gemini to enhance our AI-driven analysis. We ran into a few setup issues, especially with uvicorn and Vite file imports, but overall, the tools were powerful and helped us build quickly. It was a great learning experience working with both modern web tools and AI frameworks!",University of Maryland-College Park,"OpenAI, Anthropic, Hugging Face, Gemini, and Gemma","Yes, we implemented a **generative AI API** using **Google Gemini** in our project. We used it to generate natural language explanations of circuit behavior and to assist in identifying electronic components from images. Gemini helped translate raw image data into human-readable insights, making our tool more accessible and user-friendly for beginners and non-technical users. Its ability to interpret visual and contextual information was a key part of our AI-powered analysis pipeline.",friend is sleeping will update,"",University of Maryland - College Park,0
Lighthouse,https://bitcamp2025.devpost.com/submissions/642548-lighthouse,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 19:47:44,"Inspiration 🌅

We’ve all been there: you snap a bunch of photos to capture a moment, only to look back and feel like something’s missing. Maybe the lighting was off, the image is way too grainy, or there’s a distracting detail you wish you could erase. The photo just doesn’t feel like the moment you lived. That frustration is what inspired Lighthouse — a tool designed to reconstruct and creatively reimagine noisy or imperfect images using the power of diffusion.

What it does ⚙️

Lighthouse is powered by diffusion models — deep learning systems that can generate detailed, high-quality images from nothing but noise. Normally, these models start from pure randomness and gradually subtract noise in small steps, revealing an image as they go. I thought: what if we hijack this process? What if we insert a real image midway through the diffusion process and let the model rebuild it with enhanced detail or creative variation?

Lighthouse focuses on two main tasks:


Extreme Image Denoising: In low light or high ISO situations, your image can be so noisy it’s nearly unusable. Lighthouse uses a diffusion model’s inherent denoising ability to recover and recreate details that likely existed — even when traditional filters fail.
Creative Generation: By modifying and adding noise back into an image (a process I call renoising) and running it through the diffusion model again, Lighthouse can generate realistic and artistic variations based on small modifications — enabling expressive, intuitive editing.


Check out the demo to see both of these modes in action!

How I built it 🛠

Lighthouse is built using Google’s Denoising Diffusion Probabilistic Models (DDPM). This architecture lets me skip the initial timesteps and start the denoising process from a partially noisy, real image. To make renoising work, I also implemented a custom noise scheduler that simulates the same noise induction process used during training.

Challenges I ran into 🚧

Diffusion models are powerful — and incredibly compute-heavy. Even with a decent GPU (RTX 2070), running high-res models pushed the limits. After experimenting with several architectures, I landed on a model that struck the right balance between performance and output quality. Training a model from scratch would’ve taken more than a hundred years on my setup (literally) — so for now, pre-trained models are the move.

Accomplishments that I'm proud of ✅

The best part? Lighthouse works. It can bring detail back into unrecognizable images and enable amazing, creative edits in just a few steps.

What we learned 📚

This project was my first time working hands-on with diffusion models, and it was a deep dive. I already had a theoretical understanding, but implementing denoising, scheduling, and custom workflows really pushed my understanding to the next level — especially when experimenting with novel use cases like renoising.

What's next for Lighthouse 🔮

I’m excited to take Lighthouse even further with:


Stable Diffusion Integration: Operating in the latent space using VAEs, which could unlock faster and higher-resolution generation.
LoRA Fine-Tuning: Adding lightweight adapter training to guide generation using semantic concepts, all without retraining the entire model.

",https://github.com/dswl/lighthouse/tree/main,https://youtu.be/LWvlYRctdhg,"","python, ddpm, torch",David,Liu,davidjinliu@gmail.com,,https://github.com/dswl/lighthouse/tree/main,Best Machine Learning Track Hack,Best Moonshot Hack,Best Bitcamp Hack,"","",Github was good.,University of Maryland - College Park,Other,"Yes (diffuser model, see project details for info and implementation)",N/A,"",University of Maryland - College Park,0
MacroMaster,"",Draft,Pending,Project overview,04/12/2025 19:50:05,"","",,"","",Andrewg314,Guo,andrewguo108.dog@gmail.com,,"","","","","","","","","","","","",Winston Churchill High School,1,N,Chou,nathan07.chou@gmail.com
SoberSide,https://bitcamp2025.devpost.com/submissions/642562-soberside,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 20:09:38,"About SoberSide

Inspiration

SoberSide was born from a very personal place. During our freshman year, we witnessed first-hand the damaging effects of substance addiction on some of our closest friends. We saw peers struggle with the harsh realities of addiction—watching vibrant lives slowly deteriorate under the weight of substance abuse. The pain of seeing brilliant, talented individuals lose their spark and direction deeply affected us. We knew there had to be a better way to support those fighting the same battles.

We wanted to create something more than just a generic self-help guide. SoberSide is our attempt to harness technology to provide compassionate, personalized support for individuals on the journey to sobriety—helping them understand their patterns, track their progress, and ultimately rebuild their lives with dignity and hope.

What We Learned

Throughout the development of SoberSide, we learned a great deal about:


Full-Stack Development:
Building the backend with Node.js, Express, and MongoDB taught us how to manage data in real time.
API Integration:
Using OpenAI's API pushed us to explore how AI can be tailored to provide empathetic responses on sensitive topics like addiction.
Humanizing Technology:
We discovered that the true value of technology isn’t just in its ability to perform tasks but in its capacity to understand and support human emotions.
Interdisciplinary Challenges:
Combining mental health awareness with advanced technical solutions is challenging—especially when working on such delicate subjects—but it’s also profoundly rewarding.


How We Built the Project

We started by setting up a robust Node.js backend that integrated three core components:


Data Management:
Using MongoDB Atlas with Mongoose allowed us to persist user profiles and chat logs, capturing details such as sobriety start dates, moods, and personal coping strategies.
Artificial Intelligence:
OpenAI’s API powers our chatbot that provides thoughtful, personalized responses based on historical data, ensuring that each interaction feels uniquely tailored to the individual.
Voice & Messaging Integration:
Twilio is used to enable both text and voice communications, making sure that help is accessible regardless of the user's situation.


We meticulously crafted system prompts and data storage schemas to ensure that the AI could learn over time and offer more personalized insights with each conversation.

Challenges Faced

Building SoberSide wasn’t without its hurdles:


Emotional Toll:
Seeing how addiction impacted our friends during our freshman year made this project more than just a technical challenge—it became a personal mission for our small team.
Technical Integration:
Integrating OpenAI, MongoDB, and Twilio into a cohesive application was complex. Each system came with its unique challenges and required careful error handling.
Privacy & Sensitivity:
Handling sensitive personal data and ensuring that our AI responded in a safe, non-judgmental manner required intensive testing and iterative refinement of both our prompts and logic.
Context Management:
Since GPT is inherently stateless, designing a mechanism to feed historical data back into the conversation so that the AI ""remembers"" past interactions was one of the most challenging aspects.


Final Thoughts

Building SoberSide has been a labor of love—a fusion of technical innovation and deep personal commitment to helping those who struggle with addiction. The experiences and observations from our freshman year, filled with both heartbreak and resilience, inspired us to build a tool that not only supports sobriety but also learns and adapts to each individual. SoberSide is our ongoing journey towards better technology, better support, and ultimately, a brighter future for everyone fighting for a sober life.
",https://relapsedaddy.tech,,"Best Hack Promoting Public Health - Bloomberg, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best Use of MongoDB Atlas","javascript, react, next, openai, twilio",Arjun,Iyer,arjun.iyer42@gmail.com,,https://github.com/Kokonutman/bitcamp-frontend-2,Best Social Good Hack,Best Razzle Dazzle Hack,Best UI/UX Hack,Hack That Made You Smile,Prettiest Hack,"This weekend was an incredible deep dive into building a full-stack, real-time application, and we got to work with a range of cutting-edge technologies that pushed me out of my comfort zone.

GitHub

we really appreciated how GitHub made collaborating smooth and seamless. Our team of three set up our repositories quickly and utilized branching strategies and pull requests to iterate on our code efficiently. It was amazing to see all our work coming together despite working remotely—version control and code reviews were a central part of our workflow.

MongoDB

MongoDB was a game-changer for data persistence. I had the opportunity to integrate MongoDB Atlas into our backend, which allowed us to store detailed user profiles and chat logs. I learned a lot about writing Mongoose schemas to handle dynamic data, such as tracking sobriety progress and storing personalized feedback from our AI therapist. The real-time querying features of MongoDB truly helped in visualizing data and ensuring that each user’s progress was logged and could be recalled for context in subsequent conversations.

Twilio

Twilio’s voice integration was both exciting and challenging. We leveraged Twilio’s APIs to simulate voice-based interactions in our application. Integrating voice and chat required handling asynchronous events and ensuring reliable feedback to our users. It was impressive to see how we could build a system where users could talk to an AI therapist-making it accessible even via phone calls. The experience taught me a great deal about handling incoming calls and processing speech input, even though it took some fine-tuning to get everything working as expected.

OpenAI

The highlight for me was integrating OpenAI’s GPT models to power our AI therapist. Configuring the API and fine-tuning our system prompts to generate empathetic, context-aware responses was both a creative and technical challenge. It was fascinating to see how we could bridge human conversation with machine learning, and how important it was to manage context by integrating chat logs stored in MongoDB. This integration not only pushed our technical boundaries but also underscored the responsibility of building technology that interacts with sensitive human emotions.

Overall Experience

Working through these technologies provided a steep learning curve and a rewarding sense of accomplishment. The hackathon environment pushed us to solve real-world challenges quickly. Every tool we used—GitHub for collaboration, MongoDB for dynamic data management, Twilio for voice interactions, and OpenAI for AI-driven personalization—contributed uniquely to the project’s success. I truly enjoyed the weekend; it was a blend of collaborative problem-solving, innovative use of technology, and a shared passion for building solutions with social impact.

In summary, this hackathon experience was both technically enriching and personally meaningful. It reminded me of the power of technology to create supportive, life-changing applications while also emphasizing the importance of teamwork and continuous learning.",University of Maryland College Park,OpenAI,"We integrated OpenAI’s GPT (gpt-4) API into our SoberSide backend. The main goals were to power our AI therapist and provide personalized, empathetic responses to users struggling with addiction. We built a system that not only sends the user’s prompt to GPT but also retrieves recent chat logs and user profile information from MongoDB. This contextual data is pre-pended as part of the GPT prompt to create responses that feel more personalize-like acknowledging a user’s name, sobriety progress, or previous struggles. Overall, it enabled us to offer a compassionate, data-informed conversation that adapts to each user’s journey",N/A,relapsedaddy.tech,University of Maryland - College Park,2,Titiksha,Golla,tgolla@terpmail.umd.edu,Anish,Narang,anishnarang06@gmail.com
Travelero,"",Draft,Pending,Project details,04/12/2025 20:21:49,"Inspiration

Navigation is an extremely common task in which traditional map services purely prioritize distances between two points. This makes it hard for handicapped or wheelchair users, especially to follow or use online navigation services. To solve this, Travelero generates paths based on user preference where users can prioritize the shortest distance to their location, the least elevation to their location, or a mix of both.

What it does

Travelero allows users to search for a specific location on a user-friendly GUI map of the Earth. Use the slider to prioritize the least distance traveled or the least elevation to a specified degree. Select two locations on the map to place markers, and click generate to run our algorithm, which takes the state of the slider into consideration and generates a path.

How we built it

We used the Google Maps API to grab our map data, such as distances, paths, and elevations. 
We used GitHub, git, and VS Code to work on code as a group. We used React and TypeScript to build the frontend and a combination of JavaScript, Python, and Flask to build the backend. We also used terpai.umd.edu in order to generate ideas for specific features that we may have wanted to include, such as our personal media links and utilization for our concept. And Gemini to bring some of our ideas to life. 

Challenges we ran into

We found difficulty when brainstorming ideas as we continuously ran into runtime issues. (infinite possibilities for paths)
We found it difficult to navigate buildings, as a path generated may enter a building that the user does not have access to.
We had difficulty learning how to create a website without much prior experience, especially when trying to link the frontend and backend together.

Accomplishments that we're proud of

During the event, we learned many new things. Our research and hands-on experience allowed us to feel comfortable performing and creating a similar project in the future.
We were also able to strengthen our bonds as team building really shone throughout the project. Lots of ideas were shared, lots of helping each other within the team, and lots of considerations for everyone's strengths and weaknesses brought everyone closer together. In each and every step that we completed, the entire team felt like winners. We all celebrated each small accomplishment and felt pride that we were able to accomplish what we had hoped.

What we learned

The team generally lacked lots of experience in creating a webpage, and we were all able to make great progress in learning new skills related to problem solving and app development. Additionally, as a team coding project, we also learned skills and gained knowledge towards working in team environments. We were also able to learn about and how to use lots of different resources, especially software. From terpai.umd.edu to Flask and React.

What's next for Travelero

There are lots of different possible optimizations and additional features that can create a larger audience. Additional tools that other demographics or groups may find useful. For example, we think that being able to prioritize different factors, such as scenery or safety, could open doors to a larger audience.
","",,"","javascript, typescript, react, flask, google-maps, python, github, git",Wesley,Lau,wesleylau0524@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,0
Askit,https://bitcamp2025.devpost.com/submissions/642572-askit,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 20:22:02,"Inspiration

We wanted to create an app that helps students (or anyone, it can be used for more than academic purposes) organize and store questions for studying, reviewing, or even just note-taking. The app is specific to the user and helps them focus on fixing their weak points of a subject.

What it does

Askit lets users create folders where they can store different questions. Users can log in or sign up for an account, create folders, and add questions to them, whether they’re studying for a class or just organizing thoughts. Once those questions are made, users are free to edit or delete them if needed. They can also use the search bar feature to search for a specific question from all of their questions.

How we built it

We built Askit using React Native and Expo. For the backend, we used Supabase for database management. Navigation between screens was handled with React Navigation, and we used React Native Paper for styling the UI components. We also used Lucidchart to map out the flow and different screens of the app. 

Challenges we ran into

Our biggest challenge was setting up the login and signup. We initially tried to use Supabase Auth for it, but we ran into errors with mandatory email confirmations for users, so we had to pivot and start over (in a sense) for those pages. 

Accomplishments that we're proud of

We’re proud of building a functional and good looking application and that we were able to implement all of the features that we thought it would be possible for us to do with the time given. 

What we learned

We learned a lot about React Native navigation, Supabase integrations, and how to work quickly and under pressure. We also picked up new skills in debugging and refining UI.

What's next for Askit

We wanted to try to add a feature that allows users to take pictures of their questions and store those pictures in the folders instead of manually having to type the questions out every time. 
",https://github.com/lynn737/Askit,https://youtube.com/shorts/YKGDvdRgd5Y?feature=share,"","reactnative, reactnativepaper, supabase, npm, vscode, lucidchart",Lynn,Hui,lynn737@gmail.com,,https://github.com/lynn737/Askit,Best Bitcamp Hack,Best UI/UX Hack,Best Razzle Dazzle Hack,Hack That Made You Smile,Prettiest Hack,"GitHub, React Native, Supabase, React Native Paper","University of Maryland, College Park",I did not use GenAI,N/A,N/A,N/A,University of Maryland - College Park,1,Kashvi,Tiwari,ktiwari1@terpmail.umd.edu
World Racing,https://bitcamp2025.devpost.com/submissions/642575-world-racing,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 20:26:28,"Inspiration

We built World Racing to bring back the joy of exploration through a modern lens. Inspired by our childhood memories of games like Mario Kart and the excitement of zooming through vibrant maps, we wanted to recreate that thrill—except in the real world. With so many young people wanting to travel but facing constraints like time or money, we thought: what if you could race through the streets of Tokyo, Miami, or Paris from your browser?

On a personal level, Mark has always struggled with navigation, often making wrong turns and relying heavily on Apple Maps—even in his own city. World Racing became a fun and practical way for him to get better at recognizing streets and landmarks. Cristy, on the other hand, gets nervous driving in unfamiliar places. This project became a tool for her to build confidence, allowing her to virtually ""practice"" routes and get familiar with her surroundings beforehand.

Also? It's just really fun. With multiplayer support, you can race against your friends and explore new places at the same time!



What it does

World Racing lets you pick any real-world location and instantly create a racecourse through it. You define the start, finish, and checkpoints, and we build a fully interactive 3D driving experience around it. Before the race starts, you're treated to a cinematic drone flyover of the route, complete with roads, trees, oceans, landmarks, and more. The driving experience includes a 3rd-person camera view that follows your car, with a working speedometer, compass, and visual cues for landmarks and turns. It’s part game, part travel, part simulation—accessible from your browser in seconds.

How we built it

We used Mapbox GL JS to render realistic 3D maps and terrain, and Three.js to handle the custom 3D car physics and visuals. The app itself is built in React with TailwindCSS for a clean, responsive UI. On the backend, we used Rust with the Axum web framework for high-performance APIs, and PostgreSQL with SeaORM to store and serve course data and user-generated maps. All components work together seamlessly to generate a real-world map, overlay the chosen route, and simulate a racing experience with proper camera and collision logic. We also utilized websockets for real-time, live multiplayer.

We rolled our own physics engine for the car movement, here's some of the math that we utilized:

Angular Velocity


Acceleration over time


Velocity over time


Distance from point to line segment with endpoints


Challenges we ran into

Collision detection on 3D maps: Making sure users couldn’t drive through buildings or terrain required custom logic on top of Mapbox's 3D tiles and elevation layers. We had a version that had support for collision detection, but ultimately decided to scrap it, because the 3d maps that we really liked did not have the data necessary in order to have collision detection.

Camera tracking: Creating a smooth third-person camera that reacts naturally to speed, turning, and collisions took several iterations to get right.

Map loading performance: Loading a fully 3D-rendered city on demand while maintaining 60 FPS required lots of optimization, especially when switching between drone view and race mode.

Pathfinding and routing: Translating user-defined points into a logical, drivable path across roads in a 3D environment was more complex than anticipated.

Accomplishments that we're proud of


Building a fully interactive driving simulator that works anywhere in the world with a single click.
Achieving smooth car physics and camera movement within a 3D map of real cities.
Creating a beautiful cinematic drone sequence that previews your race before you begin.
Designing a product that blends nostalgia, practicality, and technical achievement in a unique way


What we learned


Not to wait until the last minute to implement a multiplayer system using websockets.
How to work with real-world geospatial data and turn it into an engaging, game-like experience.
The complexities of combining Three.js physics with Mapbox 3D terrain and tile loading.
How to architect a fullstack system with React + Rust + PostgreSQL in a performant and scalable way.


What's next for World Racing


Refining multiplayer support 
Multiple car car support
Racing against AI
Power-ups
Adding ramps and more

","https://worldracers.warrensnipes.dev, https://github.com/mbruckert/world-racers",,Best Web Hack Using React - Peraton,"mapbox, three.js, react, rust, axum, postresql, seaorm, docker, dockercompose, distroless, github, vite",Cristina,Chirolde,cchirolde2003@gmail.com,,https://github.com/mbruckert/world-racers,Best Bitcamp Hack,Best Moonshot Hack,Best Razzle Dazzle Hack,Hack That Made You Smile,Prettiest Hack,"We really enjoyed working with mapbox this weekend, it was a very nice developer experience!",University of Central Florida,"OpenAI, Anthropic, and Gemini",N/A,N/A,"","UCF, University of Central Florida",2,Warren,Snipes,contact@warrensnipes.dev,Mark,Bruckert,markbruckert1@gmail.com
BoxBot - The Dancing Robot!,https://bitcamp2025.devpost.com/submissions/642582-boxbot-the-dancing-robot,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 20:34:44,"Inspiration

As computer engineers, we wanted to work on a project that incorporated our passion for hardware and software, so we decided to create a robot. Our goal was to entertain -- to make your day a little brighter! That's why we decided on a dancing robot.

What it does

BoxBot dances using 3 movement systems -- arms, legs, and feet. 

How we built it

We built BoxBot using an Arduino Uno, a motor shield, 6 continuous servo motors, and many custom 3D-printed parts. We started by researching Arduino-based robots built by other engineers. After scouring for parts at the MLH hardware store, we realized that our robot would have to employ servos for movement. Using Fusion 360, we developed a simple case to house our components, accounting for tolerances, cable management, and potential redesigns. While our parts were being printed, we used a few paper coffee cups to develop a working prototype. Attaching our arms and legs to the coffee cups, we built and tested movement code using the Arduino IDE. Once our parts were printed (not without a few hiccups that led to slight redesigns), we put it all together and started programming the actual dance moves. 

Challenges we ran into

We faced challenges 3D-printing our parts, finding working, usable components, and coding our robot's dance moves. During a pivotal moment in our development process, we had to decide between speed and quality: either we could 3D print our parts at full speed, sacrificing some structural integrity, or we could print them at regular speed, maximizing sturdiness. At different points in our timeline, we prioritized different goals. Early on, we were willing to give up some time for higher-quality parts, but as the Bitcamp timer inched closer and closer to 0:00, our goal became time efficiency. We also faced challenges finding a functioning Arduino Uno and servo motors. This led to a ritual in which we systematically tested every component we added to our robot, ensuring nothing would go wrong during showcases. Finally, our code took perhaps the most time to develop. We had to develop a ""home base"" for our servos, specific angles in which they should return to when at rest. Then we had to test various speeds and delay times to develop an effective dance routine. 

Accomplishments that we're proud of

We're incredibly proud of how we used our environment to our advantage during the build process. Instead of waiting 6+ hours for our parts to finish printing, we used this valuable time to build a makeshift robot using coffee cups and cardboard we found lying around. We also searched for other hardware teams to ask questions and get feedback on our project, which led to major improvements in our design. 

What we learned

We not only learned the art of patience but also furthered our understanding of computer-aided design, low-level programming for hardware, and the overall engineering design process. Although we had previous experience working on engineering projects, this was the first that operated under very strict time constraints. We pulled all-nighters, ate while working, transferred leadership whenever either of us felt burned out, and learned more about our individual strengths and weaknesses when working under pressure!

What's next for BoxBot - The Dancing Robot!

We want to develop a more streamlined version of BoxBot -- one that incorporates more soldering to minimize cable clutter, less tape to improve aesthetics and stability, and some more personality -- maybe some paint, stickers, or googly eyes! 
","",https://youtube.com/shorts/Gp6mbKAS61I?feature=share,"","c++, arduino, 3dprinting, autodesk-fusion-360, circuits",Talha Hossain,Khan,talhahossain75@gmail.com,,https://github.com/TalhaHossainKhan/Bitcamp2025-BoxBot,Best Razzle Dazzle Hack,Best First-Time Hack,Best Hardware Hack,Hack That Made You Smile,Prettiest Hack,"We interacted with the 3D printers, hardware tools, and Arduino components provided at this hackathon. While the 3D printers and tools were great, we had a tough time finding usable Arduino parts. Several of our motors came jammed, and our first Arduino was not being recognized by our laptops. Nevertheless, we appreciate the variety of hardware options provided!","University of Maryland, College Park",DeepSeek,N/A,N/A,"","University of Maryland - College Park, mcst",1,Tanish,Parlapall,lameravens@gmail.com
Natch,https://bitcamp2025.devpost.com/submissions/642587-natch,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 20:48:16,"Inspiration

We were inspired by how Scratch lowers the barrier for coders and with machine learning at the cutting-edge of computer science, wanted to do the same for neural networks. We were also inspired by a project that visualizes network packet. Computer science concepts are typically more important than the syntax of coding, but visualizations in computer science are rare. 

What it does

Natch lets you drag and drop blocks to add parameters to a neural network, then provides code and runs your neural network. 

Below is a list of all of the features:


Drag and drop: Drag blocks and see what parameters you must add before generating your code and your visualization
Tooltips: See an explanation of every single function to guide you in your creation of the neural network
Choose number of input and output nodes
Add any number of hidden layers
Each layer has an activation function
The entire network has an optimizer and a loss function
Error handling: you will receive an alert if you have not provided all parameters, in the right format
Code generation: Generates concise PyTorch code
Visualization: View the graph of the neural network to build your intuition for deep learning
In-workspace clipboard: Use keyboard shortcuts and right click menu to copy, paste, undo, redo, and cut
Move blocks to the trash when you're done using them or if you made a mistake
Copy your generated Python code to your clipboard


How we built it

We used Blockly, the same engine that runs code.org, MIT's Scratch, and AppInventor, to build a modern interface for block coding. We carefully planned a UI that simplifies the process of building a deep learning model. We also incorporated a syntax highlighting library called react-syntax-highlighter in the generated Python code.  

Challenges we ran into

React and Blockly have a react-blocky nom package that is supposed to use Blockly in a more React style. It turns out the react-blockly library was poorly documented and it ended up causing more problems than solutions. We ended up abandoning the react-blockly. 

We tried embedding a lite Jupyter notebook instance, but we are only allowed to use a public notebook, not a private instance for each user. We decided to scrap the idea and prompt the user to run their code separately in a notebook. 

We received a strange react error that showed up on all but one computer. 

Accomplishments that we're proud of

We built a block coding tool with a beautiful idea and an original idea in only about 16 hours over 2 days!

~835 lines of code

What we learned

We learned about Blockly and the power of prompt engineering. Some of us were using React and JavaScript for the first time. 

What's next for Team Natch


Other kinds of networks: CNNs, NLPs
courses and templates. Natch is an educational tool at its core. 
adding parameters to optimizers
display the activation function as a symbol in our visualization

","https://natch-five.vercel.app/, https://github.com/ObfuscatedFuture/natch",,Best Web Hack Using React - Peraton,"blockly, gpt4o, visual-studio-code, react, vercel, node.js, css, pytorch",Varun,Singh,varun.singh2026@gmail.com,,https://github.com/ObfuscatedFuture/natch,Best App Dev Track Hack,Best UI/UX Hack,Best Bitcamp Hack,"",MOST LIT HACK,"We had more problems than expected with Node and NPM. This was our first time coding with AI for many of us and we were impressed with how quickly we could code.
We also were very permissive with using libraries. Blockly was transformative for us, which we switched to on Day 2.
Most of the team did not know Git well and we misunderstood ""git revert."" Even though we could have used a git command we knew, we kept making our merge conflicts worse at the very end of our project once everything was functional. Lesson learned, pull every 5 minutes...",University of Maryland,OpenAI and Gemini,N/A. We implemented code generation for a neural network,N/A,"",University of Maryland - College Park,3,Kevin,Li,kli36@terpmail.umd.edu,Chase,LaBarre,obfuscatedfuture@protonmail.com,m-jhin,Jin,maxjin125@gmail.com
CallerIDK,"",Draft,Pending,Project overview,04/12/2025 20:55:22,"","",,"","",Sri Tarun,Gulumuru,sritarung96@gmail.com,,"","","","","","","","","","","","","City College of New York of the City University of New York, The",2,Pragyam,Tiwari,pragyamtiwari@gmail.com,Pragyam,Tiwari,pragyamtiwari47@gmail.com
Untitled,"",Draft,Pending,Manage team,04/12/2025 21:03:23,"","",,"","",PandaMikey05,Hajasz,mikeysosilly@outlook.com,,"","","","","","","","","","","","",College of William and Mary,0
Pangaea DDE,https://bitcamp2025.devpost.com/submissions/642600-pangaea-dde,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 21:13:24,"Inspiration

Our inspiration is very close to home, as our group members have family members who struggle with chronic illnesses. As a result, we wanted to create a way to spread awareness for the most prevalent chronic illnesses worldwide. We started with dementia, since it's very personal to one of our members who lost a loved one with dementia.

What it does

Pangaea DDE is a 3D visualizer for global disease data. It takes disease data and displays it as a heatmap projected onto the globe. Users can interact with the globe by clicking on countries, and are given a pop-up with information on the population, income group, subregion, and dementia prevalence (as a rate per 100,000). Additionally, they are presented with AI Insight, courtesy of the Gemini API, which provides analysis on dementia in the country, as well as sources to learn more about dementia. 

How we built it

We used the following tools to build our project: 
Cesium, an open-source JavaScript library for 3D Geospatial Applications as our visualization method. 
React to use Cesium. 
Python to merge our data. 
Gemini API to present insights and analysis of dementia data per country. 

Challenges we ran into

Initially, we wanted to use Google Earth as our visualization method, but its API has been deprecated since 2015. 
Our group had some issues with GitHub Source Control, since two branches diverged too much when refactoring code. 
The names of countries didn't match between the geojson and the GBD data.

Accomplishments that we're proud of

We accomplished what we set out to do!!! 
While the code isn't super polished, everything that we envisioned at the start of Bitcamp is functional. In particular, getting the heat map to work and overriding the default Cesium infobox with our custom information popup was a huge milestone

What we learned

We learned how to use the Gemini API, and a lot more about React. 
We read the Cesium docs to learn how to use the library. 

What's next for Pangaea DDE

We want to increase awareness of diseases across the globe, and aggregating data so that users can view and compare the prevalence of something like Parkinson's vs dementia globally would further that goal. 
We also want to make the transition between years smoother so users can see changes in the heat map as they move through the slider, as well as make it possible to directly compare data between years.
",https://github.com/ChrisC920/BitCamp2025,https://youtu.be/SxHfL0LWPzQ,"Best Hack Promoting Public Health - Bloomberg, [MLH] Best Use of Gemini API","python, json, cesium, react.js",Arya,Patel,patela3@terpmail.umd.edu,,https://github.com/ChrisC920/BitCamp2025,Best Social Good Hack,Best First-Time Hack,Best Razzle Dazzle Hack,Prettiest Hack,Hack That Made You Smile,"We used Gemini in our project, and it was very smooth and easy to work with. We were learning a lot about the technology we worked with this weekend, and this included Gemini. Generating an API key and calling the Gemini API added very little work for us. Would recommend.",University of Maryland,OpenAI and Gemini,"We integrated Gemini API to give summaries on how dementia affects each country, as well as a predictive model which looks to see how dementia will look in that region in the future",GEMINI_API_KEY=AIzaSyDrEBtAWkkIXDOxgHN7ZFrnDt34EvNNBVs,"",University of Maryland - College Park,2,Leo,Jiang,ljiang05@terpmail.umd.edu,Christopher,Cho,christophercho0916@gmail.com
Raccoon's Port Scanner,https://bitcamp2025.devpost.com/submissions/642601-raccoon-s-port-scanner,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 21:13:43,"Inspiration

I wanted to create a tool that blends network with real world vulnerability. Most basic port scanners stop at identifying open ports, but I wanted to take it further and tell the user what those ports mean for security.

What it does


Find all open TCP ports.
Grav service banners.
Identify known vulnerabilities using a local CVE database or live CVE API lookup.
Export the results to a structured JSON report.


How I built it

I built the port scanner in Python using the following:


socket for port scanning and banner grabbing.
requests to pull CVE data from CIRCL's public API.
colorama for clean, color coded output.
Flask to simulate a vulnerable service for testing.
A small local JSON CVE database to support offline use.


Challenges we ran into


Accurately parsing service banners.
Ensuring compatibility with macOS Python and terminal environments.
Handing slow or unresponsive ports without blocking the entire scan.


Accomplishments that we're proud of


Built a working scanner that grabs banners and identifies CVEs.
Created a simulated test environment using Flask for reliable demos.
Packaged everything cleanly with outputs and an exportable report.


What I learned


How to work with low level network sockets in Python.
How banner grabbing and CVE enumeration work.
How to use APIs.


What's next for Raccoon's Port Scanner


Add multi threading to increase speed.
Add UDP port scanning and more like FTC, SMTP, etc.
Build a front end for the app.
Integrate machine learning to flag abnormal services/ports.

",https://github.com/Aads1/Racoons-Port-Scanner,,"","python, socket, requests, clorama, flask, argparse, json",Aaditya,Mandvilkar,aads@mandvilkar.com,,https://github.com/Aads1/Racoons-Port-Scanner,Best Cybersecurity Track Hack,Best First-Time Hack,Best Bitcamp Hack,MOST LIT HACK,Hack That Made You Smile,"I used the CIRCL CVE Search API to add real time vulnerability detection. It was easy to use and didn't require an API key, which was great for a hackathon. I also used Flask to simulate a vulnerable server.",West Windsor Plainsboro High School South,I did not use GenAI,I implemented the CIRCL CVE Search API to add real time vulnerability detection,N/A,"",West Windsor Plainsboro High School South,0
TerpOracle,https://bitcamp2025.devpost.com/submissions/642602-terporacle,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 21:13:45,"Inspiration

Choosing the right courses and balancing a schedule at UMD can be overwhelming. Students often rely on scattered information from Testudo, PlanetTerp, and word-of-mouth. I wanted to create a centralized tool that not only gathers this data but also uses the power of generative AI to provide a holistic, insightful analysis of a student's potential semester schedule, helping them make more informed decisions.

What it does

TerpOracle analyzes a University of Maryland (UMD) student's class schedule to provide comprehensive feedback and scores across several key categories. Users can either:


Upload an image of their schedule (e.g., a screenshot from Testudo).
Manually enter their Course IDs (e.g., CMSC430) and Section numbers (e.g., 0201).


The application then:


Extracts course information (if using image upload) using the Google Gemini Vision model.
Fetches official course details (professor, time, title) by scraping the UMD Testudo website.
Gathers professor ratings and reviews using the PlanetTerp API.
Sends the combined course, professor, and review data to the Google Gemini (2.0 Flash) model for analysis.
Generates concurrent AI analyses for each individual course using asyncio, covering aspects like Teaching Quality, Difficulty, Workload, Grading Fairness, etc.
Produces a final overall schedule analysis, synthesizing the individual course feedback and assigning scores for Overall Workload, Professor Quality, Schedule Balance, Subject Synergy, Difficulty Management, and an Overall Schedule Grade.
Displays the results in a clean web interface, including the overall grade and detailed breakdowns for each course with basic Markdown rendering.


How I built it


Frontend: Built with HTML, CSS, and vanilla JavaScript. The interface is clean and intuitive, inspired by Apple's aesthetic. It supports both image upload and manual entry, and uses the Fetch API for backend communication.
Backend: A lightweight Flask server in Python handles routing, input parsing, and orchestrates the schedule analysis pipeline.
Core Analysis Script (enhanced_schedule_analyzer.py):


Integrates the Google Gemini API via the google-generativeai library for both vision and text analysis.
Scrapes Testudo data using requests and BeautifulSoup4.
Retrieves professor data from the PlanetTerp API.
Uses asyncio to parallelize Gemini calls for each course, improving speed.
Supports both image input and structured JSON input via argparse.



Data Flow:


User submits input (image or JSON + API key + term).
JavaScript sends this data to the Flask backend.
Flask calls the analyzer script with the appropriate arguments.
The script fetches and analyzes course/professor data.
Results are saved to schedule_data.json.
Flask returns the JSON to the frontend.
JavaScript dynamically renders the schedule analysis results.


Challenges I ran into


Testudo Scraping: HTML inconsistencies made parsing challenging.
Data Integration: Merging scraped HTML, JSON API data, and image OCR.
Prompt Engineering: Fine-tuning prompts for accurate, structured Gemini output.
Concurrency: Managing asyncio workflows and debugging edge cases.
Input Flexibility: Supporting both image and manual text input cleanly.
Frontend Formatting: Handling display formatting and Markdown rendering in vanilla JS.


Accomplishments that I'm proud of


Successfully integrated vision and text AI with traditional scraping and APIs.
Achieved real-time, concurrent Gemini-based analysis.
Built an intuitive, multi-input interface with clean UI/UX.
Delivered a product that directly helps UMD students make better schedule decisions.


What I learned


Real-world application of large language models via APIs.
Robust web scraping with requests and BeautifulSoup.
Flask API design and dynamic frontend-backend integration.
Asynchronous programming with asyncio.
Effective UI design and iterative debugging based on feedback.


What's next for TerpOracle


Improve Scraping Robustness: Adapt to layout changes on Testudo.
Deeper Analysis: Consider commute time, course prerequisites, and topic diversity.
Support More Schools: Generalize scraping logic to support other universities.
Add User Accounts: Save analysis history and API keys for frequent users.
Improve Markdown Rendering: Provide richer formatting support.
Refine Prompts: Tune Gemini input for greater consistency and reliability.

","https://github.com/andrewxie04/terporacle, https://youtu.be/LAtjWFB434w",https://youtu.be/LAtjWFB434w,[MLH] Best Use of Gemini API,"python, gemini, flask, javascript",Andrew,X,andrewxie2004@gmail.com,,https://github.com/andrewxie04/terporacle,Best Machine Learning Track Hack,Best UI/UX Hack,Best Social Good Hack,Prettiest Hack,Hack That Made You Smile,"My experience this weekend focused on integrating several key technologies for TerpOracle:

Google Gemini API: The Python library was easy to use. The gemini-2.0-flash model delivered fast, insightful analysis. Implementing concurrent calls with asyncio was crucial for performance when analyzing multiple courses.
Python (Flask, Requests, BeautifulSoup): Flask provided a simple backend. requests worked well for the PlanetTerp API. Scraping Testudo with BeautifulSoup was effective but highlighted the potential fragility compared to official APIs.
HTML/CSS/JS: Standard frontend technologies were used effectively for the UI and backend communication.
Git/GitHub: Used standard workflow for version control and deployment.
Overall, combining the Gemini API with Python web tech and external data sources was powerful. The main challenges were prompt engineering for Gemini and the inherent instability of web scraping.",University of Maryland College Park,Gemini,"Yes, I implemented the Google Gemini API (specifically gemini-2.0-flash via the Python library). I used it to analyze combined data scraped from Testudo and fetched from the PlanetTerp API (course details, professor reviews). Gemini generated detailed summaries, qualitative assessments, and quantitative scores for individual courses and the overall schedule, providing insights that simple data aggregation couldn't achieve. It also optionally used the vision capabilities for initial course extraction from images.",689998774065,"","",0
AlphaParse,https://bitcamp2025.devpost.com/submissions/642604-alphaparse,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 21:17:39,"Inspiration

We were inspired by a demo done at the Bloomberg table of the Bloomberg Terminal; the intuitive design and level of accuracy are what we aimed to add to AlphaParse.

What it does

Alpha Parse uses Gemini to allow the user to query financial information directly from SEC filings. Quering includes but is not limited to creating bar graphs, pie charts, and line charts from accurate quantitative datapoints found in a company's filings.

How we built it

The app was built using Next.js as the React framework. The front end is built with Tailwind, a CSS framework. The backend is built in TypeScript.

Challenges we ran into

We ran into git challenges, we were developing very quickly on different branches, and found it challenging to manage all of our commits and branches!

Accomplishments that we're proud of

We are proud of the final product, we feel that the project is useful to us.

What we learned

We learned how to navigate the SEC's API endpoints.

What's next for AlphaParse

In the future, we plan on adding more support for international securities and potentially other asset classes (i.e Bonds, Forex, Cryptocurrency, Commodities).
",https://github.com/lukasozpaker/bitcamp25,,"Best Financial Hack - Capital One, Best Digital Forensics Related Hack - Cipher Tech Solutions, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best Use of Gemini API","react, typescript, tailwind, next.js",Esteban,Peredo,eperedo4@gmail.com,,https://github.com/lukasozpaker/bitcamp25,Best Bitcamp Hack,Best UI/UX Hack,Best Moonshot Hack,"","","Github, React, Next.js, Tailwind, Typescript, Gemini, SEC api, Finnhub api",University of Maryland,Gemini,Gemini was a key part of our app as it allowed us to parse multi-million token sec filings in seconds.,632069902471,"",University of Maryland - College Park,1,Lukas,Ozpaker,lukas.ozpaker@gmail.com
BizCamp,https://bitcamp2025.devpost.com/submissions/642606-bizcamp,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 21:20:06,"Inspiration

We drew inspiration from Salesforce/CRM Software. 

What it does

Our goal is to help increase company workflow by providing real-time meeting transcriptions, smart meeting summaries, concept graph generation, and an AI chatbot!

How we built it

We built this with Next.js, FastAPI, MongoDB, Qdrant, GitHub, and AssemblyAI. 

Challenges we ran into

We spent a lot of time defining our database structure and deciding our tech stack to ensure our application is deployable and scalable. 

Accomplishments that we're proud of

We are proud of building a deployable and scalable application, while ensuring best software security practices. 

What we learned

We learned about MongoDB and especially learned how Vector Databases work through Qdrant. 

What's next for BizCamp

Integration with with Zoom, Google Meet, Teams, and Slack. Add emotion tracking. Utilize more powerful LLMs and statistics. This would allow for more interesting data to be displayed for teams, such as attendee participation during meetings.
",https://github.com/aluthra23/bizcamp,https://youtu.be/0aQxmoBndzs,"Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","next.js, mongodb, react.js, qdrant, gemini, assemblyai, fastapi",Arav,Luthra,aravluthra@gmail.com,,https://github.com/aluthra23/bizcamp,Best App Dev Track Hack,Best Bitcamp Hack,Best Razzle Dazzle Hack,Prettiest Hack,"","We are using GitHub, MongoDB, FastAPI, Next.js, Qdrant.","University of Maryland, College Park",Gemini,"Yes,  we used Gemini's 2.0-Flash model through Gemini's API to generate AI content and also support our AI chatbot.",49729173544,"",University of Maryland - College Park,2,Vedant,Patel,vedantpatel021@gmail.com,Narain,Sriram,nsriram@terpmail.umd.edu
Corporate Chaos,"",Draft,Pending,Project overview,04/12/2025 21:29:37,"","",,"","",Mazinkhan,Nadaf,mnadaf@terpmail.umd.edu,,"","","","","","","","","","","","",University of Maryland - College Park,0
Untitled,"",Draft,Pending,Manage team,04/12/2025 22:00:39,"","",,"","",Grish,Kinger,grishkinger2008@gmail.com,,"","","","","","","","","","","","",Greenbrier High School,0
Bytecamp,https://bitcamp2025.devpost.com/submissions/642637-bytecamp,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 22:10:14,"Inspiration

Bytecamp is incredibly unique and totally not influenced by Bitcamp. We were inspired from our own journeys at Bitcamp, creating projects and embarking on side-quests. We aimed to encapsulate the entire hackathon experience with Bytecamp, simulating everything from working on your project, refueling with food, and stealing the moon with your teammate. 

What it does

Bytecamp is a hackathon simulator where you, Joe HackaThon, are trying to build the best possible project. You have various personal stats, such as Hunger, Morale, and IQ, as well as project stats, such as UI/UX Experience and Originality.

How we built it

We developed Bytecamp using the Godot game engine with GDScript and collaborated using Git. 

Challenges we ran into

While we all have experience developing games in Unity (shoutout CMSC425!), only Yuji had experience working in Godot. There was a slight learning curve at first with the Godot engine, but GDScript is very similar to Python so it was easy to integrate functionality through code. It was definitely a challenge, but we all learned a lot and are very happy with the final product.

Accomplishments that we're proud of

We are particularly proud of the visual elements in our game. The UI is filled with smooth animations, it is very responsive, and has a lot of unique interactions. Additionally, every element you see in Bytecamp is created in Aseprite. A lot of time went into creating unique characters, scenery, and animations. 

Overall, we are very proud of Bytecamp—it is fun, has a simple but unique gameplay loop, visually appealing, and funny.

What we learned

We all learned a ton about Godot and GDScript, as well as working together well as a team!

What's next for Bytecamp

One stretch feature idea we had was implementing mini-games you can play after events occur. While we didn't have time to add them into Bytecamp for Bitcamp, it would be awesome to continue to work on our game and make it even more fun to play.
",https://yuwex.github.io/bitcamp2025/,,"","godot, gdscript",Carson,Dalziel,carsonwdalziel@gmail.com,,https://github.com/yuwex/bitcamp2025,Best Bitcamp Hack,Best Razzle Dazzle Hack,Best Gamification Hack,Hack That Made You Smile,Prettiest Hack,Godot: It's too godoated,"University of Maryland, College Park",OpenAI and Gemini,N/A,N/A,"",University of Maryland - College Park,2,Noah,Kasica,nkasica21@gmail.com,Yuji,Wexler,yuji@terpmail.umd.edu
Relay,"",Draft,Pending,Project details,04/12/2025 22:13:55,"Inspiration

What it doesIn any hospital or clinic, the front desk is the heartbeat of operations. But with constant phone calls, scheduling chaos, and repeated questions, even the best staff get overwhelmed. Patients are left on hold. Voicemails go unanswered. Stress builds.

We asked: What if hospitals had a voice assistant that could actually help? One that listened like a human, responded clearly, and supported staff without ever replacing them?

That’s why we built Relay — to help healthcare teams listen better.

What we built

Relay is an AI-powered voice agent designed specifically for hospitals and clinics. It picks up every incoming call — whether it’s 2 PM or 2 AM — and engages in a clear, human-like conversation with the patient. From there, it can book appointments, handle administrative requests, answer common questions, and summarize the entire call for staff to review. All of this happens in real time.

What makes Relay different is that it’s not just a voice-activated chatbot with a script.
It sounds real. It reacts naturally. It pauses, adapts, and speaks with tone. Patients don’t feel like they’re talking to a machine — they feel heard.

Whether someone is calling to schedule a check-up, ask about billing, request a prescription refill, or leave a message after hours, Relay steps in as a reliable digital coworker — lightening the load for front desk staff and improving the patient experience from the very first “hello.”

It’s like having a trained staff member who never misses a call, never needs a break, and always remembers the details — because those details become a clear, structured summary for hospital staff to act on immediately.

How we built it

Challenges we ran into

Accomplishments that we're proud of

What we learned

What's next for Relay

-include multilingual support
","",,"",openai,Vibhav,Thiru,vibhavt06@gmail.com,,"","","","","","","","","","","","","George Mason University, University of Virginia",2,Saravana,Balaji,saravanabalaji.m2005@gmail.com,Azim,Abdulmajeeth,abdulmajeethazim9@gmail.com
Impause,https://bitcamp2025.devpost.com/submissions/642644-impause,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 22:19:51,"Inspiration

Many people struggle with impulse spending without realizing the long-term effects on their financial well-being. We wanted to create a tool that not only increases awareness but also makes financial responsibility engaging and collaborative. That’s how Impause was born—built to help users pause, reflect, and regain control of their spending habits.

What it does

Impause is a personal finance companion that helps users curb impulse purchases, track spending patterns, and stay financially accountable.
Key features include:


Impulse Buying Buffer: A pause mechanism before purchases to reflect on cost, goals, and time value.
Finance Wrapped: A fun, visually rich summary of your weekly, monthly, or yearly spending—like Spotify Wrapped for your wallet.
Accountability Buddy: A feature to pair with a friend to share progress and stay motivated through light social accountability.


How we built it

We used React, Vite, and Tailwind for the front end. We utilized Cloudflare Workers for the backend, and AWS, Node.js, Express, and MongoDB Atlas for authentication. 

Challenges we ran into

The main challenge we ran into was trying to integrate authentication in the front end and configuring the firewall. 

Accomplishments that we're proud of

We were able to deploy a functional web app. 

What we learned


Behavioral psychology plays a huge role in financial decisions.
Small UX decisions (like how long to delay a purchase) can significantly impact user satisfaction.
People respond positively to transparency, gamification, and social reinforcement in managing money.


What's next for Impause


Integrating with bank APIs for seamless, automatic expense tracking.
Expanding the Finance Wrapped to include goal tracking and streaks.
Launching mobile notifications and widgets for real-time impulse purchase alerts.

",https://impause.tech,,"Best Financial Hack - Capital One, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best AI Application Built with Cloudflare, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","react, workers, cloudflare, mongodb, node.js, gemini, express.js, passport, amazon-web-services",Richard,Sun,schoolwork@richardsuns.space,,https://github.com/finpause/impause,Best App Dev Track Hack,Best Bitcamp Hack,Best First-Time Hack,Prettiest Hack,Hack That Made You Smile,"MongoDB Atlas, React, Node.js, Express, Cloudflare Workers, GitHub.","Rensselaer Polytechnic Institute, Case Western Reserve University, University of Pittsburgh, University of Maryland","OpenAI, Anthropic, Gemini, and Other","Yes, we implemented Google Gemini to read off bank statements and come up with conclusions for wrapped.",891889355951,https://impause.tech,"Rensselaer Polytechnic Institute, Case Western Reserve University, University of Maryland - College Park",3,Ethan,Wang,ethan.wanq@gmail.com,Eric,Liu,ericliu2006@gmail.com,Abdur-Rahman,Shakir,arshakir999@gmail.com
Hiding in Plain Sight,https://bitcamp2025.devpost.com/submissions/642655-hiding-in-plain-sight,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 22:33:04,"What it does

Hiding in Plain Sight is a lightweight, high-speed tool designed to uncover hidden data embedded through LSB and DCT steganography techniques. It supports batch processing and also performs advanced statistical analyses, including chi-square and histogram evaluations, to detect anomalies and potential signs of data concealment.

How we built it

We built the tool using Python, leveraging libraries like Pillow, NumPy, SciPy to extract and analyze data from the images. 

Accomplishments that we're proud of

We’re proud of building a tool that works on a wide range of image files and can handle bulk processing efficiently. All of the easter eggs we put in the UI.

What we learned

We gained a deeper understanding of steganography and the world of digital forensics.
",https://github.com/braynguyen/Hiding-in-Plain-Sight,,Best Digital Forensics Related Hack - Cipher Tech Solutions,"python, flask, javascript",Brayden,Nguyen,btn16@pitt.edu,,https://github.com/braynguyen/Hiding-in-Plain-Sight,Best Cybersecurity Track Hack,Best UI/UX Hack,"","","",The liveshare extension on VSCode was super nice when we were in our initial stages and trying out tons of designs.,University of Pittsburgh,OpenAI and Gemini,N/A,N/A,"",University of Pittsburgh - Main Campus,2,Nij,Patel,nijpatel28@gmail.com,Julian,Alamo-Rosas,jalamorosas@gmail.com
SnortEduGuard – Student Integrity Surveillance System,https://bitcamp2025.devpost.com/submissions/642657-snorteduguard-student-integrity-surveillance-system,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 22:34:19,"Inspiration

As a Teaching Assistant at the University of Maryland, I frequently conduct lab sessions, quizzes, and in-class exams. I've seen firsthand how students try to find creative ways to bypass academic rules, especially using generative AI tools and VPNs. While solutions like LockDown Browser exist, they can often be circumvented, and they only monitor the browser, not the network. That inspired me to build something that works at the network layer: a real-time, intelligent intrusion detection system for classrooms and exams that helps preserve academic integrity in a meaningful way. This was the student's focus, which will be redirected to learning from the lectures, doing homework genuinely, and earning the real learning, which will prepare them for the real world.

What it does

SnortEduGuard is a real-time academic intrusion detection system designed specifically for educational environments. It:


Uses Snort 3 to detect access to generative AI tools, VPNs, collaboration platforms, and study-help websites
Parses Snort alerts into structured JSON using a custom Python script
Displays alerts in a real-time dashboard with charts, filtering, exporting options, and instructor login
Includes an AI-powered Smart Search assistant that understands natural language queries like “Show Chegg alerts in last 120 minutes” and summarizes results in plain English
Supports an “Exam Mode” that flags any traffic not explicitly allowed (Canvas, UMD, Zoom, etc.)
The system is production-grade and could be deployed during live exams or quizzes.


How I built it

The architecture is modular and fully functional:

Snort 3 Detection Engine: I configured Snort 3 with custom rules to detect unauthorized domains and behaviors. The rules were written to inspect TLS SNI headers, identify protocol types, and categorize alerts by AI tools, VPNs, C2 behavior, or whitelisted services.
I wrote custom Snort rules to detect traffic to:


AI tools (ChatGPT, Bard, Claude, Terp AI, Perplexity)
Study sites (Chegg, CourseHero, Quizlet, Studocu)
VPN services (NordVPN, ProtonVPN, TunnelBear, Psiphon)
Collaboration platforms (Discord, Slack, WhatsApp)
C2 behavior and port scanning (DNS tunneling, Nmap)


Logging and Parsing: Snort logs alerts to /var/log/snort/alert.fast. A custom Python parser (parse_alerts.py) uses regex to extract alert fields like timestamps, IPs, protocols, priorities, and SIDs. It saves these to a structured file (parsed_alerts.json) used by the dashboard.

Dashboard: Using Flask, Bootstrap 5, and Chart.js, I created a dashboard that displays alerts in real time. It auto-refreshes every 10 seconds, shows a visual breakdown of priority distribution, and allows CSV/PDF exports. The UI includes a login system and displays alerts with color-coded highlights for quick analysis.

Smart Search Assistant: The real “wow” factor is an AI-powered assistant. Instructors can type prompts like ""Show Chegg alerts in last 30 minutes."" The backend (parse_query.py) uses spaCy NLP to understand intent, and then smart_filter.py filters alerts. Finally, summarizer.py generates plain-English interpretations like:
""192.168.1.105 attempted to access ChatGPT at 6:42 AM.""

Exam Mode: I implemented a fallback default-deny rule that flags any traffic not matching whitelisted domains. This allows instructors to switch into a strict mode where only Canvas, UMD, or Zoom are permitted, helping enforce academic integrity during exams.

Challenges I ran into

One of the hardest parts was configuring Snort 3 logging with Lua and ensuring it streamed reliably to alert.fast without truncation or corruption. I had to carefully coordinate logging, parsing, and visualization to work asynchronously without conflicts.
The Smart Search assistant also required careful tuning so that natural language queries could be accurately interpreted and mapped to real categories like “AI” or “VPN,” even when phrased differently.
Styling the dashboard to be clean, intuitive, and visually meaningful under time pressure was also a significant design challenge, especially for me, as I have never done this before in my life. A huge learning curve for me!!

Accomplishments that I're proud of


Built a fully functional, real-time IDS dashboard that actually runs and detects live network traffic
Created over 40+ custom Snort rules with domain-specific focus on AI and academic tools
Developed a complete AI-powered Smart Search Assistant using NLP and summarization
Designed a polished frontend with auto-refresh, charts, export options, login system, and whitelisting visualization
Implemented Exam Mode using a “default-deny” rule to detect all unauthorized traffic
Built the system in a way that is scalable, real-world ready, and impactful for classrooms
Built everything independently - from rule design to AI assistant - within 36 hours, fully operational on a local macOS machine. It is, for now, limited to me as I tried to deploy it for a range of IPs, the test was successful, but later my internet connectivity by UMD has been terminated and for that reason, I'm demonstrating this for my local machine now, or else, this is production, mass use ready.


What we learned

First and foremost, ""TIME IS MONEY""!

I deepened my understanding of:


Network-level traffic inspection and deep packet analysis
Snort 3 rule syntax, logging mechanisms, and Lua-based configuration
Building Flask web applications with real-time features
Using spaCy for NLP and building AI features into security workflows
Designing systems that are not just technically correct, but also usable and valuable to real-world users-in this case, instructors and academic institutions


It reinforced my belief that cybersecurity is not only about preventing attacks; it is also about integrating trust and fairness into the systems we use daily with the security knowledge I possess.

Time is crucial. If I have an idea, that's good; the only thing that matters in a hackathon is how to think of the idea, execute it, build a model, enhance it, exceed your own expectations, and, above all, submit it on time.

What's next for SnortEduGuard – Student Integrity Surveillance System

There’s still so much I’m excited to build on top of what’s already working.

First, I’d love to deploy the entire system on a Raspberry Pi, plugged into a SPAN port in a classroom network. That way, it can passively monitor traffic without needing any installation on student devices.

I also want to build a policy configurator—a simple UI where instructors can upload custom whitelists or choose modes like “Quiz Mode” or “Normal Class Mode.” Right now, modes are enforced through rules, but making this user-friendly would make the tool way more accessible.

Another idea I’m exploring is integrating Claude or OpenRouter to generate even more intelligent summaries. Imagine an AI not just summarizing what happened, but also suggesting:
""You may want to follow up with students 192.168.1.5 and 192.168.1.9 who accessed Chegg during the assessment window.""

It would also be awesome to add live alerting—like sending instructors an email, a Slack message, or even a quick text when something suspicious happens in real time.

I definitely want to keep improving the Smart Search assistant so it supports follow-up questions, more nuanced queries, and even incident response guidance. That’s where things could really start to feel like a security analyst-in-a-box.

And eventually, I plan to open-source the project, with clear documentation, setup scripts, and a ready-to-use classroom deployment guide. I want other educators and researchers to build on top of this and adapt it to their own environments.

This project started with one simple question: “Can I build something that actually helps instructors maintain integrity in a real classroom?” Now that it’s working, I want to take it further—make it more intelligent, more scalable, and more useful to others.
","https://github.com/akshatpatel64/SnortEduGuard, https://akshatpatel64.github.io/#projects",https://youtu.be/F4_onsXjv2s?si=ucW2xNqSfkxO8h1n,"Best Digital Forensics Related Hack - Cipher Tech Solutions, Best Use of GenAI in Business - Cloudforce/Microsoft, [MLH] Best Use of .Tech","snort-3, lua, python, flask, bootstrap-5, chart.js, spacy, jinja, html5+css3, macos, vs-code, regex, wkhtmltopdf, pdfkit, csv+json, terminal, python-daemon, snort-3.7.2",Akshat Dharmesh,Patel,akshat64@umd.edu,,https://github.com/akshatpatel64/SnortEduGuard,Best Cybersecurity Track Hack,Best Bitcamp Hack,Best First-Time Hack,Hack That Made You Smile,MOST LIT HACK,"This weekend, I went deep into technologies that power real-world security systems. My project, SnortEduGuard, was heavily built around Snort 3, Python, GitHub, Chart.js, and Flask, with a special focus on making it feel production-ready rather than just a demo.

One of the biggest learning curves was working with Snort 3 on macOS - specifically configuring custom .rules files, working through the snort.lua configuration, and redirecting logs from stdout to persistent files like alert.fast using tee. It gave me hands-on insight into how packet inspection tools work under the hood and how tricky it can be to engineer stable logging in real-time environments.

I also leveraged GitHub throughout the weekend, and I honestly appreciated how clean the repo management and branching tools were when collaborating between development and documentation. The new GitHub CLI advice and keychain credential integration saved time, especially under hackathon deadlines.

For the frontend, Chart.js was a pleasant surprise. It let me create live-refreshing visualizations of alert data in under 200 lines of code, and the ability to combine it with Flask and Bootstrap 5 helped deliver a clean UI with minimal effort.

I also explored spaCy, which was critical for my Smart Search feature. It’s lightweight, Pythonic, and surprisingly effective for parsing natural language queries in real-time, even without external APIs or LLMs.

All in all, this was more than a weekend project - it felt like building a real cybersecurity tool. I’m leaving with a stronger command of security logging, Python automation, and integrating AI with traditional infrastructure, and of course, a lot of sleep in my eyes!","University of Maryland, College Park",OpenAI and Other,"No, I did not directly implement a generative AI model or API like OpenAI or Anthropic.
Instead, we built a custom AI-powered log assistant using spaCy for natural language parsing. My goal was to simulate a GenAI-like experience for instructors and TAs using purely local and lightweight AI methods.

The assistant interprets queries like ""Show VPN access in the last 30 minutes"" and maps them to Snort log filters. It then summarizes the results into clear, readable English (e.g., “192.168.1.105 attempted to access ChatGPT at 10:42 AM”) without calling any external LLMs.

While not using a GenAI model directly, my architecture is designed to be fully compatible with OpenAI or Claude APIs for future upgrades. This made my assistant privacy-friendly, offline-ready, and suitable for sensitive academic environments.",N/A – I did not use the Gemini API in my project.,N/A,University of Maryland - College Park,0
Therassist,https://bitcamp2025.devpost.com/submissions/642658-therassist,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 22:35:55,"🌟 Inspiration

Working on mental health takes incredible strength, courage, and perseverance. Yet, alongside this journey often comes the burden of tedious paperwork. For instance, therapists typically see an average of seven clients per day, and as they jot down notes in Electronic Health Records (EHR), the process can generate unnecessary physical clutter. 
Similarly, clients may be given actionable goals during their sessions, but without a system to track them, these valuable insights are easily forgotten. We saw an opportunity to streamline this process. 

That’s why we created Therassist — a solution that empowers both therapists and clients to be the best version of themselves. By reducing the busy work, Therassist allows therapists to focus on what truly matters: fostering growth and providing personalized care. For clients, it helps track mood, mindfulness, and overall wellbeing, ensuring they stay aligned with their therapy goals.

🛠️ What It Does

Therassist is an intuitive web application designed to streamline and enhance the therapy experience for both clients and therapists. Upon signing in, users select their role as either a client or a therapist to access tailored features.

For Therapists:
Therapists have access to a comprehensive dashboard where they can view client information, session notes, and associated data—all organized and easily accessible. Powered by interactive AI, the app helps therapists quickly generate session summaries and detailed notes based on what was documented on paper, as well as easily follow-up with patients, reducing manual work and ensuring accuracy.

For Clients: 
Clients have their own personalized dashboard, which includes features such as journaling, mood tracking, audio recordings, and reminders for tasks or activities designed to improve their mental well-being. These tools help clients stay engaged in their progress and stay on top of their therapy goals between sessions. 

Therabot: The Virtual Assistant 
Both therapists and clients benefit from Therabot, an AI-driven assistant that enhances the therapy experience. Therapists can ask Therabot for concise summaries of patient data or treatment plan suggestions. Clients can interact with Therabot outside of their sessions to reflect on their emotions, practice grounding exercises, and receive support for mental well-being. After each therapy session, an automatic email is sent to the client with a recap of the session's key points and any homework or tasks to focus on for the week.

Seamless Data Upload and Integration
Therassist makes it easy to upload and manage session data in three convenient ways:

— File Upload: Clients and therapists can easily upload documents or notes.

— Image Capture: Users can take pictures using their webcam, and the app will automatically detect and convert handwritten notes or journal entries into digital text using Google Cloud Vision OCR. This allows for quick, seamless updates to session records.

— Audio Recordings: Users can record their thoughts or sessions, and the app will transcribe audio to text, making it easy to track progress and review key insights from recordings.

🧱 How we built it

Frontend: Next.js, React.js, Flowbite, Tailwind CSS
Backend: MongoDB, Express.js

Challenges we ran into


Connecting to MongoDB: a port 5000 error on Mac stumped us for many hours
A loading issue with Flowbite (React UI) 
Switching from Tesseract to Google Cloud Vision for OCR


Accomplishments that we're proud of


Fully incorporating text recognition with OCR
Incorporating Gemini API with the chatbot and responds in real time
Integrating the database 


What's next for Therabot

Add integration with health data from Apple watches or fitbit (sleep, heart rate, wellness)
",https://github.com/gzhu725/BitCamp2025,https://youtu.be/EeSaEk1Xh7E,"Best Hack Promoting Public Health - Bloomberg, Best Web Hack Using React - Peraton, [MLH] Best Use of MongoDB Atlas","javascript, react, mongodb, node.js, flowbite, next.js, gemini, tailwind, git",Saileeee,Varde,saileea.varde@gmail.com,,https://github.com/gzhu725/BitCamp2025,Best Social Good Hack,Best UI/UX Hack,Best Bitcamp Hack,Prettiest Hack,Hack That Made You Smile,"MongoDB: difficult to set up and figure out how to use certain functions (PUT data), but very helpful
Gemini: very easy to use and powerful","University of Maryland College Park, Carnegie Mellon University, New York University, CG Woodson High School",Gemini,Yes. We used Google Gemini as a chat bot in our app and to aid in developing our program.,1010908330103,"","University of Maryland - College Park, New York University, Carnegie Mellon University",3,Candace,Sun,csun0@terpmail.umd.edu,Gloria,Zhu,gloriazhu66@gmail.com,Ella,Li,ellal1668@gmail.com
Control Flow,https://bitcamp2025.devpost.com/submissions/642667-control-flow,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 22:44:35,"Play NOW

Inspiration

Control Flow was inspired by Balatro, a roguelike card game that resembles poker. We thought it would be fun and challenging to incorporate programming puzzles into this style of game, something that's never been done before!

What it does

Control Flow is a roguelike programming puzzle game where players solve coding challenges by dragging and combining code blocks. Players progress through increasingly difficult levels, earning coins to purchase new code blocks from a randomized shop. Control Flow turns code blocks into limited resource and will have you code tracing like never before! The game has no one solution for many of the problems--it is up to the user to construct a strategy and use their reasoning abilities to advance. 

How we built it

We built Control Flow using p5.js for the graphics and interactions. The game features a custom block-based programming language with a visual editor that allows dragging, dropping, and nesting of code snippets completely from scratch. We implemented a roguelike progression system that generates problems with varying difficulty, alongside a shop system that offers randomly selected code blocks. The game evaluates solutions by parsing and executing the assembled code blocks against test cases to determine if the player reached the target value.

Challenges we ran into

The biggest challenge was building the evaluator. We basically invented our own interpreted  programming language that gets executed as javascript. We dealt with challenges having to do with scope, variables, (nested) loops, etc..

Creating our custom code block editor was also a huge challenge, as it was built completely from scratch with only the p5js core library. We ran into big problems with how to represent nesting and headers, and how to customize the arguments to loops and conditionals. We are beyond proud of how well it turn out.

It was difficult to create engaging problems that were not trivial, possible to solve, and, most importantly, fun. We had to make our system highly scalable to be able to hot-swap different kinds of problems quickly! We built this game with scalability in mind, and we are excited to continue adding more features that add a fun twist to how you think about code.

Accomplishments that we're proud of

We are super proud of taking a vague idea into a polished game that looks great and is super fun to play, all from scratch. We are also proud of making a super tight stack with no unneeded database, restAPI, or bloaty frameworks like react. Additionally, the game is available for anyone to play right now!!
",https://ericx1e.github.io/ControlFlow/,https://youtu.be/BichNZX7i-o,"","p5.js, javascript, html",Kieran,Khan,kierank.pc@gmail.com,,https://github.com/ericx1e/ControlFlow,Best Gamification Hack,Best Razzle Dazzle Hack,Best Bitcamp Hack,Prettiest Hack,MOST LIT HACK,"p5.js is a library that requires a decent amount of learning curve, but the limits are endless!",University of Maryland - College Park,OpenAI and Anthropic,N/A,N/A,"",University of Maryland - College Park,1,Eric,Xie,exie@terpmail.umd.edu
Qubikel,https://bitcamp2025.devpost.com/submissions/642674-qubikel,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 22:50:35,"Inspiration

Urban bike-sharing systems are a sustainable, healthy alternative to cars—but in busy cities like NYC, they’re often frustratingly unreliable. You arrive at a station only to find no bikes… or ride to your destination and can't dock because it’s full.
We asked ourselves: Can we predict and fix this chaos before it happens? That question led us to combine newly learned quantum computing concepts and urban systems modeling to tackle a real-world mobility problem.

What it does

Qubikel is a quantum-powered simulation tool designed to predict bike-sharing station behavior across NYC. It models how bikes move between stations based on population density and inferred commuting patterns.

With Markov Chains enhanced by Quantum Walks, Qubikel can:


Forecast station depletion and overflow
Suggest bike rebalancing strategies with GenAI
Simulate new infrastructure scenarios
Monitor overall bike traffic


How we built it

We developed Qubikel using a stack that combines quantum simulation with modern web technologies:








In the backend, we used Python and Flask for the server alongside implementing quantum random walks using Google's Cirq framework for quantum computing. We have Implemented both classical Markov Chain models and quantum-enhanced versions to compare effectiveness. 

In the frontend, we created interactive dashboard with react & vite for data visualization, and responsive design. 

We also added the power of Google Gemini to explain complex quantum simulation results to users with varying technical backgrounds. 

The simulation engine processes geographic data, weather conditions, and time-of-day factors to create realistic transition matrices that govern bike movement patterns. We implemented quantum random walks that leverage superposition to model the complex, non-deterministic nature of urban bike movement.

Challenges we ran into

This project represented our first dive into quantum computing and working with Cirq, which presented a steep learning curve. Despite having strong software engineering backgrounds, translating classical algorithms into quantum equivalents required a fundamental shift in thinking about computation.  As newcomers to quantum computing, understanding concepts like superposition, quantum gates, and circuit design in Cirq required extensive research and experimentation. Also, adapting classical Markov Chain methodologies to quantum random walks proved much more complex than anticipated.

Accomplishments that we're proud of

Despite these challenges, we achieved several significant milestones:


Successfully implemented quantum algorithms despite being first-time users of quantum libraries
Created a working quantum-enhanced simulation 
Developed an intuitive, responsive interface that clearly visualizes complex system dynamics


What we learned from the Bitcamp UQA workshops

This project taught us valuable lessons across multiple domains:


Fundamentals of quantum computing and practical application through Cirq
Methods for effectively comparing classical vs. quantum approaches to demonstrate quantum 
advantage
Quantum and its future (next Nvdia engineer)
Grover's Algorithm


What's next for Qubikel

Our roadmap:


Connect with actual bike-sharing APIs to test our predictions against real-world usage patterns
Implement more sophisticated quantum algorithms to further improve prediction accuracy
Continue optimizing our quantum simulations for better scalability
Combine our quantum approach with machine learning to identify hidden patterns in usage data
Rewatch Intersteller after having a deeper understanding of quantum! :)



","https://quantumbikesimulation.vercel.app/, https://www.canva.com/design/DAGkePOGOhc/xizSGv4dVKzcuzvCs3380Q/view?utm_content=DAGkePOGOhc&utm_campaign=designshare&utm_medium=link2&utm_source=uniquelinks&utlId=h749284562b",https://youtu.be/VcBaGfBvTGo,"Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best Use of Gemini API","cirq, python, flask, node.js, gemini, quantum, vite",Arijit,Chakma,arijitchakma79@gmail.com,,https://github.com/Yannaner/BitCamp-2025,Best Advanced Quantum Track Hack,Best First-Time Hack,Best Social Good Hack,Hack That Made You Smile,Prettiest Hack,"We transitioned from Qiskit to Cirq after learning from workshop notebooks on quantum computing. Cirq’s simplicity and consistency made it easier to use compared to Qiskit, which offered flexibility but was more complex. This switch has streamlined our workflow and enhanced our quantum understanding more entirely.","Drexel University, The Pennsylvania State University",Gemini,Yes. We used it to present and explain data analytic findings from our Markov Quantum Chain model for bike rental traffic simulation.,908036855850,"susquantums.us
qubikel.nyc","Drexel University, Pennsylvania State University",2,Safa,Obuz,safaeobuz@gmail.com,Ian,Fong,2005fong@gmail.com
BoxBoxBot- AI Generated F1 Commentary,https://bitcamp2025.devpost.com/submissions/642677-boxboxbot-ai-generated-f1-commentary,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 22:53:26,"Inspiration

As passionate Formula 1 fans and AI enthusiasts, we’ve always been captivated by the energy of live race commentary. But what if we could bring that same excitement into dashboards, race data apps, or even personal race reviews- all powered by AI? We wanted to build something that could narrate the thrill of the race not just with numbers, but with natural, context-aware commentary. That’s how BoxBoxBot was born- a voice from the pit wall, powered by data and driven by language.

What it does

BoxBoxBot transforms raw Formula 1 telemetry and timing data into real-time, driver-specific commentary in both text and speech. It detects key moments like pit stops, overtakes, yellow flags, fastest laps, and builds contextual statements- mimicking the style of live commentators. It can:
1) Generate lap-by-lap summaries
2) Provide position change insights
3) Highlight driver battles
4) Speak commentary aloud using text-to-speech (TTS)

How we built it

We used:
1) Python & Pandas to process and align real-time F1 data (position, laps, race control, weather)
2) OpenF1 API to fetch live/historical race data
3) Natural Language Templates with dynamic insertion logic for generating text commentary
4) pyttsx3 for speech synthesis (TTS)
5) Streamlit for a live interface

Challenges we ran into

1) Handling edge cases like lap overlaps, simultaneous pit stops, and null values in telemetry
2) Generating the right input that ensures that the information provided to the LLM is clean and creates accurate commentary

Accomplishments that we're proud of

1) Created a working pipeline that turns raw data into human-style commentary
2) End-to-End pipeline from web-scraping to front-end streamlit application in under 24 hours

What we learned

1) The power of combining data science with storytelling
2) Challenges of real-time commentary from structured data sources

What's next for BoxBoxBot- AI Generated F1 Commentary

1) Add support for multi-lingual commentary and more natural speech synthesis
2) Build a live dashboard or Twitch overlay for fans and streamers
3) Extend to other motorsports like Formula E or MotoGP
",https://github.com/atudorcarsin/f1-commentary,,"","python, openf1, gemini, streamlit, postman, pandas, web-scraping, api",Tudor,Carsin,carsin.tudor@gmail.com,,https://github.com/atudorcarsin/f1-commentary,Best Machine Learning Track Hack,Best Bitcamp Hack,Best Razzle Dazzle Hack,MOST LIT HACK,Hack That Made You Smile,"This weekend, we built BoxBoxBot- AI Generated F1 Commentary using the OpenF1 API, which gave us access to real-time race data like lap times, positions, and flags. The API was reliable and well-structured, though syncing data streams required careful timestamp alignment.
We used GitHub for collaboration, which made version control smooth and efficient. For voice output, gTTS worked great for quick and clean speech generation.
Overall, the tech we used helped us turn raw race data into engaging, real-time commentary- and we had a blast doing it.",Montgomery College/Northwest High School (Dual Enrollment),Gemini,"Yes, we used Gemini to generate F1 commentary by providing real-world race data that was scraped by API calls from openf1.",14264532837,"",University of Maryland - College Park,3,Rishi Koushik,Sridharan,rkoush@umd.edu,Samarth,Singh,samarth1@umd.edu,Aayush,Verma,aayushverma98@gmail.com
EchoLens,https://bitcamp2025.devpost.com/submissions/642684-echolens,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 22:58:34,"What it does


Vocally describes the environment and objects around you using your camera on your phone.


How we built it


Full Stack Web-App
Flask Backend
React Frontend
Ngrok to tunnel the frontend to backend using HTTPS
Local Tunnel to expose the frontend to the world using HTTPS


Challenges we ran into


We found that for phones, videos can't be used unless it's HTTPS
Ngrok only allows one tunnel on the free tier. The reason we needed Ngrok and Local Tunnel is because we needed two tunnels.
Debugging a web app meant for mobile was extra difficult, due to how you can't access console on mobile.


Accomplishments that we're proud of


It works
It remembers the previous pictures, and the differences between pictures. So if things change or move it will point that out.


What we learned


Full stack development in a team
Tunneling
HTTP Methods


What's next for EchoLens


Full deployment
Adjusting the automatic capture intervals per user

",https://github.com/JemLuu/bitcamp,,Best Web Hack Using React - Peraton,"react, flask, ngrok, localtunnel, python, typescript, openai",Jeremy,Luu,jluu196@gmail.com,,https://github.com/JemLuu/bitcamp,Best Social Good Hack,Best UI/UX Hack,"",Prettiest Hack,MOST LIT HACK,I thought that the scannable wristbands were good.,University of Pittsburgh,OpenAI,We used OpenAI 4o to process the images sent from the phone and describe objects that are in frame or have changed from the frame.,N/A,"",University of Pittsburgh - Main Campus,1,Ryder,Pham,havesomepham@gmail.com
Whole Lotta Stonks,https://bitcamp2025.devpost.com/submissions/642689-whole-lotta-stonks,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 23:03:43,"Inspiration


We watch lots of reels everyday trying to be entertained. Why no utilize a similar format but for stocks? Even better why not add everyday aura in the form of Playboi Carti as he narrates to you about said stock? 
## What it does
You can scroll on new stocks, where Playboi Carti will then tell you more about the stock, its historical prices, P/E ratio and much. You can also listen as he tell you more about recent news with the company and much. 
## How we built it
We built a content generation platform utilizing Python, Google Gemini, ElevenLabs and alpha vantage api to generate Carti dubs of stock information while recording other data for the frontend (ios device) to render. 
## Challenges we ran into
Some challenges we ran into were being rate limited by the stock api which made it harder for us to generate content. Also setting up and debugging react native and expo was difficult for us but we managed to figure it out. 
## Accomplishments that we're proud of
We are proud of having code a react native project has tiktok like features to it while also being able to render stock data and news articles. At the same time we are proud of learning to use api's and developing a program/pipeline to automate the generation of new content if needed for the platform. 
## What we learned
We learned React Native and Expo while also learning how better utilize api's and other tools to build out this project
## What's next for Whole Lotta Stonks
We would like to add mock investments and commenting so peers can feel connected as the learn more about stocks through the voice of Playboi Carti. 

",https://github.com/matths05/Bitcamp2025,https://youtube.com/shorts/VCoISF78SiE?feature=share,Best Financial Hack - Capital One,"reactnative, expo.io, python, gemini, elevenlabs, api",Kenny,Wu,wukenny0@gmail.com,,https://github.com/matths05/Bitcamp2025,Best Razzle Dazzle Hack,Best Bitcamp Hack,Best UI/UX Hack,Hack That Made You Smile,"","React Native, Expo, Python, Gemini, ElevenLabs",University of Maryland- College Park,Gemini,N/A,N/A,"",University of Maryland - College Park,2,Matt,Hsieh,matthsieh0411@gmail.com,NYCXAM,,nycxam@outlook.com
InfoGrab,https://bitcamp2025.devpost.com/submissions/642693-infograb,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 23:05:56,"Inspiration
      We wanted to get news articles fast for our research class.

What it does
      It provides a short summary on any topic and provides news articles straight to your doorstep.

How we built it
      We used Javascript in VS Code, incorporating multiple APIs to help us.

Challenges we ran into
      We had trouble determining what APIs to use.

Accomplishments that we're proud of
      We had never made a front-page before, and this is our first time.

What we learned
      We learned how to integrate APIs into our code, with custom prompting.

What's next for InfoGrab
      We plan to focus breaking down news articles, and synthesizing new information as to not only inform our users of what is already out there but to also keep them up to date on newer information that may not be so easily acquired.
","",,"",javascript,Arvin,Chen,arvinchen1211@gmail.com,,https://github.com/Houzi4444/Studying-Help-App,Best App Dev Track Hack,Best Social Good Hack,Best UI/UX Hack,MOST LIT HACK,Hack That Made You Smile,"We had trouble with some of the faulty yellow outlet, which impeded our progress, but everything else was great.",Baltimore Polytechnic Institute,OpenAI and Other,"We used Generative AI in our APIs for both the summary (Wikipedia), the news sources (NewsAPI), and also ChatGPT.",N/A,"",Baltimore Polytechnic Institute,2,Frank,Mei,ultralmasterextreme@gmail.com,Eddy,Mei,yfeddymei@gmail.com
Binary Classifier in C++,https://bitcamp2025.devpost.com/submissions/642694-binary-classifier-in-c,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 23:06:15,"Inspiration

nothing to do

What it does

classifies binary operations

How we built it

with my brain

Challenges we ran into

I suck at c++

Accomplishments that we're proud of

idk

What we learned

I learned c++ and some ml

What's next for Binary Classifier in C++

MAke better
",https://github.com/wings20055/nl,,"",c++,Kenneth,Lee,wings20055@gmail.com,,https://github.com/wings20055/nl,Best First-Time Hack,Best Moonshot Hack,Best Moonshot Hack,Prettiest Hack,Hack That Made You Smile,It was alright but the internet was slow,University of Maryland College Park,I did not use GenAI,No,none,noe,"",0
fin-sights.tech,https://bitcamp2025.devpost.com/submissions/642698-fin-sights-tech,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 23:07:03,"FinSights AI - Financial Portfolio Analyzer

FinSights AI is a modern, full-stack investment analytics web application that empowers users to securely connect their brokerage accounts, visualize
  their portfolios, and receive AI-powered investment insights - all through an elegant Next.js interface.

Tech Stack


Frontend: Next.js 15.2 with App Router, React 18.3
Styling: TailwindCSS, shadcn/ui components, Framer Motion animations
Backend: Next.js API Routes (Node.js)
Database: MongoDB (via Mongoose)
Authentication: Firebase Auth
AI: Google Gemini AI via GenKit
Financial Data:


Plaid API for brokerage connections
Yahoo Finance for market data

Charting: Recharts


Features


Secure Brokerage Connection: Connect your financial accounts via Plaid integration
Interactive Dashboard:


Total portfolio value with performance metrics
Asset allocation visualization
Holdings breakdown with detailed metrics
Market summary with major indices

AI-Powered Insights:


Portfolio analysis with risk assessment
Diversification recommendations
Performance comparison to benchmarks

Chatbot Interface: Natural language interaction with your portfolio data
News & Sentiment Analysis: Real-time market news and sentiment analysis for your holdings
Performance Tracking: Track portfolio performance over various time periods
Responsive Design: Optimized for both desktop and mobile devices

","https://fin-sights.tech/, https://github.com/jgmathew2/fin-sights.tech",,"Best Financial Hack - Capital One, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best AI Application Built with Cloudflare, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","react, nextjs, plaid, gemini, genkit, tailwind, shadcn",Tahmid,Zaman,tzaman1@terpmail.umd.edu,,https://github.com/jgmathew2/fin-sights.tech,Best App Dev Track Hack,Best Bitcamp Hack,Best UI/UX Hack,Hack That Made You Smile,Prettiest Hack,we really learned a lot about mongodb and gemini,"University of Maryland, College Park",Gemini,yes. we used genAI for the interactive chat and for investment analysis + stock overviews.,177660846156,fin-sights.tech,University of Maryland - College Park,2,Steven,Ha,stevenha75@gmail.com,Johan,Mathew,johanm@terpmail.umd.edu
Memento,"",Draft,Pending,Project details,04/12/2025 23:18:37,"Project Story: Memento 🎥

Inspiration 💡

Every day, small, meaningful moments slip away—conversations, laughter, even the quiet moments of reflection. We were inspired by the realization that memories, the very essence of who we are, are too often forgotten. Our personal experiences, as well as the struggles faced by people with memory impairments like dementia or Alzheimer’s, drove us to create an app that automatically captures and preserves life’s everyday beauty. Memento is our answer to ensuring that every gesture, every conversation, and every laugh remains a part of your personal legacy.

What It Does 🚀

Memento is an intelligent memory journal that:


Captures Daily Interactions: Listens to your conversations and automatically transcribes them.
Creates Personal Narratives: Uses AI to summarize your day into engaging, human-like stories.
Multimedia Integration: Prompts you to record photos and videos, weaving them into a seamless timeline.
Interactive Playback: Lets you relive your day with an interactive timeline that highlights the most touching moments.
Assists Memory Care: Acts as a vital tool for those with memory impairments by preserving moments that are often forgotten.


How We Built It 🛠️

We took a mobile-first, user-centric approach to design and development:


Frontend:
We built the user interface using Next.js and React, ensuring a responsive design with Tailwind CSS that looks great on both mobile and desktop devices.
AI & Speech Recognition:
Integrated advanced speech-to-text technology and natural language processing (NLP) models to convert conversations into digital memories.
Multimedia Capture:
Developed the unique BeReel feature to prompt spontaneous photo and video captures, which are seamlessly integrated into the daily timeline.
Cloud & Data Management:
Used a cloud-based backend to store, index, and manage the vast amounts of data, ensuring reliability and security.
Iterative Testing:
We continuously refined our prototype based on user feedback and rigorous testing, ensuring a smooth and intuitive user experience.


Challenges We Ran Into 😥

Our journey was not without obstacles:


Accurate Transcription & Summarization:
Training the AI to understand and accurately transcribe everyday conversation, with all its nuances, proved challenging.
Balancing Automation with Privacy:
Ensuring that data is captured responsibly, with full user consent and robust encryption, required careful planning.
User Interface Design:
Crafting an intuitive UI that serves both tech-savvy users and those with cognitive impairments demanded a thoughtful, iterative design process.
Scalability and Performance:
Managing multimedia data and real-time processing without compromising performance required us to fine-tune our backend systems.


Accomplishments That We're Proud Of 🏆


Innovative Use of AI:
Our app effectively transforms daily conversations into emotionally rich narratives.
Impact on Memory Care:
We’ve designed a tool that not only captures memories but also provides invaluable support to those with memory impairments.
Seamless User Experience:
The Memento interface has been refined through extensive user testing to ensure simplicity, responsiveness, and aesthetic appeal.
Robust Architecture:
Achieving a scalable system that integrates real-time data capture, secure storage, and dynamic UI rendering in a unified experience.


What We Learned 🎓


Empathy is Key:
Designing for a vulnerable user base, such as those with memory impairments, taught us the importance of simplicity and empathy in UX design.
Iterative Development:
Regular user feedback and agile iterations were critical in refining our product.
Bridging Technology and Humanity:
Advanced technologies like AI can be harnessed not just for efficiency but to foster deeper human connections and preserve our most cherished moments.
Resilience and Adaptability:
Overcoming technical hurdles, from transcription accuracy to performance bottlenecks, reinforced our teamwork and determination.


What's Next for Memento

Our vision for Memento is far from complete:


Enhanced Personalization:
We plan to integrate more refined mood and sentiment analysis to tailor the memory narratives even more personally.
Expanded Multimedia Capabilities:
Future updates will include deeper integration of interactive media, such as geotagged memories and voice notes, for a richer storytelling experience.
Community & Caregiver Tools:
We envision features that allow family members and caregivers to interact with, share, and contribute to the memory timelines, fostering a sense of community and shared legacy.
AI-Driven Insights:
Exploring advanced analytics to help users discover patterns in their daily interactions and overall well-being.
Broader Accessibility:
Continued improvements in UI/UX, specifically for users with special needs, ensuring Memento remains inclusive and impactful for all.


Join us as we continue to evolve Memento—a digital companion that helps you celebrate every moment, turning life’s everyday snapshots into a treasured legacy.

Thank you for supporting Memento, where every moment truly matters.
This Markdown project story mirrors the style and depth of a winning Devpost submission, covering inspiration, functionality, development challenges, achievements, lessons learned, and future aspirations—all in a clear, engaging format. Adjust any of the sections as necessary to perfectly suit your project's specifics and your team's experience. Happy pitching!

You said:
can you add emojis too but not too much

ChatGPT said:
Reasoned for 6 seconds
Below is an updated version of the Markdown project story that includes a few tasteful emojis to add personality without overwhelming the content. You can customize the placement or selection of emojis to best match your team's style.

markdown
Copy code

Project Story: Memento 🎥✨

Inspiration 💡

Every day, small, meaningful moments slip away—conversations, laughter, even quiet reflections. We were inspired by the realization that memories, the very essence of who we are, are too often forgotten. Our personal experiences—and the challenges faced by those with memory impairments like dementia or Alzheimer’s—drove us to create an app that automatically captures and preserves life’s everyday beauty.
Memento is our answer to ensuring that every gesture, every conversation, and every laugh remains a part of your personal legacy.



What It Does 🚀

Memento is an intelligent memory journal that:


Captures Daily Interactions: Listens to your conversations and automatically transcribes them.
Creates Personal Narratives: Uses AI to transform your day into engaging, human-like stories.
Multimedia Integration: Prompts you to record photos and videos, blending them into a seamless timeline.
Interactive Playback: Lets you relive your day with an intuitive timeline featuring key conversation highlights and multimedia.
Assists Memory Care: Serves as a vital tool for those with memory impairments by preserving the moments that matter most.




How We Built It 🔨

We took a mobile-first, user-centric approach to design and development:


Frontend:
Built with Next.js and React, using Tailwind CSS for a responsive, modern design that shines on both mobile and desktop devices.
AI & Speech Recognition:
Integrated advanced speech-to-text and natural language processing (NLP) to convert conversations into vivid digital memories.
Multimedia Capture:
Developed the unique BeReel feature to prompt spontaneous photo and video captures, enriching your daily timeline.
Cloud & Data Management:
Utilized a secure cloud backend to store, index, and manage large volumes of data reliably.
Iterative Testing:
Constant refinement through user feedback and rigorous testing ensured a smooth and intuitive experience for all users.




Challenges We Ran Into 🛠️

Our journey was not without obstacles:


Accurate Transcription & Summarization:
Training the AI to understand everyday conversation nuances proved challenging.
Balancing Automation with Privacy:
Ensuring responsible data capture, with full user consent and robust encryption, required careful design.
User Interface Design:
Creating an intuitive UI that caters to both tech-savvy users and those with cognitive impairments demanded multiple iterations.
Scalability & Performance:
Managing multimedia data and real-time processing without compromising performance posed a significant challenge.




Accomplishments That We're Proud Of 🏆


Innovative Use of AI:
Our solution effectively transforms everyday conversations into emotionally rich narratives.
Impact on Memory Care:
Designed a tool that provides crucial support for those facing memory challenges.
Seamless User Experience:
Achieved a refined interface through extensive user testing, resulting in a consistent and delightful experience.
Robust Architecture:
Developed a scalable system that seamlessly integrates real-time data capture, secure storage, and dynamic UI rendering.




What We Learned 🎓


Empathy is Key:
Designing for vulnerable users taught us the importance of simplicity and empathy in UX design.
Iterative Development:
Regular feedback and agile improvements were critical to refining our product.
Bridging Technology and Humanity:
Advanced AI isn’t just about efficiency—it can also deepen human connections by preserving precious memories.
Resilience & Adaptability:
Overcoming technical challenges reinforced our teamwork and problem-solving skills.




What's Next for Memento 🔮

Our vision for Memento is far from complete:


Enhanced Personalization:
We plan to integrate deeper mood and sentiment analysis to make daily narratives even more personal.
Expanded Multimedia Capabilities:
Future updates will include interactive media like geotagged memories and voice notes for richer storytelling.
Community & Caregiver Tools:
Envisioning features that allow family members and caregivers to contribute to and interact with the memory timelines, fostering a supportive community.
AI-Driven Insights:
Exploring advanced analytics to help users uncover patterns in their daily interactions and overall well-being.
Broader Accessibility:
Continuously improving UI/UX to ensure Memento is inclusive and impactful for every user.




Thank you for supporting Memento—where every moment truly matters.
","",,"","react, next.js, supabase, tailwind, gemini, postgresql, python, flask",Thejitha,Rajapakshe,thejitha.rajapakshe@gmail.com,,"","","","","","","","","","","","",Montgomery College,2,Damith,Imbulana Liyanage,damithckumara@gmail.com,Suneth,Ramawickrama Gamachchige,ramasuneth@gmail.com
Room Bloom,https://bitcamp2025.devpost.com/submissions/642708-room-bloom,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 23:18:45,"Inspiration

Individuals with mental health struggles often find it overwhelming to complete seemingly simple everyday tasks such as cleaning your room. When motivation is low, chores like these tend to become less of a priority. We wanted to design an app that turns these tasks into something rewarding, fun, and achievable. We aim to turn everyday cleaning into a gamified experience in order to provide motivation and reinforcement, one small win at a time. 

What it does

Room Bloom provides users with motivation through visual progress and positive reinforcement. The app allows users to upload a picture of their space before and after cleaning it up. The app then uses Google Cloud Vision to asses the difference in cleanliness of the space and awards points to the user as well as a positive affirmation. Points can be used to decorate a virtual room, by adding features plants, furniture, and other visual rewards that grow as they complete more tasks.

How we built it

We used React Native with Expo to create a mobile app with a clean, simple and easy to use interface. The images that the users upload were encoded in Base64 and sent to used Google Cloud Vision's API. We used its object localization feature to detect the number of objects in the before and after pictures that the user uploaded. We used the change in number of detected objects to calculate a cleanliness score and award points. If the number of points was past a certain threshold the user was allowed to add a decoration to their room and was also presented with a positive affirmation. 

Challenges we ran into

Both teammates were completely new to React as well as using API's so there was a major learning curve from enabling Google Cloud Vision's API, to implementing the API key in our code, to actually sending the images to the API and receiving the correct information. Another challenge came with the limitations of the API. It is not completely accurate in detecting the amount of items removed especially in a very cluttered space, or when the difference in images is not very obvious which limited the accuracy of our app. We also struggled with awarding a range of points based on the ratio of items removed rather than simply 0 or 100 depending on if the space was completely cleaned or not which is what our current product does. Another issue was with the use of Expo Go to view the app on our phones - we did a lot of trial and error with different installations before we were able to successfully load it. We struggled with sharing our project on GitHub and had to even make SSH keys to publish correctly. Finally, it was a challenge to figure out how to successfully view our project on both teammates' devices by sharing servers but we figured it out eventually. 

Accomplishments that we're proud of

We are proud of creating a finished product that can have a real world impact on people struggling with mental health issues. We both know from personal experience how much of a difference a little bit of motivation can make when battling mental struggles. We are proud of being able to incorporate new and evolving AI such as Google Vision in to our app as it was the first time for both of us. We are also proud that we were able to effectively work together as a team, splitting up tasks and leaning on each other for support when needed. We are also proud of our ability to debug complex issues such as getting accurate information from the image recognition software. 

What we learned

Both teammates were completely new to using React Native, Expo and any sort of image recognition so we learned a lot about app development as well as implementing API's. From React Native we learned the basics of building mobile friendly UIs. We also learned how to navigate Expo to test across multiple devices. We learned how to implement API's like Google Cloud Vision as well as how to format at send data using Base64 encoding. We learned how API responses work by parsing JSON data to drive our app's functionality. We also learned how to effectively debug and the importance of collaboration and version control. 

What's next for Room Bloom

We want to create a way for users to select which specific chore they are completing so that our app can work more accurately for that specific task. We also want to add a levels component so that users level up as they gain more points as well as a way for users to keep track of cleaning streaks. We want to create a leaderboard system with a way for users to connect with their friends as this would add a more competitive aspect. We would also like to make our ""room"" more realistic with a more of a 3D look and options to drag and drop decorations. We also want to train our own AI model instead of using Google Cloud Vision's API to detect cleanliness in a way that is specific to our app, as this could improve both speed and accuracy especially for niche environments. 
",https://github.com/simrankhotra/room-bloom,https://youtube.com/shorts/Vr7_R6eTTFU?feature=share,"","react-native, expo.io, google-cloud-vision, javascript, react-native-animatable, react-native-image-picker, mobile-app-design-principles",simran,Khotra,skhotra@terpmail.umd.edu,,https://github.com/simrankhotra/room-bloom,Best Gamification Hack,Best First-Time Hack,Best Social Good Hack,Hack That Made You Smile,Prettiest Hack,"This weekend was a huge learning experience for my team mate and me. This was one of our earlier hackathons and at the start we had no idea how to use APIs or even make an app. To build our project we used React, Google Cloud Vision, and GitHub. GitHub was used to submit our project which had issues at the beginning where we had to create SSH keys.  We also used Google Cloud Vision API which was used for object detection in order to determine the cleanliness factor. With this API, we definitely noticed detention accuracy issues with partial clutter. In the future we would want to use a better API, one that detects changes pixel by pixel or even train our own to determine it. Expo helped us create this project into an app and we used a QR code that was provided to open it on our phone. Overall, all the technology that was used was pretty beginner friendly and easy to understand.",University of Maryland - College Park,OpenAI,We used Google Cloud Vision API and we used generative images to test our app.,N/A,"",University of Maryland - College Park,1,Aashna,Nanal,aashnananal@gmail.com
Vibelist,https://bitcamp2025.devpost.com/submissions/642721-vibelist,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 23:25:59,"Inspiration

We wanted to create something that blends music and emotion. We wanted to create something that could understand how you're feeling and translate that into the perfect playlist. Music is a powerful mood enhancer, and we thought, why not let AI help pick the vibe?

What it does

Vibelist generates a personalized Spotify playlist that aligns with your current mood. You simply input how you're feeling, and the app uses AI to interpret your emotion and build a playlist that matches the vibe, using both the OpenAI and Spotify APIs.

How we built it

We built Vibelist using Node.js with Express for the backend. The frontend is styled with Bootstrap, HTML, and CSS for a clean and responsive UI. We used the OpenAI API to analyze mood descriptions and generate keywords or themes, which we then used with the Spotify API to create and curate mood-matching playlists. We use FastAPI to connect frontend to backend as well as to test backend.

Challenges we ran into

The SpotifyAPI is very particular with the Access tokens and timeout errors and this is somewhat of a nuisance when trying to build out a product. Integrating the OpenAI and Spotify APIs seamlessly took some troubleshooting, especially around token management and response parsing. Translating natural language mood descriptions into meaningful playlist criteria was tricky as we had to fine-tune the prompts and logic. Getting the UI to work fluidly with the backend and handle async API calls required careful coordination.

Accomplishments that we're proud of

We successfully connected two powerful APIs to deliver a fun and useful user experience. We created a smooth and intuitive app where users can go from mood to music in just a few clicks. We learned how to structure a full-stack application with real-world API integrations.

What we learned

We learned how to effectively work with third-party APIs, including handling OAuth flows and managing rate limits. We gained experience with prompt engineering, which helped us get the most useful and relevant outputs from the OpenAI API. We also learned how to build a responsive user interface using Bootstrap, as well as how to manage asynchronous interactions between the frontend and backend on the web.

What's next for Vibelist

We plan to add user authentication so that users can save their playlists and revisit them later. We also want to expand the mood detection capabilities by incorporating facial recognition or sentiment analysis from longer text inputs. In the future, we aim to allow users to fine-tune their playlists even further by selecting preferences such as energy level or music genre. Finally, we hope to deploy the app for public use and continue improving the user experience based on real-world feedback.
",https://github.com/tspanguluri/bitcamp-25,https://www.youtube.com/watch?v=Ta-7Gi1JzaY,"","bootstrap, javascript, node.js, express.js, html, css, python, openai, spotify, fastapi",Anvay,Panguluri,apangulu@terpmail.umd.edu,,https://github.com/tspanguluri/bitcamp-25,Best App Dev Track Hack,Best First-Time Hack,Best Razzle Dazzle Hack,MOST LIT HACK,Hack That Made You Smile,"GitHub-convenient to store
FastAPI- easy to use
SpotifyAPI- difficult to use
OpenAI API- conflicting documentation
React- Easy to use
Bootstrap- hard to work with
JavaScript/Node/Express: Intuitive to work with",University of Maryland- College Park,OpenAI,I used OpenAI API for our hack this weekend to generate suggestions.,N/A,"",University of Maryland - College Park,2,Vikaas,Venkatesh,vikaasvenkatesh@gmail.com,Tanmay,Panguluri,tpangulu@terpmail.umd.edu
StakeHouse,https://bitcamp2025.devpost.com/submissions/642724-stakehouse,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 23:27:24,"Inspiration

I was inspired by the lack of user-friendly NFT staking platforms that offer both functionality and a clean, responsive design. I wanted to build something that feels smooth to use and provides real value through passive rewards.

What it does

Stake House lets users mint NFTs, stake them, and earn $SH tokens over time. It supports wallet connections, real-time staking stats, and a sleek, dark-themed UI—all directly in the browser.

How we built it

We built our staking system using a combination of web3 technologies and smart contract development. On the frontend, we used JavaScript, HTML, and CSS to create a responsive and interactive user interface that allows users to connect their wallet, stake NFTs, and claim rewards. The backend was developed in JavaScript and Solidity, where we handled the smart contract logic, ABI integration, and controller functions to interact with the Ethereum blockchain. We deployed our smart contract to an Ethereum Testnet, which also served as our server and database, storing all user and staking data on-chain. Additionally, we used IPFS to manage and retrieve NFT metadata in a decentralized way.

Challenges we ran into

One of the biggest challenges we faced was understanding how smart contracts interact with the frontend. Since blockchain data isn't stored traditionally, we had to learn how to read and write data on-chain using ABI and ethers.js. We also struggled with wallet connection issues, especially when testing on the Ethereum testnet, and ensuring that users’ NFTs were correctly approved and transferred to the contract for staking.

Accomplishments that we're proud of

We built a fully functional, secure, and elegant NFT staking platform. The real-time reward tracking, bulk staking support, and IPFS metadata integration are features we're especially proud of.

What we learned

We deepened our knowledge of smart contract architecture, frontend-driven Web3 development, and how to create responsive UIs that interact smoothly with the Ethereum network.

What's next for StakeHouse

We plan to add staking leaderboard features and possibly mobile wallet support for a more inclusive staking experience.
",https://github.com/sueun-dev/stake-house,https://vimeo.com/1075014162?share=copy,"","javascript, solidity, css, html, ethereum, testnet, ipfs",Somy-Park,Park,h20040908@gmail.com,,https://github.com/sueun-dev/stake-house,Best Cybersecurity Track Hack,Best Bitcamp Hack,Best Moonshot Hack,MOST LIT HACK,Prettiest Hack,I primarily worked with Solidity to write smart contracts for our NFT staking system and used ethers.js on the frontend with JavaScript to connect to the Ethereum testnet. It was exciting to see how blockchain apps function in real-time.,"University of Maryland, College Park",OpenAI,N/A,N/A,"",University of Maryland - College Park,1,Sueun,Cho,scho1236@terpmail.umd.edu
UncompliGrow,https://bitcamp2025.devpost.com/submissions/642736-uncompligrow,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 23:35:11,"Inspiration

Balance is everything. When something becomes too heavy on one side, it topples — this truth applies to life just as it does to trees. A tree with too many branches and leaves, but no strong trunk or deep roots, cannot support its own weight. It's bound to collapse. Likewise, a tree with an abundance of roots but no growth above the ground withers away, its full potential unseen.

UpRoot is built on this metaphor. We are the tree. If we take on too many goals and tasks without solid discipline and mental well-being, burnout is inevitable. Conversely, if we invest deeply in wellness and inner growth, but don’t apply it toward any visible accomplishments, we fail to bloom in the eyes of the world.

What it does

UncompliGrow turns your personal growth into a living tree — a visual, interactive reflection of your habits, goals, and mental wellness.


Roots represent belief, rest, and recovery
The trunk symbolizes discipline — the consistency of daily habits
Branches are your goals
Leaves are the tasks you complete


Each element of the tree is connected, helping you see not just what you're accomplishing, but how balanced your growth really is. The metaphor encourages self-awareness:


Are you growing outward without building a foundation?
Are you rooted but hiding from the world?


UncompliGrow helps you answer those questions and take action.

How the tree grows:


Leaves appear when you complete tasks tied to a goal
Branches grow as you create new goals or subgoals
Roots expand when you complete foundational wellness challenges
The trunk thickens as you complete daily habits


You can add or remove habits, goals, subgoals, tasks, and challenges — giving you full control over how your tree (and you) grow.

How we built it

Technology Stack


Frontend Framework: React (with TypeScript)
Build Tool: Vite
Styling: Tailwind CSS
Component Library: Shadcn UI
State Management: React Query
Routing: React Router
Icons: Lucide React


Architecture

Frontend Structure

The app uses a component-based architecture:

Pages


Index.tsx: Main dashboard with the growth tree
Habits.tsx: Habit management page
Goals.tsx: Goal tracking and management
Challenges.tsx: Daily challenges and roots


Components


Tree.tsx: Core visualization of user’s growth
HabitItem.tsx: Represents individual habits
GoalItem.tsx: Handles goal and task management
ChallengeItem.tsx: Tracks and displays challenge progress


Utilities


api.ts
Custom hooks for:


Toast notifications
API interactions



Challenges we ran into

Coming into Bitcamp 2025, I had very little experience with frontend development. Most of my background was in backend and logic-heavy systems, so diving into React, Tailwind CSS, and component-based design was a steep learning curve.

I had to quickly learn how to manage state with React Query, structure pages with React Router, and build responsive, modular interfaces using Tailwind — all within a high-pressure environment.  

Accomplishments that we're proud of


Built a fully functional and interactive growth-tracking tree in just 36 hours
Learned and implemented a modern frontend stack (React, TypeScript, Tailwind CSS) with no prior experience
Created a visually tree structure that grows based on user input. (Trust me! This was really an accomplishment...) 


What we learned


Gained hands-on experience with React and TypeScript, including managing state and routing
Learned how to style components using Tailwind CSS
Understood the importance of component reusability and clean architecture in frontend development


What's next for UncompliGrow


User Authentication: Enable users to securely log in and save their growth data
Backend Integration: Replace the mock API with a real database and backend service
Personalized Challenges: Generate daily challenges based on past activity and habits 
More Tree Variations: Let users customize their growth tree based on their goals or themes (like positioning branches, color, etc.)

",https://github.com/Rohan-Payyavula/UncompliGrow,,"Best Hack Promoting Public Health - Bloomberg, Best Web Hack Using React - Peraton","react, typescript, vite, tailwind-css, shadcn-ui, javascript",Rohan,Payyavula,payyavularohan2004@gmail.com,,https://github.com/Rohan-Payyavula/UncompliGrow,Best App Dev Track Hack,Best Social Good Hack,Best Gamification Hack,MOST LIT HACK,Hack That Made You Smile,"This weekend was a deep dive into frontend technologies for me. Coming into Bitcamp, I had limited experience with modern frontend frameworks — but over the course of the hackathon, I learned to build an entire interactive app from scratch using React, TypeScript, and Tailwind CSS.

I used Vite as the build tool, which was incredibly fast and smooth for development. Shadcn UI helped me create accessible and clean components quickly, while React Query made managing local API-like data feel intuitive. I didn’t get a chance to use sponsored APIs like Twilio or MongoDB Atlas in this project. Overall, I had a great experience with these technologies and feel much more confident working with frontend stacks now. Bitcamp pushed me out of my comfort zone — and I’m glad it did.",George Mason University,I did not use GenAI,N/A,N/A,I have not done so.,George Mason University,0
MedSave,https://bitcamp2025.devpost.com/submissions/642742-medsave,Submitted (Gallery/Visible),Pending,Submit,04/12/2025 23:39:37,"Inspiration

Every year, billions of dollars in unused medications are discarded by:


Pharmacies
Hospitals
Nursing homes


Often, this is simply due to expiration dates.
One study found that hospitals waste over $800 million in usable drugs annually.
Meanwhile, 1 in 3 Americans skip medications due to cost.



What it does

MedSave bridges the gap between surplus medications and communities in need.

It connects:


Pharmacies with excess or soon-to-expire inventory that can offer steep discounts 
Nonprofits and clinics serving underserved populations:


Community health centers
Shelters
Aid organizations



By reducing waste and improving access to essential medicine, MedSave makes a tangible impact — both environmentally and socially.



How we built it


Frontend: React, HTML, CSS, JavaScript, Tailwind CSS
Authentication: Google OAuth
Address Lookup: Mapbox API
Backend: Flask with multiple RESTful endpoints
Database: SQLite (for user, drug, and pharmacy data)
External API: NDC Directory API (for drug metadata)




How it works

Upon visiting MedSave:


Users log in via Google authentication
They select one of two roles:


Pharmacy
Nonprofit



Pharmacy users can:


List surplus/soon-to-expire medications by entering:


NDC code
Pharmacy name
Location
Price
Expiration date

The backend uses the NDC Directory API to autofill:


Generic drug name
Manufacturer info

All data is stored in a SQLite database and displayed in the marketplace


Nonprofit users can:


Search for medications by entering:


Drug name
Their address

The backend returns:


Listings from the nearest pharmacies selling the drug
Prioritized by proximity





Challenges we ran into

As first-time hackathon participants, we faced challenges such as:


⚙️ Git and version control mishaps
🎨 Frontend styling and layout tweaks
🔐 Troubleshooting authentication flow
🔄 Keeping backend and frontend in sync




Accomplishments that we're proud of


✅ Completed and deployed a full-stack web application
🔐 Successfully integrated Google OAuth and Mapbox
🤝 Collaborated and adapted quickly under time pressure
🧠 Gained practical experience with APIs, databases, and user roles




What we learned


📁 Effective use of Git/GitHub for collaboration
🐞 Debugging full-stack issues
🔒 Best practices for authentication and session management
🔁 Frontend-to-backend integration techniques




What's next for MedSave

We plan to expand MedSave’s functionality with:


🔍 More advanced filtering (by radius, drug brand, etc.)
📬 Alerts for nonprofits when needed medications become available
📊 Dashboards with analytics and request tracking
🧱 Upgrade to a scalable backend (e.g., PostgreSQL)
✅ Verification & incentive systems to promote responsible listings

",https://github.com/arsh-goenka/MedSave.git,,"Best Hack Promoting Public Health - Bloomberg, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton","react, python, flask, sqlite, html, css, javascript, tailwind, mapbox, terpai, googleauth, openfda",Arsh,Goenka,agoenka@terpmail.umd.edu,,https://github.com/arsh-goenka/MedSave.git,Best First-Time Hack,Best Social Good Hack,Best Sustainability Hack,"","","Flask - Made it pretty nice and seamless to integrate with the frontend
React - It was nice especially with the API stuff but tedious
GoogleAuth - Made authentication much more easy and saved lot of time","University Of Maryland, College Park",Other,N/A,N/A,"",University of Maryland - College Park,3,Ayush,Mishra,ayushmishraca@gmail.com,Israel,Ogwu,izzymax192@gmail.com,Pramukh,Bhushan,pramukh.bhushan@gmail.com
Untitled,"",Draft,Pending,Manage team,04/13/2025 00:08:16,"","",,"","",Pranav,Anand,panand1@terpmail.umd.edu,,"","","","","","","","","","","","",University of Maryland - College Park,0
Knowledge Quest,"",Draft,Pending,Additional info,04/13/2025 00:10:53,"Inspiration

We wanted to build something that makes studying fun and interactive, so we took inspiration from Bitcamp's dinosaurs and created an adorable and engaging way to study readings!

What it does

Knowledge Quest takes in the user's reading and uses Gemini to generate five questions to test the user's knowledge. To spice things up, the user can choose one of four elemental dinos and answer questions to defeat enemy dinos. The app checks for accuracy and provides the relevant portions of the text for review.

How we built it

We used Figma to visualize our app and Pygame. We used PixilArt to design all visuals and Cats on Mars by SEATBELTS for the background music.

One of our teammembers also made a React version of the application.

Challenges we ran into

We ran into challenges throughout the hackathon including limitations of the pygame package for the implementation of features we had in mind.

Accomplishments that we're proud of

We are extremely proud of our visual elements! They are adorable!! Our teammembers did all of the pixel art themselves and it really added to the look of our project.

We are also proud of completing our user flow the way we envisioned it.

What we learned

We learned a lot about pygame, the use of Gemini in code, and how pygame compares with React.

What's next for Knowledge Quest

We would like to incorporate the opportunity for user's to retry question that they previously missed so that they get a chance for redemption. 
","https://github.com/ilydoa/knowledge-quest, https://www.figma.com/design/lORPhrT5Nar4augalABIzT/Hackathons?node-id=94-1414&t=uFkCdjXkkJiieaqW-1, https://github.com/mqzyiv/Bitcamp-Hack",,[MLH] Best Use of Gemini API,"pygame, python",Hellen,Ou,hou@terpmail.umd.edu,,https://github.com/ilydoa/knowledge-quest,Best Gamification Hack,Best First-Time Hack,Best UI/UX Hack,Prettiest Hack,Hack That Made You Smile,We used GitHub and it was nice to be able to have all our code in one place and be able to compare changes across different people's code.,University of Maryland College Park,OpenAI and Gemini,"Yes, we used the Gemini API because we wanted to make the game flexible to what different people are studying, so we used generative AI to create questions based on whatever text you submit.",35829611565,"",University of Maryland - College Park,3,Cadence,Cheng,ccheng27@terpmail.umd.edu,Ilayda,Dogan,idogan@terpmail.umd.edu,Karen,Li,mqzyiv@gmail.com
BattleTask,https://bitcamp2025.devpost.com/submissions/642779-battletask,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 00:11:48,"Inspiration

Getting on your laptop with a task at hand then getting distracted and wasting hours on end.

What it does

This chrome extension tracks the active windows and tabs in order to see if the content on them is productive or not. Using this information it creates a score for each tab, where the average of all the tabs is used in setting up the game. When the average of all the tabs is less than 50 the user loses a life (heart). Once the user loses a three hearts in their chosen time period, the game ends and the user dies. If however, the user remains concentrated and productive for their chosen time period, the user wins the game and gets to track the cumulative time they were focused for. 

How we built it

We built this app primarily in JavaScript and HTML.

Challenges we ran into

Integrating Gemini to analyze the tabs the user was on to determine if they were educational or not. 

Accomplishments that we're proud of

Using AI to distinguish between productive Youtube, and entertainment Youtube.  

What we learned

We learned how to incorporate an AI API and Chrome Extensions API.

What's next for BattleTask

We want to set it up with a serverless setup through Google's services.
",https://github.com/J0shua2/BattleTask-main,https://youtu.be/LlapCDMqTKE,[MLH] Best Use of Gemini API,"javascript, html, css, chrome-extensions-api, gemini-api, chrome-local-storage, rest-api",Frank Hayden,Leo Rex Arockia Raj,frankhl1@umbc.edu,,https://github.com/J0shua2/BattleTask-BC2025,Best Gamification Hack,Best First-Time Hack,Best Razzle Dazzle Hack,Hack That Made You Smile,Prettiest Hack,"We interacted with the Gemini API, which was really cool and we decided to use it. Used Github for version control.","University of Maryland, College Park; University of Maryland, Baltimore County",Gemini and Other,"Yes, we used the Gemini API to process data from chrome API through a sqlLite database. The Gemini API returned a value which we incorporated into our productivity game.",279027565964,"","Univeristy of Maryland Baltimore County, University of Maryland - College Park",1,Joshua,Lin,j0shua@terpmail.umd.edu
Terpsicle,https://bitcamp2025.devpost.com/submissions/642786-terpsicle,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 00:25:46,"Inspiration

Each of us noticed how difficult planning got when registration time came around each semester. Having to have a custom-made 4-year plan, the schedule of classes, and your degree audit open on separate tabs gets confusing, hard to manage, and takes a lot of time.

What it does

Terpsicle combines the functionality of a 4-year planner, a CS major degree audit, and a semester-schedule planner into one app. Changes made to the 4-year plan are reflected in the audit and your data when the page is reloaded without the need for a server. To get started quickly, you can copy and paste your unofficial transcript to fill in the classes you’ve already taken!

How we built it

We used Next.js and React to create a serverless site that can be updated with your course information. Web scraping and parsing are used to get information from your transcript and the schedule of classes, to make sure the information displayed is accurate. Courses are fetched dynamically when adding to the schedule builder, and the sections the user chooses are stored locally and displayed with a calendar layout. All changes made are saved in your browser’s local storage so that they persist between sessions.

Challenges we ran into


There were many inconsistencies in cross-listings, gened. credits, and graduation requirement categories.
The Schedule of Classes had inconsistent formatting that led to parsing difficulties
Displaying courses in the schedule builder as a calendar required special care with how to calculate offsets for elements based on their times of day — dealing with the CSS here was not particularly early. 


Accomplishments that we're proud of


1000+ lines of graduation requirement logic
Transcript can be pasted into the website and automatically loads every class taken
Good new user experience
Data successfully reading/writing between pages written by different team members
Intuitive UI/UX for adding and displaying courses to the schedule builder


What we learned


Hot to use TypeScript and web technologies to develop a web app
How complicated CS/GenEd graduation requirements are 
How to store and read lots of information via local storage
How to create dynamically updating react components
The value of linting code during development


What's next for Terpsicle

To start factoring in other concentrations of majors/minors, more than just CS, and making it usable for all students. The goal is to ease the stress of many who are consistently alt-tabbing to figure out their future academic plan, and we want all areas of studies to benefit from this project. We also want to add additional features such as upcoming registration info, possible time conflicts, or even extra pathways that might fit into one’s 4-year plan.
","https://terpsicle.com, https://github.com/zsrobinson/terpsicle",,Best Web Hack Using React - Peraton,"nextjs, react, vercel",Zachary,Robinson,me@zsrobinson.com,,https://github.com/zsrobinson/terpsicle,Best App Dev Track Hack,Best First-Time Hack,Best Bitcamp Hack,Prettiest Hack,"","We used GitHub for collaboration and had no hiccups, despite the occasional strange merge conflict.","University of Maryland, College Park",I did not use GenAI,N/A,N/A,"","University of Maryland - College Park, poolesville high school",3,Shubham,Bhatnagar,shubhambhatnagar2017@gmail.com,Simon,Ambrozak,simon.ambrozak@gmail.com,Carter,Pry,clpry@pm.me
Riffs,https://bitcamp2025.devpost.com/submissions/642787-riffs,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 00:26:06,"🎸 Riffs — Hum. Play. Create.



🧠 Inspiration

Learning guitar often feels intimidating. Traditional tools rely on robotic MIDI playback, confusing tabs, or assume you have a studio setup. We wanted to create something that feels natural, creative, and instant.
Riffs was born from a simple idea:  


Hum a melody, and immediately turn it into something you can build on.




💡 What it does

Riffs is a browser-based tool that lets users:


Hum a melody into their microphone
Detect the pitches and convert them into notes
Automatically map those notes to guitar string+fret positions
Display the result on a piano-roll-style editor
Allow full editing of notes with click, drag, and delete interactions


It’s a seamless pipeline from voice to visual melody creation — all in-browser, no plugins required.



🛠️ How we built it


pitchy – for pitch detection
TypeScript – to ensure strong type safety
React – for dynamic UI
TailwindCSS – for rapid and clean styling
FL Studio – to generate authentic piano and guitar samples




🚧 Challenges we ran into


Pitch detection in a noisy hackathon environment
Accurately converting notes realistic guitar positions
Designing a piano roll editor that feels intuitive and musical
Managing app state across pages without a backend




🏆 Accomplishments we're proud of


Built a custom visual note editor from scratch in under 36 hours
Seamlessly converted voice input into playable, editable data
Used real guitar samples for future playback instead of default MIDI
Delivered a clean UX with clear and musical visual feedback




📚 What we learned


Real-time browser audio processing is very possible
Less is more when it comes to UI/UX in creative tools
A good editing experience doesn’t need audio playback to feel musical
Pitch-to-note conversion is a powerful creative bridge




🚀 What's next for Riffs


Integrating our guitar samples for live playback
Adding tempo, measure lines, and rhythmic structure
Supporting multiple tracks or layers
Saving riffs to the cloud and letting users remix each other's melodies
Exporting riffs to MIDI, tablature, or Guitar Pro formats
Exploring AI tools to suggest chords or harmonize based on your input




🌟 TL;DR

Riffs is a musical sandbox for guitarists and creators.
Hum your melody. See it. Shape it. Share it.
","https://riffs-three.vercel.app/, https://github.com/cataladev/riffs",,Best Web Hack Using React - Peraton,"typescript, next.js, pitchy, vercel, tailwind",Carlos,Catala,catalacarlos23@gmail.com,,https://github.com/cataladev/riffs,Best App Dev Track Hack,Best Gamification Hack,Best Razzle Dazzle Hack,Hack That Made You Smile,Prettiest Hack,"We used pitchy for real-time pitch detection, which worked surprisingly well even in a noisy hackathon setting. We also used FL Studio to generate clean guitar samples for playback instead of relying on robotic MIDI.",University of Central Florida,"OpenAI, Anthropic, and Gemini",N/A,N/A,N/A,University of Central Florida,3,Carlos,Lopez,lopez212318@gmail.com,Daniel,Efres,da947619@ucf.edu,Rafael,Niebles,rnieblesealo@gmail.com
Pantry Pal,https://bitcamp2025.devpost.com/submissions/642795-pantry-pal,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 00:35:56,"Inspiration

As busy students and food lovers, we realized how often we let ingredients go to waste simply because we didn’t know what to make with them. We wanted to create a tool that helps people make the most of what they already have—whether they’re college students, working professionals, or home cooks trying to reduce food waste.

What it does

The user inputs the ingredients they have in their pantry and fridge and the website sends them options of recipes they can make.

How we built it

There are 3 steps to our website:


Create an account
Search for recipes with ingredients in your pantry
Like your favorite recipes.
Our tech stack is React JSX, Firebase Authentication with Firestore, API Integration (from Groq Cloud and Pexals API), and general web development tools such as CSS. Users begin by creating a Pantry Pal account, which uploads their log in information (name, email, and password) into our Firebase Authenticator. The user may then log in with their new credentials and move forward to our main page of Recipe Searching. Our customers can then use the search bar to input the ingredients they currently have in their pantry (hence Pantry Pal). When the ""Search"" button is clicked, our Groq Cloud API fetches data and displays Based on what our customers have in their pantry, they may search o Our application functions by have 
Frontend: Built with React, we designed an intuitive and responsive user interface that makes searching and saving recipes easy.
Backend/Database: We used Firebase Authentication for secure user sign-up and login, and Firestore to manage user data and saved recipes.
AI Integration: We used Groq Cloud's API to generate recipe content based on user-inputted ingredients.
Media: Recipe images are sourced from the Pexels API, bringing our suggestions to life.


Challenges we ran into

We found it hard to find APIs to find recipes at first, but we ended up using groqCloud and Prexel.

Accomplishments that we're proud of

We're proud of building a fully functional web app in just 24 hours that takes user-inputted ingredients and returns recipe suggestions using an external API. We created a clean, easy-to-use interface and learned how to effectively manage state and data in React. Even though we ran into API limitations, we adapted quickly and found creative solutions to keep the project on track. Most importantly, we collaborated smoothly, combining our skills in design, frontend development, and logic.

What we learned

Through this project, we learned how to integrate APIs into a React app, fetch and display external data dynamically, and troubleshoot common issues like API errors and JSON parsing. We also gained experience designing an intuitive UI and working with React features like hooks and conditional rendering. This hackathon also taught us how to manage time efficiently, divide tasks as a team, and iterate quickly based on feedback and challenges.

What's next for Pantry Pal

In the future, we plan to add a ""Favorite Recipes"" page where users can log in and see all the recipes they've saved by clicking the heart icon on any recipe card. This will allow users to easily return to meals they love and plan their cooking more efficiently. We're also working on integrating an image scanner that will identify ingredients from the picture users upload, and automatically feed them into the app, generating recipes based on what users actually have on hand. These features will make Pantry Pal more intuitive, user-friendly, and tailored to real-life cooking needs.
","",,"","groqcloud, prexel, react, firebase",Keerthana,Narabetla,knarabet@terpmail.umd.edu,,https://github.com/MeenakshiRamaS/Pantry-Pal/tree/main,Best First-Time Hack,Best Bitcamp Hack,Best Razzle Dazzle Hack,MOST LIT HACK,Hack That Made You Smile,I really enjoyed working with firebase. I found it very easy and striaght forward to work with and it really helped out project.,University of Maryland - College Park,OpenAI,We use GroqCloud API in our hack,N/A,"","University of Maryland - University College, University of Maryland - College Park",3,MeenakshiRS1,Rama Subramanian,mramasub@terpmail.umd.edu,Anika,Keshri,keshri.0915@gmail.com,riyam07,,rmakwana2219@gmail.com
GeoGuard,https://bitcamp2025.devpost.com/submissions/642797-geoguard,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 00:36:55,"Inspiration

Social media has become more and more common in our everyday lives, to the point where it's basically a normal part of how people communicate and stay connected. It’s not just adults who use these platforms anymore. Many kids and younger teens now have easy access to social media apps and websites. Even though most platforms have age restrictions, it’s pretty easy for minors to get around those rules just by entering a fake birthdate or using someone else’s information. Because of this, there’s a growing concern about how safe the internet really is for younger users. With more children spending time online, the risks of being exposed to inappropriate content, interacting with strangers, or accidentally sharing private information are much higher. This makes online safety, digital responsibility, and awareness more important than ever.

What it does

Users can upload images directly to the website, where our tool automatically scans for sensitive location information, like street signs, building names, or anything that could give away where they are. The website then highlights these areas, and users get to choose whether or not they want to blur each highlighted region. After making their selections, they can download the edited version of the image, which helps protect their privacy before sharing it online. We also added a feature that lets users use their live camera. This is especially useful before things like video calls or livestreams. The tool scans the background in real time and checks for any location-sensitive details that might be visible. This gives users a chance to fix or hide anything before they go live, helping them stay safe and aware of what’s around them. We wanted to make sure the process was simple and easy to use, especially for younger users, so they can better understand how to protect their personal information online.

How we built it

To build our project, we started with the frontend using React, which is a JavaScript library that helped us create an interactive user interface. For styling, we chose Tailwind CSS because it made designing the pages quicker and easier with its simple utility classes. To make the website feel more dynamic, we added animations using Framer Motion, which helped make elements on the page move and change smoothly. On the backend, we built a Flask API to connect the frontend with different tools for detecting sensitive content. We used Google Vision, a machine learning tool, to analyze images and look for any location-revealing information like street signs, buildings, or addresses. This allowed us to identify and remove potentially harmful content in the images being uploaded or streamed. We also used Gemini, a large language model, to analyze the live camera feed. It looked at any text in the background, like signs or labels, to check if there was a risk of revealing someone’s location. This helped us assess whether the content needed to be blurred or removed for safety. Altogether, combining these tools helped us build a system that can better protect minors from unintentionally revealing their location online.

Challenges we ran into

Some of the challenges we faced during the project were related to detecting sensitive locations using Google Vision and integrating it with React. One of the biggest difficulties was figuring out how to coordinate the blur box coordinates with the location of the buildings in the images. Since the buildings could be in different positions, sizes, or orientations, it was tricky to match the coordinates of the blur boxes with the correct spots on the images. Another challenge we ran into was the process of merging our work together using GitHub. Since we split up the tasks, there were times when combining everyone’s individual work created conflicts, which made it harder to keep everything in sync. This slowed us down at times, especially when we had to fix merge issues or rework certain parts of the code. Lastly, detecting text and objects in a live camera feed was also difficult. Unlike working with static images, we had to deal with a continuous stream of frames from the video, which made the processing much more complex. We had to find a way to detect the sensitive content in each frame, which required more advanced handling to ensure we didn’t miss anything important while keeping the process efficient.

Accomplishments that we're proud of

We’re really proud of a few things we accomplished during this project. First, we’re excited about successfully using Google Vision for the first time. We’d never worked with this technology before, and it was a huge learning experience. We were able to get it working for both text detection and object detection, which allowed us to identify sensitive location information in images and blur it out. Another accomplishment we’re proud of is the UI and design we put together. We wanted the app to be easy to use, so we worked hard on making sure the design was simple and clean. We used React and Tailwind CSS to build the layout, and the result was a responsive design that looks great on any device. We also added some cool animations with Framer Motion, which made the app feel more interactive and fun to use.

Overall, we’re really proud of how everything came together, from using Google Vision to making the app look and feel smooth and easy to use.

What we learned

Throughout the project, we learned a lot about working with different technologies and improving our skills. One of the main things we learned was how to use the Google Vision API to analyze images and visual data. We got hands-on experience with extracting text and detecting objects, which was a key part of our project. By using Google Vision, we were able to locate sensitive information, such as location-revealing text, and we created coordinate boxes around these objects to define regions of interest that would be blurred for safety. We also deepened our experience with React during this project. As we worked with React to build the frontend, we learned more about handling state, managing components, and creating smooth user interactions. This experience helped us feel more confident in using React for future projects, and it was valuable in expanding our technical skills. Additionally, we explored using OpenCV, a computer vision library, to apply a Gaussian blur to the images. This allowed us to blur out sensitive information automatically in the regions we identified using Google Vision. By experimenting with OpenCV, we learned how to manipulate images and apply filters, which helped us improve the functionality of our project. Overall, this project gave us a chance to learn new technologies, overcome challenges, and improve our skills in both front-end and back-end development.

What's next for GeoGuard

One of our main goals is to incorporate our app as an extension for children’s cameras or social media apps. This way, the app can automatically screen photos and videos for sensitive information before they are uploaded, ensuring that nothing revealing or unsafe is shared. In addition to that, we plan to add a feature that sends notifications to parents via email. These notifications would update them on the status of their child’s social media posts, such as whether a post was approved, blurred, or flagged for sensitive content. This would give parents peace of mind while still allowing kids to use social media safely. Another key feature we want to work on is user login authentication. By adding a secure login system, we can ensure that users have personalized experiences and settings, and also help parents monitor and control the app’s functionality more easily. Finally, we aim to improve the app’s performance and scalability, so it can handle larger amounts of data and work efficiently for a wider range of users. We’re excited about expanding GeoGuard to make it even more helpful for keeping kids safe online and empowering parents with the tools they need to protect their children.
",https://github.com/msavya/Bitcamp2025,,"Best Hack Promoting Public Health - Bloomberg, Best Digital Forensics Related Hack - Cipher Tech Solutions, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of Gemini API","react, tailwindcss, googlevision, framermotion, python, fastapi, geminiai",Savya,Miriyala,msavya@gmail.com,,https://github.com/msavya/Bitcamp2025,Best Machine Learning Track Hack,Best Social Good Hack,Best First-Time Hack,Prettiest Hack,Hack That Made You Smile,"This weekend, the main technologies we have worked with were Google Vision, Gemini, OpenCV, FastAPI, and React. We have had prior experience with React, FastAPI, and Gemini, so these technologies were relatively easy to implement. However, OpenCV (even though we have worked with it before) was getting really difficult because we were trying to perform image detection on individual frames of a video. It was hard to use Google Vision to detect the location information.",University of Maryland,Gemini and Other,"We implemented Gemini AI, Google Vision, and FastAPI. Gemini AI was used to determine whether the text that Google Vision read from the camera. Google Vision was also used to detect sensitive location information/areas on the images. FastAPI was used to connect the frontend to the backend by creating multiple endpoints that returned information regarding image detection.",418732326893,"","University of Maryland - College Park, University of Maryland - University College",3,Suhani,Agrawal,suhani.agr@outlook.com,Esha,Vigneswaran,eshavigneswaran@gmail.com,bhavyat-01,,bhavya.tanugula@gmail.com
MacroTerpitech,https://bitcamp2025.devpost.com/submissions/642802-macroterpitech,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 00:42:29,"MacroTerpitech

Inspiration

Our team noticed a common struggle among UMD students: balancing nutrition goals with the reality of campus dining options. As students ourselves, we experienced the challenge of trying to meet specific macro targets while eating at dining halls. We were inspired to create a solution that would make it easier for students to make informed food choices that align with their health and fitness goals.

What it does

MacroTerpitech is a web application that transforms how UMD students interact with campus dining. Users can:


Set personalized macro goals such as calories, protein, carbs, and fats 
Input dietary preferences and restrictions
Receive customized meal recommendations from UMD dining halls that match their nutrition targets
Track their nutritional progress throughout the day
Plan meals ahead of time based on real-time dining hall offerings


How we built it

We developed MacroTerpitech using a modern tech stack:


React for the frontend with Tailwind CSS for responsive design
Firebase for storing user profiles and nutritional data
BeautifulSoup and Selenium to collect and update UMD dining hall menu information
Gemini for querying in order to create the custom meals at the dining halls


We implemented a user-centered design process, conducting interviews with fellow students to understand their needs and pain points before developing our solution.

Challenges we ran into

Building MacroTerpitech wasn't without obstacles:


Web scraping the dining hall websites was difficult due to the amount of inconsistent formatting
Being able to get the information in a consistent format from Gemini in order to display properly was harder than expected 
Developing a clean UI for users required a good amount of trial and error until we settled on something that all users would enjoy 


Accomplishments that we're proud of

Despite the challenges, we're proud of creating a solution that:


Successfully integrates with UMD's dining system to provide actionable recommendations
Delivers a clean, intuitive user experience that makes nutrition tracking accessible
Accounts for diverse dietary needs and preferences across the student body
Received enthusiastic feedback during our testing phase with fellow students


What we learned

This project taught us valuable lessons about:


The complexity of nutritional data and the importance of accuracy
Balancing technical capabilities with user experience
Collaborative development practices and effective task distribution
The importance of continuous user feedback throughout the development process
How technology can address everyday challenges in campus life


What's next for MacroTerpitech

We have ambitious plans for the future:


Expanding our database to include more detailed nutritional information perhaps expanding to UMBC and other campuses
Developing a mobile app to complement our web platform
Creating a community feature where users can share successful meal combinations
Partnering with UMD Dining Services to integrate our solution more deeply with their systems


MacroTerpitech aims to continue evolving to better serve the UMD community and potentially expand to other universities facing similar dining challenges.
",https://github.com/Paradox560/bitcamp-2025.git,,"Best Hack Promoting Public Health - Bloomberg, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best Use of Gemini API","javascript, python, react, tailwind, firebase, gemini, beautiful-soup, selenium, flask, typescript, shadcn",Shivank,Bhimavarapu,emailshivank@gmail.com,,https://github.com/Paradox560/bitcamp-2025,Best Social Good Hack,Best UI/UX Hack,Best Sustainability Hack,Prettiest Hack,Hack That Made You Smile,"Working with React as our frontend framework proved to be an excellent choice for our MacroTerpitech application. The component-based architecture allowed our team to work on different UI elements simultaneously without conflicts. Tailwind CSS significantly accelerated our development process by enabling us to style components directly within JSX. The utility-first approach eliminated the need for switching between files and maintained consistency throughout our application. The responsive design utilities also made it simple to ensure our application worked well on various screen sizes.

One of the most challenging aspects of our project was obtaining accurate and current data from UMD dining halls. We utilized web scraping tools such as Selenium and BeautifulSoup to extract this information. The tools were relatively straightforward to implement but required careful handling to ensure we respected the university's website usage policies. This approach gave us access to the data we needed, though in a production environment, we would prefer to work with an official API.","University of Maryland, College Park",Gemini,"We leveraged the GPT API to analyze user preferences, dietary restrictions, and nutritional goals alongside the available dining hall options. The AI generates personalized meal combinations that optimize for macro targets while respecting individual preferences. This creates a much more tailored experience than traditional filtering systems alone could provide.",52382534220,macroterpi.tech,"TJHSST, University of Maryland - College Park",3,Jay,Kammula,jkammula21@gmail.com,Pranav,Palle,pranav4096@gmail.com,Amogh,Gurram,mintu.gurram@gmail.com
TerpFit,https://bitcamp2025.devpost.com/submissions/642817-terpfit,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 00:57:39,"TerpFit

Inspiration

The inspiration behind TerpFit came from wanting to make working out smarter and more personalized for everyone. A lot of people struggle with planning workouts, finding nearby gyms, or staying consistent with their fitness goals. We wanted to create a platform where users don’t have to think too hard about building a workout plan, instead they can simply chat with an AI and get a customized plan built for them. Combining location tracking, AI conversations, and workout logging felt like the perfect way to make fitness easier and more interactive.

What it does

TerpFit is a user based web application that helps users find nearby gyms or parks, chat with an AI chatbot to build a workout plan, and log their workouts for future tracking. Users can log in or sign up, select a location from an interactive map using Leaflet.js, and then talk to a chatbot powered by Google’s Gemini API. The AI will ask smart questions until it gathers key workout details like the type of workout, duration, estimated calories burned, and a health rating. After the workout plan is created, users get a checklist of exercises to complete. Once finished, their workout, notes, and stats are saved to their personal dashboard where they can track their progress and streaks.

How we built it

We built TerpFit using Python Flask for the backend and SQLAlchemy with SQLite for managing the database. The frontend was developed using HTML, CSS, and JavaScript, with Leaflet.js powering the interactive map that displays gyms, parks, and courts near the user. The AI chatbot was integrated using Google’s Gemini API, which allowed us to have conversations that adjust based on user input. We also implemented user authentication so users can log in, log out, and securely track their workout history. Workouts are stored in the database with attributes like date, time, location, workout type, duration, calories burned, and user notes. The dashboard page allows users to view their full workout history and track their workout streak.

Challenges we ran into

One of the biggest challenges we faced during this project was working with the Gemini AI API. Since we were heavily relying on the API to generate workout plans and handle conversations, we quickly ran into issues with API key errors and usage quotas being exceeded. There were multiple times where the API would stop responding or throw unexpected errors, which slowed down development and forced us to find creative ways to minimize requests. Another major challenge was building the interactive map using Leaflet.js and trying to get it to properly identify and display nearby gyms, parks, and courts. Accurately handling location data, saving user selected locations, and ensuring markers appeared correctly on the map took a lot of testing and troubleshooting. Additionally, the front-end page formatting often broke due to conflicting elements between the chatbot, checklist, and map features all being displayed together. 

Accomplishments that we're proud of

We are really proud of being able to fully integrate AI into the workout planning process in a way that feels personalized and useful. We’re also proud of creating a smooth user experience where people can easily log in, select a location, and build a complete workout plan within minutes. Building an interactive map that lets users find real locations and saving that along with their workout data felt like a huge accomplishment. Lastly, being able to track workout streaks and display full workout history in a clean dashboard really brought the whole project together.

What we learned

Throughout this project, we learned how to build a full-stack web application from start to finish while combining several complex technologies. We gained experience working with the Google Gemini API to handle dynamic conversations, integrating Leaflet.js for real-time maps, managing databases with SQLAlchemy, and securing user authentication. We also learned how to design and structure an interactive user flow where all parts of the app communicate smoothly between frontend and backend.

What's next for TerpFit

Moving forward, we want to continue expanding TerpFit by adding even more features. Some ideas include adding Google Maps Street View for location previews, exporting workout plans to PDF, adding email reminders for workouts, and improving the AI’s ability to recommend specific exercises based on user goals. We also want to explore adding social features like sharing workouts with friends or creating group challenges. Overall, we see a lot of potential in TerpFit and hope to keep developing it into an even more powerful fitness companion.
","",https://youtu.be/cRkdUHtTy_0,Best Hack Promoting Public Health - Bloomberg,"javascript, html, css, sqlite, leaflet.js, gemini, api, ai, flask, cors, python",Pranav,Anand,panand1@terpmail.umd.edu,,https://github.com/Pranav-Anand04/TerpFit/tree/main,Best Social Good Hack,Best Sustainability Hack,Best Bitcamp Hack,Hack That Made You Smile,MOST LIT HACK,"We didn’t really use much of the hackathon’s provided technology like Twilio or MongoDB. Most of our time was spent working on our project, TerpFit, which came with a lot of challenges. The experience felt like a roller coaster, frustrating when things wouldn’t work like the Gemini API errors, quota limits, or trying to get the map to show gym locations, but super exciting when everything finally came together and worked the way we wanted.",University of Maryland College Park,Gemini,"Yes, we implemented the Google Gemini API in our project to power an AI chatbot. The chatbot interacts with users, asks them personalized workout related questions, and generates a customized workout plan based on their responses.",149901544261,"",University of Maryland - College Park,2,Tejvir,Badwal,tbadwal@terpmail.umd.edu,Amr,Almehdar,aalmehda@terpmail.umd.edu
μTeX (MuTeX),https://bitcamp2025.devpost.com/submissions/642818-tex-mutex,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 00:57:49,"μTeX: A LaTeX Transcription Tool

Inspiration

μTeX is my two loves of my life combined: chalkboard and LaTeX. However, that does not mean I am the fastest LaTeX writer. I always aspired to be that student who writes math notes in LaTeX, but my slow typing (66 wpm: embarrassing for a CS major) limits my dreams. I needed some external help for accurate LaTeX math notes.
Thinking about my love for LaTeX, I remembered an essay I wrote for an English class about LaTeX. I wrote about how my professor refuses to give LaTeX source code for our homework, and why he should start doing so. One thing I learned at that time was LaTeX's accessibility.


... characteristics of standardized syntax across LaTeX users “...allow the blind user to access some mainstream mathematical resources on the web. … If a blind person reads the formulas on this page with a screen reader, she/he will hear their LaTeX representations.” (Maneki and Jeans) Verbally reading mathematical expressions is challenging, and LaTeX provides a standard tool. 


Then I questioned, how does a blackboard-based math class scene translate to blind individuals? With this question and my struggles writing LaTeX notes, I decided to create μTeX.

What it does

μTeX is a web-based application that transcribes handwritten math equations from blackboards into LaTeX syntax. μTeX uses the Gemini library to recognize math equations in real-time. Live camera captures the user's view, which is then fed into Gemini with a prompt. The output is a clean LaTeX script for easy integration into notes, papers, or digital documents. A key feature is accessibility: LaTeX is accessible, and it provides real-time assistance in equation interpretation for blind and/or visually impaired individuals. TTS is available, which lets blind and visually impaired individuals engage with math content independently. The platform supports both casual users and academic institutions aiming for more inclusive education. Overall, it bridges the gap between traditional math instruction and modern, accessible digital workflows.

How we built it

I used plain HTML, CSS, and JavaScript for this project. I like the simple, old-style websites that are easy to navigate. The Gemini API was used for OCR and the browser's TTS. 

Challenges we ran into

I initially planned to deploy the project via GitHub, but encountered a limitation: using plain JavaScript made it difficult to secure the API key without exposing it. Rather than compromising security, I chose to prioritize improving the core functionality of the product. I believe that building a strong, feature-rich foundation is more important at this stage. Deployment can be addressed later using a more suitable environment like Node.js, where secure key management is easier to implement.

Accomplishments that we're proud of

This is my first time solo-hacking, and I had to start late with limited resources (me and myself). I was able to create a functional product that I am passionate about, hence my two loves combined. I am proud of the hours I put into making μTeX work and keep trying despite the absence of teammates.

What we learned

I think out of all things I learned, the most important lesson was making products accessible. Balancing features with user needs is crucial, and inclusive technology can help everyone in the community.

What's next for μTeX (MuTeX)

Increased accuracy in transcription, deployment, live LaTeX compilation for visual accuracy checking, and volume and speed control for TTS
",https://github.com/jihyopark/bitcamp-2025,https://youtu.be/Swm77OUwtF0,[MLH] Best Use of Gemini API,"gemini, html, css, javascript, tts",Jihyo,Park,jihyopark@gmail.com,,https://github.com/jihyopark/bitcamp-2025,Best Social Good Hack,Best Bitcamp Hack,Best Moonshot Hack,Hack That Made You Smile,"","It was my first time using the Gemini API, which took me a while to implement. It was difficult trying to make a web app just with plain JavaScript, and most functionalities I wanted to implement needed runtime environments.",University of Maryland College Park,Gemini,Gemini API was used to process the images to get the LaTeX script.,322781216403,"","",0
DataVision,https://bitcamp2025.devpost.com/submissions/642819-datavision,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 00:58:11,"Inspiration

We wanted to give people the ability to easily discover interesting relationships and obtain actionable insights from any data they have, regardless of their data science prior knowledge/background.

What it does

DataVision lets users upload CSVs and instantly receive cleaned data, plain-English summaries, suggested visualizations, and hypothesis test results powered by an agentic AI workflow.

How we built it

For our frontend, we used Next.js with TailwindCSS, building a real-time drag-and-drop interface and a dynamically rendering dashboard for the data analysis. For the backend we used Python + Flask to power a microservice that cleans data, runs tests, and streams p-values and visuals, using Google Gemini for our agentic workflow.

Challenges we ran into

Some challenges we had were syncing frontend-backend data streams and coordinating inputs and outputs for our agentic workflow.

Accomplishments that we're proud of

We built an agentic data exploration tool that interacts with pandas dataframes to find interesting relationships in the data with an amazing looking UI.

What we learned

We deepened our understanding of real-time data streaming, backend/frontend orchestration, and agentic AI systems.

What's next for DataVision

We aim to improve our system by creating even more detailed and relevant visuals, adding more useful information about the analysis procedure conducted, and making our data analysis agent more intelligent and capable overall.
",https://github.com/aadia1234/DataVision,https://youtu.be/DR2BjuHC1mY,"Best Financial Hack - Capital One, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best Use of Gemini API","azure, python, typescript, next.js, react.js, gemini, tailwind",Aprameya,Kannan,aprameyakannan@gmail.com,,https://github.com/aadia1234/DataVision,Best Machine Learning Track Hack,Best Moonshot Hack,Best Bitcamp Hack,Prettiest Hack,Hack That Made You Smile,"This project uses React, Next.js, TypeScript, TailwindCSS, Radix UI, React Syntax Highlighter, Lucide Icons, Python with Flask on the backend, Gemini API for AI capabilities, pandas, SciPy, Matplotlib, and Seaborn for data analysis and visualization, all served via Nginx and Gunicorn, deployed on a Microsoft Azure Virtual Machine. We used GitHub for version control.","University of Maryland, College Park",Gemini,"We used Google's Gemini API to generate code to clean data, and provider the user with graphs and analysis about the data they uploaded.",163260503075,www.data-vision.tech,"University of Maryland - College Park, University of Maryland - University College",3,William,Chen,bobbybotbop@gmail.com,Tanay,Naik,tanaynaik149@gmail.com,Aadi,Anand,aadiananddeveloper05@gmail.com
TerpMaps,"",Draft,Pending,Project overview,04/13/2025 01:00:54,"","",,"","javascript, html, css, openai, github, git, directionsapi, mapsjavascriptapi, placesapi, serviceusageapi, mapsstaticapi, vscode, umdapi",Anthony,Nguyen,nnguyen11910@gmail.com,,"",Best First-Time Hack,"","","","",The most important technology we used this weekend was GitHub. Being able to use version control to separately develop the front end and back end before integrating it all together was vital for finishing this hack within the time constraint.,"University of Maryland, College Park",OpenAI,"We first used the University of Maryland API to gather information and data on building names, addresses, latitudes, and longitudes. We also used the Google Maps API to mark waypoints and calculate the distance and time needed to travel between each location marked.",N/A,"",University of Maryland - College Park,1,Kevin,Nguyen,goseiguy@gmail.com
Keep It Up!,https://bitcamp2025.devpost.com/submissions/642833-keep-it-up,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 01:05:10,"Inspiration

Firstly, we just wanted to make a silly game with the hand landmark detection from MediaPipe, a pose detection software developed by Google. Gameplay-wise, we primarily took inspiration from the childhood experience of trying to keep a balloon in the air by any means possible, but also took some gameplay elements from mainly Fruit Ninja and other endless arcade games popular in the 2010s. Stylistically, this game aims to recapture the essence of childhood with stylized animations, taking inspiration from the aesthetics of the early internet era, such as 2010s mobile games, Windows XP, and early Facebook design. Because the goal of the game is to keep a balloon in the air, the name ""Keep It Up!"" serves a double meaning, referring to both the balloon and a simple message of encouragement to the player, accompanied with a thumbs up :) 

What it does

Keep it up, the game uses your webcam that tracks hand landmarks to hit your balloon. LAZERS?

There are also bombs that drop randomly, that you want to dodge, or else your game will end and you will lose. There is a score that keeps track of how many taps you take to keep your balloon in the air. Once you lose the game by letting your balloon fall past the bottom of the screen or hitting a bomb, you have the option to save your score to add to the leaderboard. Once it is saved, you can view your current and previous scores on the main menu's ""scores"" button. Other features we added to the game are the setting function, which enables you to change the volume settings.

How we built it

Frontend:
We built the majority of the game using ReactJS as the frontend. The frontend handled ball rendering, finger tracking overlays, and game logic. In the frontend, we used MediaPipe for hand pose detection and finger tracking.

The bulk of the visuals (titles and buttons) were hand drawn two frame animations in photoshop, which were then put into React Components to swap between frames. 
Backend:
We built a backend using Flask and SQLAlchemy, which allowed us to store scores in a PostgreSQL database hosted on SupaBase. 
Our frontend would make requests to the routes defined in our backend to save scores and get scores to render the leaderboard. 

Challenges we ran into

During the development process, we ran into several issues with modules and packages for both the backend and the frontend. In the frontend, we had difficulty importing the MediaPipe packages. In the backend, we had trouble with virtual environments and python versioning. 

Accomplishments that we're proud of

We made a game yay! PROUD OF OUR FIRST TIME HACKERS!!!!!

What we learned

We learned how to utilize MediaPipe, SupaBase, React.js and more.

What's next for Keep It Up

We play :)
","",,Best Web Hack Using React - Peraton,"react, mediapipe, python, flask, javascript, photoshop, css",Vanessa,Li,vqli412@gmail.com,,https://github.com/echang505/Keep-It-Up,Best First-Time Hack,Best Gamification Hack,Best Razzle Dazzle Hack,Hack That Made You Smile,Prettiest Hack,We used GitHub as the repository. Thumbs Up,University of Maryland,Other,N/A,N/A,"",University of Maryland - College Park,3,Donovan,Lee,dlee124@terpmail.umd.edu,Eric,Chang,eric.chang1000@gmail.com,Jonathan,Xu,jonxu@terpmail.umd.edu
Jesture,https://bitcamp2025.devpost.com/submissions/642834-jesture,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 01:05:20,"Inspiration

The inspiration for Jesture mainly comes from popular fiction. The idea started as one one-off comment by a couple of my friends who thought it would be interesting to be able to control your computer through hand gestures like in Iron Man, where Tony Stark uses hand gestures to control devices or interact with virtual environments. This concept felt futuristic and exciting, but we also saw a practical application: making technology more accessible and intuitive. Gesture-based controls can provide an alternative interface for individuals with limited mobility or those who find traditional input devices like keyboards and mice challenging to use. By combining creativity and accessibility, we aimed to create a tool that is not only fun but also empowers users to interact with technology in a more natural and inclusive way.

A paint application is the perfect demo for this new idea as it showcases the potential of gesture-based controls in a highly interactive yet fun way. Drawing is a universal activity that doesn't require any prior technical knowledge, making it accessible to users of all skill levels (even kids, in this case). By allowing users to create art with simple hand movements, the application demonstrates how natural gestures can replace traditional input devices like a mouse or stylus. A paint application also highlights the precision and responsiveness of the gesture recognition system. ""The artists"" can see immediate feedback as their gestures translate into lines, shapes, and colors on the canvas, making the technology feel seamless and engaging. 

Generally, a paint program provides a solid base to explore features, whether it be the general controls of a gesture-based system or integrating Gen-AI to guess the user's creations.

What it does

Jesture is an interactive drawing and guessing game powered by gesture recognition. Users can draw on a virtual canvas using hand gestures, switch between drawing modes (free draw, line, circle), and have an AI guess what they are drawing. It combines creativity, technology, and fun into one seamless package.

How we built it


MediaPipe for real-time hand gesture recognition.
Raylib for rendering the drawing canvas and UI.
OpenCV for capturing webcam input.
Google GenAI for generating AI-based guesses of user drawings.
Python as the primary programming language to integrate all these technologies.
Tensorflow used to train the hand gesture recognition (thumbs up/thumbs down/pointing/etc.)


Challenges we ran into


Fine-tuning gesture recognition model to ensure accurate and responsive controls.
Managing real-time performance while processing webcam input, rendering graphics, and running AI predictions simultaneously. (Some of my earlier implementations were painfully slow)


Accomplishments that we're proud of


Successfully integrating gesture recognition with a drawing application.
Creating an ""objectively"" fun and interactive experience where AI guesses user drawings.
Building a smooth and responsive interface that feels intuitive to use.


What we learned


How to work with MediaPipe for gesture recognition and its potential for other applications.
The power of Raylib for creating interactive graphics in Python.
The importance of optimizing performance when working with real-time systems.
How to integrate AI models like Google GenAI into creative projects in a fun way


What's next for Jesture


Adding more gesture-based controls?
A multiplayer mode update where users can collaborate on drawings?
Other platforms like mobile?

",https://github.com/jinson-j/jesture,https://youtu.be/K0joSJFCJzc,[MLH] Best Use of Gemini API,"python, tensorflow, mediapipe, opencv, google, gemini, google-genai, raylib",Jinson,Jimmy,jjimmy@terpmail.umd.edu,,https://github.com/jinson-j/jesture,Best Machine Learning Track Hack,Best Razzle Dazzle Hack,Best Moonshot Hack,Hack That Made You Smile,Prettiest Hack,"MediaPipe:
MediaPipe's gesture recognition capabilities were very simple and intuitive to integrate. The GestureRecognizer API provides accurate hand tracking (critical for this project), and training the gesture detection was very fun. 

Raylib:
Raylib was the perfect choice for rendering and creating interactive graphics. Its simplicity and performance make it ideal for projects like this. The ability to easily manage textures and draw shapes is a big plus. More impressively, Raylib has what may be the best documentation I have ever used in a framework.

Google GenAI:
The API is straightforward to use, and the results are engaging. 

OpenCV:
OpenCV is a reliable choice for capturing and processing webcam input. Its compatibility with MediaPipe and ease of use make it a great fit for this project.","University of Maryland, College Park",OpenAI and Gemma,The Google Gemini generative AI API was used to score and compare drawings and guesses based on drawings.,813970917723,"",University of Maryland - College Park,0
ChatGPT Bookmarker,https://bitcamp2025.devpost.com/submissions/642839-chatgpt-bookmarker,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 01:10:14,"Inspiration

When using ChatGPT, I would sometimes get confused and ask ChatGPT to elaborate on a subsection of its output. However, it was annoying to have to scroll and find the original thread after I finished asking the clarification question. 

What it does

Therefore, our team decided to create a web extension, named ChatGPT Bookmarker, to prevent the user from needing to scroll up and down the message history when trying to solve a problem.

Features: 


Collapsable bookmark list 
Draggable and resizable panel for ease of use
Ability to bookmark sub-parts of messages
Renaming and removing bookmarks


How we built it

We created a manifest.json file to make it a Chrome extension. Then, we created a bookmark by using a popup, that we could add to by clicking an add button. To do this, we created Javascript files. We used MutationObservers to tell when ChatGPT generated new messages. We stored the bookmarks and made a popup window that lets the user go back to saved messages. We created icons on Canva to match which the aesthetics of ChatGPT.

Challenges we ran into

One challenge we ran into was preserving the bookmark tab across conversations.

Accomplishments that we're proud of

We created a functional Chrome Extension! 

What we learned

We gained more experience with JavaScript and created our first Chrome extension!

What's next for ChatGPT Bookmarker

We hope to publish this on the Chrome web store soon to gain more users!
","",,"","javascript, html, css",ssong22032,s,ssong22032@gmail.com,,https://github.com/leeann-s/ChatGPT-Bookmarker,Best Bitcamp Hack,"","","","",We used GitHub to distribute files.,"University of Maryland, College Park",OpenAI and Other,"Yes, we developed a Chrome extension for ChatGPT.",N/A,"","Thomas Jefferson High School for Science and Technology, University of Maryland - College Park, UMD",3,Neha,Joshi,neha.joshiusa@gmail.com,Sachi,Verma,shinygem321@gmail.com,Leeann,Shaw,leeann.the.artist@gmail.com
Cyber Sentinel,https://bitcamp2025.devpost.com/submissions/642840-cyber-sentinel,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 01:12:38,"Inspiration

We wanted to build a simple and affordable way to help people stay safe online, especially those who might not be very tech-savvy, like parents, grandparents, or small office teams. A lot of security tools today are either too complicated, cost a monthly fee, or rely on cloud services that collect your data without you really knowing how they work.

With Cyber Sentinel, our goal was to create something that’s easy to set up, doesn’t require ongoing payments, and gives users complete control over their own data. Whether you’re helping a family member avoid risky websites or setting up protection on shared computers, this tool runs quietly in the background and only steps in when something seems suspicious.

What it does

Cyber Sentinel is a Raspberry Pi-powered web threat detection system that works alongside a custom browser extension to:

Collect system statistics during website loads (CPU, RAM, network usage)
Compute z-scores using a baseline dataset to identify anomalous behavior
Perform static code analysis on JavaScript using Semgrep
Block network requests to suspicious sites until the user explicitly allows them
Display a live risk dashboard in the browser popup
All of this happens locally, ensuring both privacy and transparency.

How we built it

We used Tailwind CSS with React components to provide a clean interface. In order to accomplish our malware detection, we used industry-standard YARA rules, which are commonly used amongst cybersecurity companies to detect patterns of malware within website code and downloadable files. In addition, we used the Raspberry Pi to continuously monitor tabular statistics and run a Python script behind the scenes to determine CPU, RAM, and network stability. 

Challenges we ran into

The Raspberry Pi had some security constraints that we needed to circumvent in order to run bash scripts over the SSH-ethernet connection. To circumvent this, we used MongoDB to act as the middleman in processing tasks in a queue-like format.

Accomplishments that we're proud of

Developing a finished product that we have an everyday use for in our lives.

What we learned

It's better to start off with a bigger scope and then narrow down to a Minimum Viable Product. That way, we can ensure that the core functionality of the product stays central. That being said, throughout this journey, we all learned more about cybersecurity and the difficulty that comes with safeguarding networks, websites, and local devices.

What's next for Cyber Sentinel

In the future, we plan to streamline the system by reducing our reliance on a cloud-hosted middleman to connect the Raspberry Pi and the browser extension. While some cloud interaction will still be necessary for generating the final risk assessment using Gemini, the core data collection and analysis will remain local. We're also working on adding the ability to track which websites have already been analyzed, so we can avoid repeating checks and improve performance. Additionally, we want to give users more control, including options to disable blocking temporarily or trigger a deeper, more detailed analysis on certain sites. These next steps will make Cyber Sentinel even more flexible, efficient, and user-friendly, while keeping transparency and privacy at its core.
",https://github.com/Suvrath421/bitcamp2025,,"Best Digital Forensics Related Hack - Cipher Tech Solutions, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","javascript, raspberry-pi, shell, yara, gemini, python, mongodb, css, html",Jordan,Abraham,jabrahamny23@gmail.com,,https://github.com/Suvrath421/bitcamp2025,Best Cybersecurity Track Hack,Best Hardware Hack,Best Moonshot Hack,"","",It was a fun experience full of little frustrations and maximum enjoyment with a touch of pride at the end.,University of Maryland - College Park,Gemini,"Yes, we used the Gemini API to help us build a security profile of a website. The profile we built was a score from 0-100 on how safe the website is to visit and a description on how the score was calculated. The data inputted into the model were the change in machine statistics before and after visiting the website, whether or not it passed the static code analyzer using the rules we wrote, and the website name.",1029346952439,"",University of Maryland - College Park,2,Mihir,Mahesh,emailmihirmahesh@gmail.com,Suvrath,Chivukula,suvrathc@gmail.com
QuantumDecider,https://bitcamp2025.devpost.com/submissions/642844-quantumdecider,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 01:16:18,"Inspiration

Too many kinds of peanut butter, what to wear on a nice day out, or the staggering number of choices when looking for a place to eat. Decision paralysis affects us all - so we thought it would be nice to let someone, or something, else make those choices for a change. 

What it does

QuantumDecider makes a random choice for you - but instead of the usual boring way, it uses magic quantum computing. You can upload a picture of a menu, a list of options, or anything and Gemini will return a list of everything in the image, and then a number of qubits will make a truly random choice for you. You can even set some preferences, and Gemini will help narrow down those choices for you as well. You can also enter your own custom options to choose from, or select a point on a map to get a place to eat near you. It will also show you how Grover's algorithm compares to a linear search on the same problem. 

How we built it

We used Qiskit to implement Grover's and our circuits, Flask for the backend, and Bootstrap to pretty-up the front-end. 

Challenges we ran into

We didn't (and still don't really) know anything about quantum computing, so we spent the majority of the weekend doing research and trying to wrap our heads around such fancy words like diffusion, amplification, and oracles. 

Accomplishments that we're proud of

Just getting a working project - none of us had ever worked with Qiskit or anything quantum-related before. 

What we learned

quantum computing is hard 

What's next for QuantumDecider

Refinement of our Grover's implementation, better image recognition, and more versatility in what kind of choices it can make. 
","https://github.com/edtton/QuantumDecider, https://quantum-decider-ba3a8ee592f0.herokuapp.com/, http://randumb.tech",https://youtu.be/AN4JpzIMyiY,"[MLH] Best Use of .Tech, [MLH] Best Use of Gemini API","python, qiskit, flask, bootstrap, gemini, google-maps",Ethan,T,hainu4235@gmail.com,,https://github.com/edtton/QuantumDecider,Best Advanced Quantum Track Hack,Best Razzle Dazzle Hack,Best UI/UX Hack,Hack That Made You Smile,Prettiest Hack,"GitHub is goated as always, Flask is beautifully lightweight, and Qiskit is cool. Gemini was very easy to integrate with our workflow.","George Mason University, Virginia Tech, Northern Virginia Community College",OpenAI and Gemini,"Gemini to extract text and options from an image, aggregate it into a JSON dictionary, and using Gemini to narrow items down to user-specified criteria.",969471537683,randumb.tech,"George Mason University, Northern Virginia Community College, Virginia Tech",3,Sion,Marak,www.ssmsion@gmail.com,Chris,Browne,cbrowne9472@gmail.com,Aren,Waheed,arenimations@gmail.com
Prod,https://bitcamp2025.devpost.com/submissions/642850-prod,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 01:26:49,"Inspiration

I love music production, however getting into it is not easy. It took me countless hours spent to make decent beats, and learning how to use a DAW like FL Studio is not straightforward. There are many technical challenges that even today after using for 5 years, I am still unsure how to navigate with it.

I wanted to make an online DAW that allows for anyone to cook up. This is designed to be as intuitive as possible for beginners to pick up production fast and pros to cook up without the need to be on FL or another DAW. 

What it does

Beat production functionalities like pattern creation with drums, synthesizers, and allows for sound imports for anyone to import their kit and uses an algorithm to assign what kind of sound the imports are.

How we built it

Next.js, , Gemini for LLM

Challenges we ran into


Getting the UI and audio to respond properly to step adds, pause/play/stop, etc.


Accomplishments that we're proud of


Interviewed 3 producers
Made beats


What we learned


Music production is pretty math-based
Building a DAW is hard


What's next for Prod


Adding customizable mixing effects like reverb or EQ

",https://github.com/amahjoor/Prod,,"Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best Use of Gemini API","nextjs, gemini",Arman,Mahjoor,amahjoor@gmu.edu,,https://github.com/amahjoor/Prod,Best UI/UX Hack,Best Bitcamp Hack,"",Prettiest Hack,MOST LIT HACK,"GitHub: Nice
Gemini: Pretty solid
Nextjs: Amazing",George Mason University,Gemini,"Yes, I used Gemini to assist with beat production",1017322473697,cookingupwith.tech,George Mason University,0
AI Resume Coach,https://bitcamp2025.devpost.com/submissions/642853-ai-resume-coach,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 01:32:47,"Inspiration

Every student in the world wants one thing: a job. But like many of my peers, I feel uncertain about whether the skills I have are translated and explained well in my resume. This inspired me to create an AI-powered Resume Coach to provide instant, personalized feedback that would help job seekers improve their resumes and feel more confident when applying for jobs.

What it does

The AI Resume Coach allows users to upload their resumes in PDF format and receive instant, AI-powered feedback. The app analyzes the content of the resume and compares it to others in similar fields. Using advanced natural language processing and machine learning models, it provides actionable suggestions focused on improving formatting, wording, and content to better align the resume with job goals. The goal is to help job seekers optimize their resumes, boost their chances of landing interviews, and feel confident in their applications.

How I built it

The app was built using Streamlit for the interface, which allowed me to quickly create a simple and intuitive web application. I then took a dataset of 13,000+ resumes and preprocessed them using Natural Language Processing. I added this preprocessed data into a database using ChromaDB. These stored resumes act as a ""training set"" for the AI model to look at stored resumes for context. For the AI-powered feedback, I used Hugging Face’s GPT-2 model to generate detailed feedback on resumes. From the user's POV, the process starts with a user uploading their resume, then the app extracts text from the PDF, processes it, and uses the AI model to generate feedback, which is displayed to the user in an easy-to-understand format.

Challenges I ran into

One of the major challenges I faced was integrating PDF extraction into the app. Extracting text from PDF resumes in a way that preserved the formatting and structure was tricky, especially when dealing with resumes of varying complexity. I also had to refine the AI feedback generation to ensure that the suggestions were practical and actionable without being too generic. Performance was another concern, as generating feedback could take a few seconds, and I wanted to balance speed and accuracy.

Accomplishments that I'm proud of

I'm proud of building my first functional app that allows job seekers to receive personalized resume feedback in real-time. The integration of GPT-2 for text generation provided high-quality, context-aware feedback, and ChromaDB helped me store and retrieve similar resumes for comparison. Deploying the app on Streamlit Cloud made it easy to share with users, making it accessible to anyone who needs it. I'm especially proud of how the app supports students and job seekers who might not otherwise have access to professional resume feedback.

What I learned

This project taught me a lot about integrating machine learning models with web applications. I learned how to use Hugging Face’s model pipeline and Streamlit to quickly prototype a functional app. I also gained hands-on experience with ChromaDB for managing and querying embeddings, which was essential for making the feedback more relevant. Most importantly, the project reinforced the value of user experience in building an app that not only works well but is also intuitive and easy to use.

What's next for AI Resume Coach

Maybe after this competition, it's time to get a team together and create a more fleshed-out application. This project has a lot of potential to help any college student.
",https://github.com/kavin-manivannan/Bitcamp2025/tree/main,https://youtu.be/tuDykTjaqR4,Best Use of GenAI in Business - Cloudforce/Microsoft,"python, streamlit, chromadb, hugging-face, natural-language-processing, vscode",Kavin,Manivannan,kavinmanivannan055@gmail.com,,https://github.com/kavin-manivannan/Bitcamp2025,Best First-Time Hack,Best Social Good Hack,Best Moonshot Hack,Hack That Made You Smile,MOST LIT HACK,"This weekend, I worked primarily with Streamlit, Hugging Face, and ChromaDB to develop my AI Resume Coach app. I was able to create a user-friendly interface with Streamlit, which allowed users to upload their resumes and receive AI-powered feedback. This experience helped me deepen my understanding of Streamlit's capabilities for rapid prototyping and how it can be seamlessly integrated with other tools for building real-time applications. I also used the Hugging Face API to integrate GPT-2 for generating feedback. This was my first time using Hugging Face’s model pipeline. Additionally, I worked with ChromaDB to store and retrieve resume data. This was my first experience with ChromaDB, and I was impressed by its simplicity and effectiveness in handling embeddings. It helped me quickly set up a database to store similar resumes, which I used for comparison when generating feedback.",University of Maryland Baltimore County,Hugging Face,"Yes, I implemented a generative AI model in my hack. I used the GPT-2 model via the Hugging Face API to provide AI-powered feedback on resumes. The model analyzes the content of a resume and generates constructive feedback, focusing on areas such as formatting, wording, missing details, and how well the resume aligns with a candidate’s likely job goals. I integrated this model into my AI Resume Coach app to offer personalized, actionable insights to users in real-time, helping them improve their resumes and increase their chances of landing job interviews.",N/A,http://localhost:8501,University of Maryland - Baltimore County,0
CreditGuard,"",Draft,Pending,Project overview,04/13/2025 01:46:36,"","",,"","",Nitya,Chigurupati,nschigurupati@gmail.com,,"","","","","","","","","","","","","",1,Anu,Daga,anudaga312@gmail.com
Netflix Summary Extension,https://bitcamp2025.devpost.com/submissions/642862-netflix-summary-extension,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 01:47:02,"Inspiration

Originally we were working on creating a kindle plugin that had a similar feature. It would summarize a book up to the current page and provide key insights so that you could continue reading. We wanted to broaden our scope to more than just kindles and Netflix seemed like a good place to start.

What it does

It is an extension for Chrome and any other browsers that are compatible with Chrome Extensions. When you have your browser open, you can use the pop up window. If you are on Netflix then it will ask you to select a show or movie. If you are not on Netflix then it will tell you that you are not on Netflix. It has two buttons that can give a review of a select show or a summary up to the current episode. In the case of movies, it will see how far you are in a movie and use that progress to generate a summary. 

How we built it

We first followed a guide to make a very simple Chrome Extension that says Hello. From here, we simply added and changed the functionality to suit our case. A lot of googling was involved, understanding how extensions can interact with webpages and get data from them. We had some knowledge of HTML and CSS, so the designing part was not too bad. Chrome Extensions are basically like mini webpages, utilizing the same tech that websites do. Most of the time was figuring out how to make the scripts that ensured button functionality and used Gemini.  

Challenges we ran into

The first issue we ran into was how to get data from the webpage. Netflix has a lot of HTML elements and we needed to find the right ones to get the information we wanted. There were multiple options but we wanted consistency across different types of media on Netflix. 

The next issue we ran into was making the call to Gemini. It was hard to test with this and configure the call to the model. Since we were using a fetch call with the url, there were many issues that would resolve to an error 404. Eventually, we were able to properly call the model and get the data we needed. Figuring out the correct model for our use cases was also an issue, but we ended up with 2.0 flash. 

Accomplishments that we're proud of

We are really proud of the styling. We wanted to match the Netflix feel and I think we did a good job of that. We also added in some basic animation for the buttons. 

What we learned

The biggest thing we learned was how to work together. At the start, we found it hard to work together. Eventually we found our grove and communicated effectively. We were able to delegate tasks and ensure that work actually got done by everyone. 

We also learned how to effectively build a project up. Usually we start with something but here we had nothing. I think it was a good experience to figure out where to start from and where to find resources.

What's next for Netflix Summary Extension

We hope to add functionality to other popular streaming services like DisneyPlus. Another aspect that we could improve on is the Gemini calls. The summaries are not perfect. We could implement technologies like RAG or fine tuning to make the model more effective for our use cases. 
",https://github.com/ajain-us/NetflixSummaryExtensionWT,https://youtu.be/BmfbKu-4YL0,[MLH] Best Use of Gemini API,"javascript, chrome, html, css, gemini",Aditya,Jain,ajain55@terpmail.umd.edu,,https://github.com/ajain-us/NetflixSummaryExtensionWT,Best UI/UX Hack,Best First-Time Hack,Best Razzle Dazzle Hack,Prettiest Hack,Hack That Made You Smile,We mainly used github and had a positive experience.,University of Maryland,Gemini,We used Gemini to generate summaries and reviews. We used it to generate up to date information from the internet.,813970917723,"",University of Maryland - College Park,1,Matthew,Maher,mmaher0722@gmail.com
TerraCare,https://bitcamp2025.devpost.com/submissions/642863-terracare,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 01:48:08,"Inspiration

Before coming to Bitcamp, a few of us were feeling anxious and unsure about what to expect. We wished there was someone we could talk to, someone who could understand and offer support. This idea of connecting with someone in a more personal way, especially in a tech environment like Bitcamp, sparked the inspiration for TerraCare. We realized that many people, especially in stressful environments, need someone or something to talk to that’s both helpful and supportive. Our project, TerraCare, aims to bridge that gap, offering a platform that can provide real-time assistance and advice, especially for people going through tough moments.

What it does

TerraCare is an AI-powered platform designed to help users navigate moments of stress and anxiety, providing them with personalized recommendations, tips, and guidance. It leverages natural language processing (NLP) and machine learning to offer real-time conversational support. Whether you’re feeling overwhelmed or just need someone to talk to, TerraCare is there to listen, understand, and guide you through those moments.

How we built it

The data is stored in PostgreSQL, and we use cloud hosting for scalability. The user interface is built with HTML, CSS, and JavaScript, ensuring it’s simple yet effective. The backend relies on gemini-2.0-flash and flask.

Challenges we ran into

One of the biggest challenges we faced was designing the cache system in such a way that it didn't use too much data or too many tokens when prompting the model. With long conversations, we risked overloading the model with excessive data. Balancing efficient storage of conversation history while ensuring that the context given to the model was meaningful and concise was tricky. We had to optimize how much historical data we stored and selectively pass only the most relevant parts of the conversation to the model to prevent unnecessary data usage.

Additionally, Git collaboration was another significant challenge. As a team, we had trouble managing multiple branches and resolving merge conflicts. It was tough to stay on the same page with all the changes being made, especially since some of us were more familiar with Git than others.

Accomplishments that we're proud of

We’re really proud of how we were able to build a functional and intuitive AI assistant that can understand emotional cues and provide personalized responses. It’s not just about delivering information—it’s about offering support. We also integrated real-time conversation history into the model so that TerraCare can provide more relevant responses based on the user’s previous interactions, which makes the experience feel more human-like. The careful design of our cache system allowed us to reduce token usage while maintaining context, which is a key feature of the system.

What we learned

We all came from different knowledge and experience levels, so each of us learned new languages or tools. Some of us got more familiar with machine learning, others with web development frameworks, and some learned how to use Git more effectively. The biggest takeaway for all of us was learning how to work in a team. We all had to communicate better, handle different perspectives, and collaborate on tasks in a way that allowed us to build something meaningful together.

What's next for TerraCare

We’re planning to expand TerraCare’s capabilities by integrating more advanced features, like voice to text, so users can interact with it hands-free. We also want to explore deeper personalization features, such as mood tracking and long-term emotional support, to make the experience more meaningful.
","https://therapistai-vput.onrender.com/, https://github.com/ayank674/therapistAI/tree/production",,[MLH] Best Use of Gemini API,"html, css, javascript, flask, python, postgresql, jinja, ajax, git, render",Phani,Kuppa,phanimkuppa@gmail.com,,https://github.com/ayank674/therapistAI/tree/production,Best First-Time Hack,Best Social Good Hack,Best Bitcamp Hack,"","","The Gemini API was central to the success of our AI-powered platform. It provided us with accurate and contextually aware responses, which were crucial for building TerraCare’s conversational abilities. Integrating Gemini was relatively straightforward, and its ability to handle complex user queries allowed us to focus more on refining the user experience. However, we did run into challenges with token limits when managing conversation history. Ensuring that the AI received enough context without overwhelming the system required us to optimize the cache system carefully.

PostgreSQL was the backbone of our data storage, allowing us to manage user data and conversation history effectively. We used it to store both user demographics and the chat history in the cache, making it easy to retrieve relevant information for the AI to generate responses. The relational structure of PostgreSQL fit well with our needs, and it provided the scalability we needed for handling growing datasets. The only challenge we faced with PostgreSQL was optimizing database queries to handle large volumes of data efficiently, which we resolved by refining indexing and query structures.

Git played a critical role in our collaboration. Since our team had varying levels of experience with version control, we initially struggled with managing multiple branches and resolving merge conflicts. However, once we adopted a more structured Git workflow and communicated more effectively, our use of Git became much smoother. It helped us stay on track with the development process, ensured we could work on separate features simultaneously, and allowed us to keep our codebase organized.",University of Maryland,OpenAI and Gemini,"Yes, we implemented Gemini, a generative AI API, in our hack this weekend. The Gemini API was used to power the conversational aspect of our project, TerraCare. By leveraging this API, we were able to generate human-like responses to user input, enabling our platform to provide real-time support and emotional guidance.

The model was key to creating an engaging user experience, allowing TerraCare to interact in a meaningful and empathetic way, especially when dealing with sensitive topics such as anxiety and stress. We used the Gemini API to generate contextually relevant responses based on the conversation history, giving users the sense that they were talking to a caring entity. We did face some challenges, particularly with managing token usage and conversation history, but overall, Gemini played a crucial role in delivering personalized interactions and shaping the AI's ability to respond intelligently to user queries.",78453606944,"","University of Maryland - University College, University of Maryland - College Park",3,Ethan,Chavis,ethanchavis11@gmail.com,achanmya,Thu,ahlinkar8965@gmail.com,Ayan,Kakadiya,kakadiyaayan123@gmail.com
BizLingo,https://bitcamp2025.devpost.com/submissions/642866-bizlingo,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 01:50:14,"Inspired by the fact that we as college students don't have that much knowledge on finances.

What it does

How we built it

Challenges we ran into

Accomplishments that we're proud of

What we learned

What's next for BizLingo
",https://hack-vf6d.vercel.app/,,"Best Financial Hack - Capital One, Best Web Hack Using React - Peraton","react, node.js, firebase, vercel, vue",Nell-Tes,Tesfaye,simrettasina@gmail.com,,https://github.com/MeronAGithub/Hack.git,Best App Dev Track Hack,Best Gamification Hack,Best UI/UX Hack,Hack That Made You Smile,MOST LIT HACK,Had so much fun playing around with different software and narrowing down what to use.,University of Maryland College Park,Other,Yes,N/A,"",University of Maryland - College Park,2,M,.,meron3319@gmail.com,Moosay,Hailewold,moosayhailewold@gmail.com
MarlowOS,https://bitcamp2025.devpost.com/submissions/642870-marlowos,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 01:55:26,"Inspiration

We recently set up a security camera in our college apartment after getting a server to host our website. It worked fine, but I quickly realized its biggest limitation — it could only monitor one room. I didn’t want to spend more money on multiple cameras, so I started thinking: what if we used a drone instead? That’s how the idea for a drone-based security system was born — one that can track intruders, follow them around the apartment, and even snap pictures in real time.

What it does

Our system uses facial recognition to detect and track a person’s face. Once an intruder is spotted, the drone automatically follows them and records their movement. It can also take snapshots of the person and save them for review later — all while navigating around the space autonomously.

How we built it

We combined off-the-shelf facial recognition libraries with a basic drone SDK. The drone takes camera input, processes it on-device (or through a lightweight server), and uses that to make real-time movement decisions. A lot of testing went into making sure the drone could actually keep up with someone moving and respond quickly enough.

Challenges we ran into

Facial tracking was no joke — it took a lot of tweaking to get it to work consistently, especially when lighting or angles changed. Getting the drone to fly smoothly without crashing while also tracking a moving target was another huge pain point.

Accomplishments that we're proud of

Honestly, the fact that we got it working at all is something we’re super proud of. Watching the drone actually follow someone and take photos was one of the coolest moments — it felt like something out of a sci-fi movie.

What we learned

We learned a ton about real-time computer vision, how to control drones through code, and the limits of consumer tech when you try to push it just a little further. We also realized that hardware projects are a different kind of grind — more bugs, more crashes, but way more satisfying when things click.

What's next for MarlowOS

We want to make it smarter — like recognizing specific people, integrating voice commands, or syncing it with a full smart home setup. We’re also thinking about turning this into a more accessible plug-and-play security solution that anyone can set up at home without needing multiple expensive cameras.
","",,"","javascript, node.js, python, machine-learning, github",Sai,Rangineeni,sairangineeni1@gmail.com,,https://github.com/Laphatize/MarlowOS,Best Hardware Hack,Best UI/UX Hack,Best Bitcamp Hack,MOST LIT HACK,Hack That Made You Smile,We used Github and MongoDB!,The Pennsylvania State University,OpenAI and Llama,"Yea, I used Gemini to help build the app.",N/A,"","Pennsylvania State University, Garnet Valley High School",1,Pranav,Ramesh,laphatize@protonmail.com
YummyBytesAI,https://bitcamp2025.devpost.com/submissions/642873-yummybytesai,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 01:59:51,"Inspiration

As college students, sometimes we are limited to certain ingredients left in our kitchen and on a small budget. With YummyBytesAI, we can input our personal dietary preferences and restrictions for AI to help us create an idea for a meal. 

What it does

YummyBytesAI takes in the dietary restrictions, allergies, cuisines, and ingredients you input to generate a complete ingredient list and instructions to cook a delicious meal.

How we built it

With the use of Gemini API and React, we coded the project in JavaScript, HTML, and CSS. 

Challenges we ran into

As this was our first time utilizing an AI API and having little foundations in React, we had to do a lot of research and test runs within a short amount of time to gradually get our project to its final stage. Making specific prompts to ensure that Gemini produces the proper results was also a difficult challenge, given the creative lengths and results that it can have. 

Accomplishments that we're proud of

In a short number of hours, we were able to learn two complex tools and develop a project with the functions that we were aiming to create.

What we learned

Collaborating together effectively across GitHub and VSCode, utilizing APIs, UI/UX design, prompt engineering, problem-solving, critical thinking, communication, and staying composed under pressure.

What's next for YummyBytesAI

As we continue learning new things in computer science, we hope to be able to come back and revisit this project and implement new features to develop a more enhanced experience for users.
",https://nickyuannn.github.io/ai_chef/,https://youtu.be/B16ltDNyJXo?feature=shared,[MLH] Best Use of Gemini API,"javascript, react, css, html, geminiapi",Allison,Lee,allisonlee422@gmail.com,,https://nickyuannn.github.io/ai_chef/,Best First-Time Hack,Best UI/UX Hack,Best Social Good Hack,MOST LIT HACK,Hack That Made You Smile,"We made use of React, GitHub, and Gemini API to create our project, all of which were very useful resources. GitHub was incredibly helpful for collaborative efforts, while Gemini and React were quite straightforward to pick up and use as first time users of API and website creating.","University of Maryland, College Park",Gemini,"Yes, we implemented Google's Gemini API in our website to generate recipe names, ingredients, and instructions with the selected and typed inputs provided by users.",718405014457,yummybytes.tech,University of Maryland - College Park,2,Nick,Yuan,nk.yuan2005@gmail.com,Juno,Song,jsong04@terpmail.umd.edu
Untitled,"",Draft,Pending,Manage team,04/13/2025 02:00:52,"","",,"","",Israel,Ogwu,izzymax192@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,1,Pramukh,Bhushan,pramukh.bhushan@gmail.com
CapitalClarity,https://bitcamp2025.devpost.com/submissions/642876-capitalclarity,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 02:01:38,"CapitalClarity

“Smart spending starts with smart awareness.”

CapitalClarity is a Gemini AI-powered React Native app designed to help users take control of their finances. By analyzing your spending history, the app delivers real-time, personalized financial insights that empower you to make smarter, budget-conscious decisions. Think of it as your intelligent financial coach, always learning and guiding you.



The Problem

Many people—especially students—struggle with managing personal finances. The issue isn’t always about income, but rather a lack of awareness and context around spending.


“I check my bank app and just think… where did all my money go?”


In fact:


42% of college students carry credit card debt, often unaware of how daily habits accumulate.
Small, frequent purchases tend to slip under the radar, making budgeting difficult.
Existing financial tools often feel disconnected or require too much manual input.




Inspiration

CapitalClarity was born out of personal pain points:


“We’ve both had moments where we blew our entire paycheck in a weekend—without really knowing how.”



On-campus job earnings would quickly disappear with little to show for it.
Impulse spending became a norm due to lack of financial feedback in real-time.
We needed something proactive—not reactive—that could learn from us and help build healthier habits.




The Project

CapitalClarity doesn't just track expenses—it understands them. By combining AI-powered reasoning with user-specific context, the app classifies expenses, flags potentially wasteful spending, and adapts over time.

Technologies Used



Tech
Why We Chose It



React Native
Unified mobile development across platforms, enabling quick iteration and a polished user experience.


Flask (Python)
Gave us access to Python’s ecosystem for processing data and running AI models efficiently.


Gemini AI 2.5
Handled deep analysis—e.g., determining if a purchase at ""Target"" was an emergency or impulse buy.


Gemini AI 2.0 Multi-Input
Allowed fast, continuous input handling for adaptive learning based on real-time transaction streams.





What We Learned


Financial behavior is complex – Classifying spending as “unnecessary” often requires understanding personal context, not just transaction data.
User-centric design is crucial – The more intuitive the interface, the more likely users will stick around and benefit.
End-to-end integration matters – Building a fast, reliable pipeline from Plaid → Backend → AI → UI was key to delivering real-time insights.
Feedback loops improve accuracy – Our predictions became much sharper once we added in user feedback and historical behavior reinforcement.



“The model got better once it listened to what we had to say.”




⚠️ Challenges Faced

1. Real-Time Data Pipeline

Getting real banking data is hard. We attempted to use the Plaid API to seamlessley integrate with Capital One, Chase, and other bank accounts, but were blocked by multi-day api approval. 

Thus, we had to pivot into creative solutions for getting transaction data, emulating the real-time nature we wanted the application to capture.

2. Accurate Predictions with Limited Data

Here’s the tough part: we often had only the vendor name and amount for each transaction.


“$8.99 at Taco Bell —was it a meal or a craving induced snack”


To tackle this, we:


Analyzed time of transaction to detect patterns
Used merchant categories to infer context (e.g., groceries vs. fast food)
Collected user feedback to fine-tune prediction weights


This created a feedback loop that made the app smarter the more you used it.



Looking Ahead

CapitalClarity is still evolving. Some ideas we’re excited to explore:


Better AI Insights to teach users about their spending habits in a personal manner
Bank Integrations to allow for a more seamless approach to our application
Personalizing the app, recognizing different cultures and spending habits to create a more inclusive environments.



“We want this to be less of an app—and more of a personal money mentor.”




Built with clarity in mind. Powered by AI. Designed for your life.
",https://github.com/ukataria/Bitcamp2025,https://youtu.be/P1f9dcfQWEo,"Best Financial Hack - Capital One, Best Web Hack Using React - Peraton, [MLH] Best Use of Gemini API","pandas, javascript, react-navigation, react-native-modal, react-native, expo.io, python, flask, rest, google-gemini, csv, typescript",MehulG873,Goel,mehulg6th@gmail.com,,https://github.com/ukataria/Bitcamp2025,Best App Dev Track Hack,Best Bitcamp Hack,Best UI/UX Hack,"","","This weekend was enriching as we gained hands-on experience with React Native for rapid mobile app development, Flask for backend API integration, and Google Gemini's AI capabilities. The seamless interaction between front-end mobile development and backend API handling was particularly rewarding.

Gemini API: Powerful and easy to integrate, Gemini API provided reliable and insightful analysis of transaction data, significantly enhancing user experience.
Expo: Allowed rapid iteration and testing across multiple platforms, speeding up development significantly.
Flask: Offered simplicity and flexibility, making backend API creation straightforward and highly effective for our use case.","University of Maryland, Carnegie Mellon University",OpenAI,"We used Google Gemini API extensively to analyze transaction data, generating personalized financial insights categorized as tips, warnings, and achievements. 

This was done in two parts, the first was a comprehensive multi-modal analysis into a structured output giving insights for the application. The second was a multi-turn thinking model to update it's understanding of the user to get new features.","1057115058857, 566042363836","","Lynbrook High School, University of Maryland - College Park",1,Utsav,Kataria,utsavkataria05@gmail.com
oto hana.,https://bitcamp2025.devpost.com/submissions/642879-oto-hana,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 02:05:06,"Inspiration

Both of us love listening to Spotify but sometimes its annoying to think long of all the songs you want to add to your playlist. Maybe you want to find music that matches your current mood, without having to search through countless tracks yourself. This inspired us to create a platform leveraging the Gemini 2.0 Flash AI to do that work for us.

What it does

You log into your Spotify account, describe how you're feeling, and the application generates a corresponding mood profile. It then presents you with ten pre-selected songs based on that mood, allowing you to click to either add a song to your playlist or skip it. Gemini then analyzes your selections along with your mood description to fill the playlist and the remainder of the playlist with compatible songs.

How we built it

""oto hana"" integrates Spotify's Web API with Gemini API on a Next.js/React foundation. We used Gemini   to create a sentiment analysis pipeline that maps a user's feeling/description into a single mood. The system generates initial recommendations, then fine tunes the remaining selections from the user feedback based on the their initial selection of songs.

Challenges we ran into

The environment setup proved quite tedious, and we encountered some difficulties perfecting the UI design. We faced significant compatibility issues between different Node.js versions and the Spotify Web API, resulting in hours of debugging. The Spotify Web API also required us to have an SSL certificate, so setting it up for Spotify user authentication made local setup more tedious. Additionally, optimizing the application's performance while maintaining responsive design across devices led to a lot of troubleshooting and refactoring of our initial implementation.

Accomplishments that we're proud of

We successfully fine-tuned Gemini to generate songs based on moods derived from user descriptions, and effectively connected Spotify and Gemini APIs to work in unison with each other. The simplistic user flow was also something we were proud of, as we managed to create a simple experience that guides users seamlessly from mood description to playlist generation. Our prompt engineering efforts also very impressive in matching emotional states to musical selections, creating a more personalized playlists that consistently align with the users' described feelings. 

What we learned

We learned that setting up a development environment properly is more challenging than anticipated. We also learned that sometimes simple is better for product and implementation. A database could have been implemented but was not necessary in order to have a successful product. We also learned how to work with endpoints through ngrok for local development testing with external APIs. The importance of prompt engineering became evident as we refined our approach to natural language processing. User testing was also very important because for simple prompts it worked very well, but for more complex prompts it showed that refinement was needed in enhancing our API calls.

What's next for oto hana

We plan to incorporate users' Spotify listening activity to further enhance personalization. By importing users' listening history, we can identify favorite songs and artists to incorporate into future playlists.
",https://github.com/govinds108/Otohana,https://www.youtube.com/watch?v=oFG_m8ieXPM,"Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of Gemini API","javascript, next.js, react.js, spotify, gemini",Govind,Singhal,govindsinghal108@yahoo.com,,https://github.com/govinds108/Otohana,Best Machine Learning Track Hack,Best Moonshot Hack,Best Bitcamp Hack,Hack That Made You Smile,MOST LIT HACK,Github Co-pilot was a great resource for debugging and searching for documentation on API usage. Spotify Web API was difficult to work with because it tends to have issue depending on versions being used. Gemini was easy to use and integrate within the web implementation.,"University of Maryland, College Park",OpenAI and Gemini,Yes. We used Gemini API to perform sentiment analysis on user responses.,1036995308480,"","",1,Helen,Li,hli12349@umd.edu
TravelBuddy,https://bitcamp2025.devpost.com/submissions/642880-travelbuddy,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 02:05:48,"🚀 What it does

TravelBuddy allows users to:


✈️ Generate smart travel itineraries based on source, destination, and dates
🚗 Get a route summary that estimates travel time by road (if in-country) or flight (if international)
📍 Discover attractions and restaurants near destinations (via the Foursquare API)
🤖 All powered by LLMs like Gemini, integrated into a smooth React frontend + FastAPI backend




🛠 How we built it


Frontend:  


React + Vite for blazing-fast SPA development
Tailwind CSS for sleek, responsive styling

Backend:  


FastAPI (Python) with modular route/logic separation
Asynchronous handling and LLM prompt management

AI Integration:  


Google Gemini Pro/Flash for natural language content generation
Custom prompt engineering for itinerary and route summaries

APIs Used:  


🗺 Foursquare Places API – for attractions & restaurants
🧠 Google Gemini API – for AI-generated trip content





🧩 Challenges we ran into


🧾 LLM response parsing
Gemini returns natural text, not JSON — formatting the response reliably was tricky
🛫 Multiple airports per city
Estimating flights is tough when cities have several airports (e.g. New York, London)
🌦 Weather forecast limits
Most APIs cap forecast data to 14 days — which limits trip planning beyond that window
🔄 Frontend-backend sync
Async API calls + Gemini latency required careful loading state management in React




🏆 Accomplishments that we're proud of


🚀 Built a full-stack AI travel planner from scratch in under 36 hours
🧠 Integrated a production-grade LLM into a real-time planning tool
🧱 Designed a clean, scalable backend with reusable prompt modules
🎨 Built a beautiful, responsive frontend with interactive trip planning flow




📚 What we learned


📝 How to structure prompts for consistent LLM outputs
🕸 FastAPI + async best practices
🔁 Designing scalable frontend-backend data flows
🧩 Implementing fallback logic when APIs are rate-limited or fail




🔮 What's next for TravelBuddy


🗺 Show maps and directions between locations
📱 Package as a mobile app or Chrome extension
⛅ Integrate live weather and local events into itineraries
🧭 Let users save + share travel plans

","https://github.com/jayeshpamnani99/TravelAgentBackend-BitCamp, https://github.com/jayeshpamnani99/TravelAgentFrontend, https://travel-buddy-frontend-drab.vercel.app/, https://www.loom.com/share/d1bc1e11de2446658f857da582a1a463",,"","fastapi, react, amazon-web-services, gemini, tailwind",Jayesh Jairaj,Pamnani,jpamnani@umd.edu,,https://github.com/jayeshpamnani99/TravelAgentBackend-BitCamp,Best App Dev Track Hack,Best Social Good Hack,Best Social Good Hack,Prettiest Hack,MOST LIT HACK,"Frontend: React + Vite + Tailwind CSS for fast UI development.

Backend: FastAPI (Python) with clean modular routing and logic.

AI Integration: Google Gemini Pro/Flash used to generate itineraries and route summaries using custom prompts.

APIs Used:

Foursquare Places API (for restaurants/attractions)

Google Gemini API (for AI-generated content)",University of Maryland - College Park,Gemini,"AI Integration: Google Gemini Pro/Flash used to generate itineraries and route summaries using custom prompts.
How to structure prompts for better LLM results.",889911423350,"",University of Maryland - College Park,3,Hiren,Khanchandani,hiren14@umd.edu,Pradyumna,Upadhyay,pdupd28@umd.edu,Ishaan,bajpai,bajpaiishan@yahoo.in
BudgetBuddy,"",Draft,Pending,Additional info,04/13/2025 02:07:23,"Inspiration

As college students who like to have fun, we wanted to make budgeting less boring and more emotional for all. Financial apps are often too cold or overwhelming, we wanted to create something that felt alive, fun, and deeply personal. Inspired by Tamagotchis and the psychology of money habits, we built BudgetBuddy: a virtual pet that reacts to your real-world spending! 

What it does

BudgetBuddy helps users understand and manage their finances in a gamified, emotional way:
💳 Connects to your transaction history (via Capital One's Nessie API)
🧠 Analyzes your recent spending to detect trends, big purchases, and to help keep you on track
🐣 Your virtual buddy reacts to your spending behavior, getting happier or sadder depending on your spending habits
🗣️ Chat with your buddy, who gives feedback in a sarcastic, supportive, or soft tone based on your preferences
📈 Tracks your monthly budget and provides LLM-powered insights

How we built it

Frontend: React with TypeScript and Vite, Zustand for state management, TanStack Router for routing, styled-components for styling, and HTML/CSS/JS for the core UI structure and interactions.
Backend: Python (Flask), MongoDB Atlas for user data, and the Nessie API for real financial transaction simulation.
Transaction Generator: Custom Python logic to generate realistic spend data based on merchant categories and emotional context.
AI Integration: Google Gemini API for generating feedback and conversation.
Fraud Detection: Used IsolationForest from scikit-learn to flag anomalous purchases.

Challenges we ran into


Mapping transaction data to meaningful categories for emotional interpretation.
Ensuring smooth API interaction with Nessie while syncing accounts and transactions.
Balancing LLM personality to be fun and helpful, without losing clarity.
Debugging transaction history fetches and aligning real-time insights with our buddy’s emotions.


Accomplishments that we're proud of


Creating a character-based feedback loop that feels emotionally engaging.
Implementing a working fraud detection model in a limited timeframe.
Combining LLMs + classic ML with real-time financial behavior to create a unique scoring system.
Designing a user onboarding flow that feels more like a game than a finance form.
Integrated the Nessie API to simulate real-world financial transactions 
Implemented MongoDB Atlas to securely store user data


What we learned


How to integrate multiple APIs, including Capital One's Nessie banking simulation and a generative LLM model.
How to apply machine learning in a real-world use case like fraud detection.
The power of emotional design, turning something cold like finance into a warm, interactive experience.
That financial literacy doesn't have to be intimidating if you make it personal and playful.
How to deal with running into roadblocks (many of them).


What's next for BudgetBuddy

🔄 Integrate with real financial institutions (Plaid, Stripe, etc.)
📲 Launch as a mobile app rather than just a web app, alongside a widget for your homescreen
🎯 Add personalized goals, weekly spending challenges, and social sharing features
","https://github.com/team-moondust/BudgetBuddy-frontend, https://github.com/team-moondust/BudgetBuddy-notifications-api, https://github.com/team-moondust/BudgetBuddy-backend",,"Best Financial Hack - Capital One, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","python, nessie, flask, mongodb, react, vite, zustand, tanstack, html, css, isolationforest, gemini, scikit-learn",Ayan,Banerjee,ayanbanerj@gmail.com,,https://github.com/team-moondust/BudgetBuddy-backend,Best Machine Learning Track Hack,Best Gamification Hack,Best UI/UX Hack,Hack That Made You Smile,Prettiest Hack,"NessieAPI - We had a struggle using it because while we were using it, we discovered that the enterprise API was down, which was vital to accessing/creating financial data for us to use. However, to combat this, we used the customer data and the Nessie API data model to create our own customers and transactions while maintaining the data style of the API, allowing us to still use it as required. 

MongoDB Atlas - It was our first time using MongoDB Atlas, but despite this, we were able to work with it successfully. The integration with Flask was seamless and reliable throughout the whole development process. 

GeminiAPI - This API allowed us to bring our Tamagotchi-style chatbot to life. We appreciated how easy it was to get started with these tools  and it allowed us to bring our product to a whole new level.","University of Maryland, College Park",Gemini,"We integrated Google's Gemini API to power our chatbot. This will analyze a user's spending history and be able to respond with personalized, emotionally intelligent financial advice. Additionally, it well keep in mind the goals and tone the user input and use them while talking with the user.",186897952284,"","University of Maryland - College Park, Bridgewater-Raritan High School",3,Tanish,Bollam,tansboys@gmail.com,Marvin,Lin,larvinmin@gmail.com,Raghav,Misra,raghav.m2014@gmail.com
Footprint,"",Draft,Pending,Project overview,04/13/2025 02:08:27,"","",,"","",Reza,Jalil,rjalil@terpmail.umd.edu,,"","","","","","","","","","","","",University of Maryland - College Park,1,Jishnu,Deep,jdeep@terpmail.umd.edu
campfireify,https://bitcamp2025.devpost.com/submissions/642888-campfireify,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 02:10:18,"Inspiration

Other websites such as icebergify or receiptify have a unique approach on dealing with Spotify user data that allows them to have some fun facts about their usage, listening history, etc. Combined with the fact that Spotify is well known for its Spotify Wrapped, we wanted to do our own, smaller scale take on it.

What it does

Currently, the Spotify Development App is able to access and display your top artists, top tracks, currently played track, and also look at more universal Spotify stats such as top tracks per country, all of which is displayed in various formats.

How we built it

We coded a python back-end and a html front-end which is currently locally hosted, in interaction with Spotify's Web API.

Challenges we ran into

We had a couple of issues learning the syntax that was new to some of us for the languages. We also dealt with issues on authorization and local hosting, as well as proper Github syncing.

Accomplishments that we're proud of

What we learned

We learned to start working with Python, HTML, and how to utilize API to our advantage, especially with Spotify's OAuth2 protocol. This also allowed us to experiment with Microsoft VS and Github for simultaneous work.

What's next for campfireify

Not very sure; Tung would like to work with the front just to see how far we can take the appearance and design of it all, since it looks very basic. Also, working with Python was new but also very confusing, and I think being able to implement more functionality and use even more of Spotify API would be good in order to add depth and complexity to our project, since it is rather basic right now.
","",,"","python, html, css, javascript",Tung,Tran,ttran19@terpmail.umd.edu,,https://github.com/tungqtran/campfireify,Best UI/UX Hack,Best First-Time Hack,"","","","Switching from the eduroam wifi to the bitcamp wifi was personally much slower for my laptop, and at times, had to deal with connection issues that were short-lived. My teammates experienced a lot of issues with GitHub pushing and pulling.",University of Maryland College Park,OpenAI,N/A,N/A,"",University of Maryland - College Park,1,Nazia,Rangwala-Vohra,nazia.rangwala@gmail.com
Moodify,https://bitcamp2025.devpost.com/submissions/642889-moodify,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 02:13:31,"Inspiration

We were inspired when listening to music from the hackathon and thought of being able to listen to similar music

What it does

It takes a given song as an mp3 file and predicts its mood, then provides recommendations to similar songs based on the mood

How we built it

We used python to generate a model using training data from MTG Jamendo's dataset. Our front end uses html/css.

Challenges we ran into

The most challenging part of this project was creating the AI model as none of us had extensive experience with AI. Also used Flask for the first time and so had a challenging time integrating it. 

Accomplishments that we're proud of

We are proud of being able to create an AI model for the first time.

What we learned

We learned how to create an AI model and work with data sets. USed Flask to integrate the backend with the frontend. 

What's next for Moodify

We will continue to improve our AI model to provide better predictions and recommendations. We also hope to be able to integrate with Spotify, Apple Music, or another music app
","",,"","python, flask, javascript, html, css, machine-learning, scikit-learn",Mihir,Kulkarni,mihirk5rocks@gmail.com,,https://github.com/skottchen/mood_classifer/tree/main,Best First-Time Hack,"","","","","We interacted with Python, Flask, HTML, CSS and JavaScript and machine learning(Scikitlearn). Also GitHub",University of Maryland,OpenAI,No,NA,"",University of Maryland - College Park,2,Scott,Chen,schen78997@gmail.com,xiaofan,li,xiaofan0517@gmail.com
Therassist,"",Draft,Pending,Project details,04/13/2025 02:23:57,"🌟 Inspiration
*** THIS IS FROM MY OLD PROJECT PLEASE CHANGE ** 
There are moments when calling for help isn’t an option. Sometimes a wrong move could make things worse. Whether it’s feeling uneasy in a rideshare, being followed on the street, or navigating an abusive situation, victims often need a way out that doesn’t put them in more danger. Having quick and easy access to calling services can be a lifeline, offering both reassurance and immediate help when it’s needed most. We wanted to create a solution that feels natural, provides reassurance, and most importantly, helps people safely remove themselves from harm. SafeCall is designed to give users a believable excuse to leave, a discreet way to seek help, and a sense of control in situations where options feel limited.

🛠️ What it does
SafeCall is an AI-powered safety tool designed to assist users in high-risk situations. It connects them to a real-time voice AI that simulates a natural conversation. Unlike static prerecorded messages, SafeCall adapts dynamically, allowing for fluid, lifelike interactions that can continue as long as needed. In addition, SafeCall securely logs call details, including time, severity level, and last known location and puts it all onto a dashboard for emergency response analysis.

🧱 How we built it
The app is separated into two parts: the frontend and backend.

The frontend is built with React.js, Next.js, and Tailwind CSS, and is deployed on Vercel for scalability and performance. It leverages React components to construct an intuitive dashboard, displaying call logs in a structured format. Users can easily initiate and schedule calls, as well as review their previous interactions through the different pages.

The backend is made with FastAPI, designed for high-speed performance and seamless API communication. We use Twilio to handle call routing, enabling real-time voice interactions with AI-generated responses. OpenAI's real-time API powers the dynamic conversation, ensuring natural and adaptive speech that makes the call feel realistic.

⚔️ Challenges we ran into
Combining Twilio and the OpenAI real-time voice API to facilitate calling.
Integrating the backend real-time call data with the frontend dashboard.
Building the prompts for seamless natural conversation.
🎖️Our Accomplishments
We successfully integrated the frontend with the backend, and we are incredibly happy that we are able to call and utilize the realtimeAPI and get a responding voice AI with minimal latency, which is crucial for this app. We are also proud of successfully managing and navigating our database, which took some time to work with.
",https://github.com/gzhu725/BitCamp2025,,"","next.js, javascript, html5, tailwind, react, google-cloud, ocr, mongodb",Gloria,Zhu,gloriazhu66@gmail.com,,"","","","","","","","","","","","",New York University,0
Corporate Chaos,https://bitcamp2025.devpost.com/submissions/642901-corporate-chaos,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 02:24:09,"Inspiration

We were inspired to take the subject of finance as the focus of our project, and we were also seeking to create something fun. This inspiration ultimately led to us deciding to make a game focused on real estate and financial decisions, incorporating financial literacy and intuition as key skills required from the player to be successful

What it does

It is a financial management and literacy game. The player is trying to reach a monetary goal within a set in-game time frame while trying to prevent bankruptcy and keep monthly income above a certain threshold. The player must engage in a variety of actions such as buying properties, selling properties, and interacting with the in-game bank on their way to reaching their financial finish line.

How we built it

The entire game was built on a platform called Unity, for game development. The entirety of the code was written in the C# programming language.

Challenges we ran into

Numerous bugs and compilation errors were part of the entire process. Designing the dynamic financial system and interactions of the game also took a lot of planning and thought before we could even begin coding. We also hit some bumps in the road with feature creep, and had to make sure we could get a product we were proud of out there and not let wandering ideas that we wanted to implement stop that from happening.

Accomplishments that we're proud of

We were all proud of getting to a working final game product, especially considering 3 of us hadn't even touched Unity before. We were also particularly proud of being able to mix a bit of silliness, like incorporating ""Capital Won"" as the main bank, into a game that you can still use as a bit of a mind teaser with financial math. 

What we learned

Three of the members of the team were brand new to C# and game development, so they learned new and crucial skills in that regard. We also all got a lot more experience with the depths of version control like GitHub on a project like this. Additionally, we learned a valuable lesson of not letting our wild dreams halt us from getting a fun final product, and choosing the features that we could implement in time.

What's next for Corporate Chaos

One of our original intentions with the game was to have a sort of 4 person multiplayer experience where you could sabotage other players and get into bidding wars against them. We definitely see that as a fun future we could explore for the project.
",https://github.com/abmanos/CorporateChaos,https://www.youtube.com/watch?v=-FNjjbBGO5E,Best Financial Hack - Capital One,"unity, c#",Austin,Manos,austinbmanos@gmail.com,,https://github.com/abmanos/CorporateChaos,Best First-Time Hack,Best Razzle Dazzle Hack,Best Moonshot Hack,Hack That Made You Smile,Prettiest Hack,"We primarily used Unity in 2D and GitHub Desktop, both of which integrated with each other quite well, though there were certainly some obstacles along the way that we had to work through.",University of Maryland - College Park,I did not use GenAI,N/A,N/A,"","University of Maryland - College Park, UMD",2,Mazinkhan,Nadaf,mnadaf@terpmail.umd.edu,Duy,Bui,buithaiduy2006@gmail.com
Roameo,"",Draft,Pending,Additional info,04/13/2025 02:25:23,"","https://roameo.tech/, https://github.com/Vitthal-Agarwal/travel",https://www.youtube.com/watch?v=dQw4w9WgXcQ,"Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best AI Application Built with Cloudflare, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","openai, n8n, browser-use, gemini, mongodb, react, tailwind, google-flights, google-hotels, google-maps, google-map-tiles, python, flask, elevenlabs",Diyan,Chokshi,diyanc@terpmail.umd.edu,,https://github.com/Vitthal-Agarwal/travel,Best Machine Learning Track Hack,Best UI/UX Hack,Best Razzle Dazzle Hack,Prettiest Hack,Hack That Made You Smile,"n8n:
I explored n8n, a LangChain-based workflow automation platform. It was genuinely impressive how it uniquely blends AI capabilities with business process automation. It gave our project the flexibility of custom coding alongside the speed and convenience typically associated with no-code tools. Connecting different AI agents and routing their data efficiently to webhooks was intuitive and significantly accelerated our workflow setup. One area for improvement could be clearer documentation on complex integrations, but overall, it's a powerful platform for any AI-driven automation.

Browser Use:
The Browser Use technology that incorporates AI-driven browser automation was incredibly useful, especially for tasks such as automated flight and hotel bookings. Its ability to intelligently break down complex website elements into easy-to-use formats for the AI agents allowed for seamless navigation and interaction. The AI-based browser controls were precise and efficient. A possible improvement would be increasing the speed of interactions during heavy scripting scenarios to reduce latency, but otherwise, it worked exceptionally well for automating intricate web tasks.

ElevenLabs:
Using ElevenLabs for speech synthesis was a standout experience. The natural-sounding speech, created using deep learning, significantly enhanced user interaction by providing a lifelike AI voice guiding through travel planning processes. The generated voice was smooth, clear, and engaging, which greatly improved user comfort and accessibility. Future improvements could include greater variety in speech styles and emotions, but overall, this technology provided impressive realism and ease of use.

Google APIs:
Integrating Google APIs proved essential in our application, particularly Google Flights and Hotels for bookings, Google Maps for finding restaurants, cafes, and activities, and the Google Maps Tiles API to generate an interactive 3D globe visualization. These APIs performed reliably and provided comprehensive, accurate data that enriched user experiences significantly. The integration was straightforward with robust documentation available. Enhancements could include simpler billing options or extended free tiers for hackathons, but functionally, these APIs were solid, reliable, and feature-rich.","University of Maryland, College Park",OpenAI and Gemini,"The OpenAI ChatGPT model was integral to our entire application, serving as the core intelligence behind multiple features. It was responsible for receiving and responding dynamically to chatbot messages, providing users with natural, intuitive interactions. Beyond simple messaging, ChatGPT also acted as the backbone of our voice agent, generating conversational content that was passed directly to the ElevenLabs AI, enabling lifelike spoken interactions. This created a smooth, seamless dialogue experience, guiding users effortlessly through complex tasks like travel planning. Additionally, ChatGPT effectively collected, parsed, and managed data obtained from text and voice inputs. It intelligently filled and edited content directly within booking input fields, significantly simplifying complex processes like flight and hotel reservations. Moreover, OpenAI was essential for generating clear, detailed prompts that instructed our Browser Use agent, enabling precise navigation and automation of websites. Finally, it provided concise summaries that we automatically emailed to users, ensuring they received clear, accurate, and timely information about their bookings and planned itineraries.

Google's Gemini was employed for specialized training and fine-tuning of our Browser Use agent. Gemini’s advanced contextual understanding and powerful generative capabilities allowed us to teach our browser automation agent sophisticated navigation behaviors. This training significantly boosted the agent's precision, enabling it to efficiently handle intricate website interactions such as selecting flights, hotels, and managing other travel-related services.",AIzaSyBMPeEWiZgZzvWPD8k0wsyJPQUyjt-KkGM,roameo.tech,University of Maryland - College Park,2,Vitthal,Agarwal,vagarwa4@terpmail.umd.edu,Rohan,Chintakindi,rchintak@terpmail.umd.edu
BarNone,https://bitcamp2025.devpost.com/submissions/642903-barnone,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 02:26:02,"💡 Inspiration

We’ve all experienced the frustration of dropped calls and buffering videos in ""dead zones."" BarNone was born to turn this universal pain point into actionable data—helping users find reliable connectivity in real-time. But beyond convenience, we saw a deeper impact. What if this data could help non-profits bring services to the people who need them most? With BarNone, we crowdsource signal strength to map where connection fails—so organizations can identify where to set up mobile clinics, distribute aid, or support remote learning.

It’s not just about signal bars—it’s about bridging digital divides and empowering communities through connection.

⚙️ What it does

BarNone tracks and visualizes cellular/WiFi strength as you move, creating live color-coded indicators of signal quality. Crowdsourced data helps users avoid dead zones and find optimal connection spots. Users essentially contribute data as they move, building a reliability network.

🛠️ How we built it

Frontend: React Native + Expo for cross-platform performance
Backend: MongoDB for crowdsourced data storage
Mapping: Dynamic circle scaling using signal strength metrics
Live tracking: GPS-integrated movement updates

💪 Challenges we ran into

This was our first hackathon ever, so we were honestly overwhelmed at first. The concern of the time constraints mixed with some rushed planning and lack of experience led to a rather lackluster start to the project, but we quickly turned that around. Our key was that we didn't take small victories for granted, which boosted team morale and motivated us to push through a variety of difficulties.

🏆 Accomplishments that we're proud of

We were proud to have developed circle overlaps in dense areas, created a dynamic map view using react native libraries, and connecting our database to our map to show user data points.

💭 What we learned

Do not underestimate dependencies and version control...

⏭️ What's next for BarNone?

We aim to configure ""best path"" suggestions to avoid poor-signal zones, carrier comparisons that show which networks perform best locally, and an offline mode to cache maps for areas with spotty bandwidth.
",https://github.com/nitinenj/signalscape,,"Best Web Hack Using React - Peraton, [MLH] Best Use of MongoDB Atlas","mongodb, typescript, javascript, react-native, html/css, expo.io, node.js",Shaunak,Ghosh,shaunak.ghosh2016@gmail.com,,https://github.com/nitinenj/signalscape,Best First-Time Hack,Best Social Good Hack,Best UI/UX Hack,Hack That Made You Smile,Prettiest Hack,"MongoDB, Expo Go, React Native",University of Maryland - College Park,Other,Speed Test API,N/A,"",University of Maryland - College Park,3,Devansh,Singh,devanshs7905@gmail.com,Nitin,Enjamuri,nitin.enjamuri@gmail.com,Nethan,Binu,nethan.binu@gmail.com
API - A Prehistoric Island,"",Draft,Pending,Additional info,04/13/2025 02:33:37,"🦕 Inspiration

College life can feel like being dropped into a mysterious prehistoric jungle - full of chaos, hunger, unknown rules, hidden chances, and the occasional Testu-Rex. We wanted to build a mysterious amazing website that offers games and tools to make campus life easier and a tad more prehistoric for our fellow students.

So we asked ourselves:

""What if there was a prehistoric island - API (you see it right?) - where students could keep track of their healthy achievements, be protected knowing essential rules, and spark inspiration from complex, lengthy materials - but at the same time feeling like they are taking part in an epic quest?""

That (and a LOT of caffein) is how API: A Prehistoric Island was born. A world that’s easy for even cavemen, built by future tech wizards.

🧭 What it does

API, apart from you-know-what, also stands for Achievement, Protection, and Inspiration—the three pillars behind our 3 star features:

Quest for Campus Embers

A gamified campus quest where students turns into Hunters, Gatherers and Trackers, earn fiery streaks and top the Lava Leaderboard by checking in regularly with healthy habits like exercises and nutritious diet. 

DinoLaws

Drop the location, and it gives you a simple, funny explanation of must-know laws and campus rules tailored to the area, should it be UMD, Hogwarts or even your backyard! So simple even a caveman could understand. Read by the caveman himself.

Ancient Scrolls

Upload your study notes or docs, and it’ll summarize them and even suggest creative project ideas. Students will leave chasing opportunities and with a voiceover guiding them on.

🛠️ How we built it

Frontend: HTML/CSS/JavaScript/Python with prehistoric-themed visuals and animations

Backend: Python with FastAPI, deployed on an AWS EC2 (Amazon Linux) instance

AI Magic: Powered by Gemini 1.5 Pro for ultra-fast and smart responses

Voice + Animation: Synced Terp’s caveman narration with scroll animations and sound FX

Version Control: GitHub + pure survival instincts

It’s all lively, and purely Jurassic

🪨 Challenges we ran into

Rate Limits: Gemini got too close to daily quotas (just like us) when too many requests were called. We built smart testing strategies and alternating between models to utilize daily requests.

Lack of Experience: One of the team members has zero experience with web/backend/frontend/full stack development, yet through sheer delulu, the hackathon resources and the kind help of their teammates, pulled of the DinoLaws feature!

Voice Syncing: Making the caveman’s voice match the scroll carving animation took a lot of fine-tuning (and some mammoth-level patience).

Theme Cohesion: Balancing humor with actual usability took several iterations. We had to make sure it wasn’t just all jokes—it had to bring meaningful benefits and smiles to our users' faces too.

Thinking Back: We were too focus on moving forward that we forgot to include the Back (to main page) buttons in our features. A valuable lesson that will never grow old.

🏆 Accomplishments that we're proud of

Fully deployed full-stack mini functions that combines to a single unified product with user benefits close to our hearts 

Was not afraid to shot for the moon and in this case, at meteors too!

Made project brainstorming fun, walks in the rain productive, minds strong and friendships happen

Developed a consistent theme with an easy to navigate, playful UI/UX that goes hand in hand with the hackathon's atheistic

Learned new tools and APIs handling mid-hackathon and still finished strongly and proudly

Actually made other hackers smiled during testing 👀

📚 What we learned

How to quickly create and deploy full-stack mini services to AWS

Leverage and have fun with Google Gemini APIs

How to write good prompts for that mix clarity, humor, and usefulness

That great UI/UX isn’t just about art or design - it’s about sympathizing with our users

A little humor goes a long way when solving real life pain points

🗺️ What's next for API – A Prehistoric Island

We’re just getting started. Here’s what’s ahead:

Add more tribes and roles so students can customize their campus journey

Expand DinoLaws to cover more regions and tailor to university-specific rules

Let users share achievements and compare scrolls with friends on social channels

Build a full mobile version so you can carry the island in your pocket!

Integrate calendar syncing, class data, and club events
",http://apeye.tech:5173/,https://youtube.com/shorts/VBweLjRH5eo?feature=share,"Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best Use of Gemini API","react, node.js, amazon-web-services, python, google-cloud, html, javascript, typescript, zustand, tailwind, css, shadcn, radix, lucide, sonner, 3.js, vite, gemini1.5, gemini1.5pro, .tech",Hai,Le,lehai8595@gmail.com,,https://github.com/shreya222mishra/bitcamp-MLH,Best UI/UX Hack,Best Moonshot Hack,Best Bitcamp Hack,Hack That Made You Smile,Prettiest Hack,"We were all first-time hackers, and diving into so many technologies in one weekend was both wild and incredibly rewarding!

We followed a microservices pattern to build our project, using multiple APIs and frameworks that worked together like a well-oiled (prehistoric) machine. Our backend services were powered by Gemini 1.5 Pro and Gemini 2.0, which we integrated using Node.js and modern Python frameworks like FastAPI. For the frontend, we used React along with Three.js to create an immersive, scroll-carving, cave-like user interface.

We deployed everything using Amazon EC2, where we hosted our microservices and connected them into a seamless experience. GitHub helped us manage our codebase and collaborate effectively as a remote team.

We also used the .tech domain to bring our product to life on the web—because nothing says ""cutting-edge prehistoric island"" like a clean, custom domain!

Using these technologies was not only fun, but also a huge learning experience. From prompt engineering with Gemini to syncing animations with backend APIs, every piece taught us something new. We'd love to keep exploring and building with them beyond BitCamp!",University of Maryland - College Park,"OpenAI, Gemini, and DeepSeek","Yes, we implemented multiple generative AI models and APIs in our project!

We used Google Gemini 1.5 Pro and Gemini 2.0 to power two major features:

PDF summarization and idea generation through our Ancient Scrolls feature

Local laws and campus rule explanations through our DinoLaws tool, making legal info digestible and even fun

We used Terp AI to help us generate content for our scripts and reports, adding a unique and consistent voice to our project.

During development, we leaned on OpenAI models (like ChatGPT) and GitHub Copilot to learn new technologies on the fly, troubleshoot bugs, and brainstorm solutions.

Generative AI wasn't just at the heart of our features—it was a key part of how we built, learned, and iterated throughout the entire hackathon.","781472095496, 222673555444",http://apeye.tech:5173/,University of Maryland - College Park,3,Sumiran,Jaiswal,sumi0309@umd.edu,Aditi,,aditii@terpmail.umd.edu,Shreya,Mishra,shreya222mishra@gmail.com
MetaVAULT,https://bitcamp2025.devpost.com/submissions/642910-metavault,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 02:37:11,"Inspiration

Being from California, we witnessed first hand the destruction natural disasters can have on families. We noted that Home Insurance providers were doing everything they could to prevent insuring all the lost items. Thus, we wanted to create a tool to help those in dire circumstance, easily keep track of all their items.

What it does

MetaVault takes an image provided by the user and is able to display all notable items within the image. It then suggests prices for each item, allowing the user to change the price before ultimately logging the items and prices to the inventory. Users have accounts and can keep track of all their assets through simply logging in and out of accounts.

How we built it

We built it using Gemini AI as a our computer vision. We also used Gemini AI's accurate model to count the number of items, as well as calculate the prices for each item. Using Tailwind, CSS, and Next.JS, we were able to create a UI to allow Users to upload and access their assets. We used Mongo DB to store User's data and login information to ensure that data is accessible from anywhere!

Challenges we ran into

Due to being beginners, we went into this challenge pretty blind. We had issues with using the right technologies as we started with outdated methods such as Open CV. We then decided to switch to Gemini, which was our first time using an API. We then ran into some challenges we ran into were issues with Gemini hallucinating. This was also our first time creating a full stack web app and we struggled with databases in the backend.

Accomplishments that we're proud of

We are all super happy with what we accomplished and we know there is so much potential to this project. Creating this project for our first hackathon was taxing, but it was so fun and we learned so much along the way! We see ourselves using our own app and we are all satisfied with the outcome.

What we learned

We learned almost everything for this project as most of us really only had front-end experience. It was so cool witnessing a functioning sign in function, as well as utilizing advanced tools such as Gemini AI. What we took away the most was how to work together as a team and building off each others strengths and weaknesses!

What's next for MetaVAULT

We see a lot coming with this project. We hope to get users for this app and hopefully HomeInsurance partners to endorse this product as a trustworthy way to keep track of assets. We want to add better UI which we have already designed, a dashboard to keep track of total value of all assets, and live market price comparisons to get the most accurate prices for your products.
","https://assetmanager-eta.vercel.app/, https://www.canva.com/design/DAGkdvlhNPI/BVxJJieVASG_TG4wP1R86A/edit?utm_content=DAGkdvlhNPI&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton",https://youtu.be/k98gfARxcLU,"Best Financial Hack - Capital One, [MLH] Best Use of Gemini API","next.js, gemini, mongodb, tailwind, python, javascript, css",Alan,Deng,alandeng@terpmail.umd.edu,,https://github.com/zSt4rz/assetmanager,Best First-Time Hack,Best Social Good Hack,Best UI/UX Hack,Prettiest Hack,MOST LIT HACK,"We used GitHub Desktop a lot. It worked pretty well. We also used MongoDB, which was really smooth. Finally, using Gemini API was extremely cool and we really enjoyed using such a ground breaking technology.",University of Maryland College Park,Gemini,"Yes, we used it to recognize assets within our image and also create a reasonable price for each item.",593046828122,"",University of Maryland - College Park,2,Alex,Slokvenko,aslokvenko@gmail.com,Patrick,Zhang,patrick8751@gmail.com
Clear Jobs,https://bitcamp2025.devpost.com/submissions/642911-clear-jobs,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 02:37:18,"Inspiration

As students, we strive to help our community find a place to work ranging from part time during university to professions post graduation.

What it does

Our application takes in a resume, job title, and location and searches through a public API to find online job postings based on nearby locations. There, it returns median salary earnings and the link to apply for the job.

How we built it

We built our front end completely in TypeScript, and our data base in Python. For our database, we used Py Mongo, and we also used Flask for our endpoints to link the front end and back end. We also used multiple APIs to get job postings from the internet and access OpenAI for our resume parser. Lastly, we finished the UI with Figma.

Challenges we ran into

The deployment of the website was a big challenge that took a lot of debugging.

Accomplishments that we're proud of

Using OpenAi's APR, we were able to make an extremely accurate resume parser that saves the user the time of filling out various forms. This provides an advantage over other competitors.

What we learned

We learned about Mongo Database and how to effectively incorporate it in Python and link it into our front-end in TypeScript. We also learned about OpenAI's API and how to properly parse pdfs into script using AI.

What's next for Clear Jobs

More Accurate Information on Location for Job Postings, Hopefully getting Recognized.
",https://github.com/VirTrivedi/Bitcamp25.git,,Best Financial Hack - Capital One,"typescript, python, mongodb, figma, api, openai, react, flask, tailwind, css",Sid,Karthik,skarthik1259@gmail.com,,https://github.com/VirTrivedi/Bitcamp25,Best Social Good Hack,Best First-Time Hack,Best Hardware Hack,MOST LIT HACK,Hack That Made You Smile,"One technology we used was MongoDB where we incorporated it in Python and linked it into our front-end in TypeScript with Flask. Though we had little experience in using MongoDB, we were able to eventually understand and use it.",University of Maryland College Park,OpenAI,We implemented OpenAi's APR to make an extremely accurate resume parser that saves the user the time of filling out various forms. We used an API to provide us with the list of job postings.,n/a,"",University of Maryland - College Park,3,Ian,Wang,ianjwang16@gmail.com,Vir,Trivedi,vtrived1@terpmail.umd.edu,Alexander,Pent,pentalex1@gmail.com
CyberSafe HUB,https://bitcamp2025.devpost.com/submissions/642918-cybersafe-hub,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 02:43:38,"Inspiration

The inspiration for SafeHub came from the growing need to make cybersecurity accessible to everyone. People often underestimate the dangers lurking online until it's too late. We wanted to create a tool that not only educates users but also empowers them to take control of their digital safety. By providing practical tools for analyzing network security and website links, we aim to foster a cybersecurity mindset in everyday life.

What it does

SafeHub is a user-friendly web platform offering tools to enhance personal cybersecurity. It provides:  


Website Link Verification: Analyzes URLs for potential threats, such as phishing or malware.
WiFi Security Analysis: Identifies vulnerabilities in nearby WiFi networks, helping users make safer choices.
Download Verification: Validates downloaded files to prevent malicious software.


SafeHub brings cybersecurity awareness to users before they encounter threats.

How we built it

We built SafeHub using a Django backend to handle the core functionality, leveraging Python scripts for security checks and analysis. The front-end uses Django templates for a clean, intuitive UI, ensuring ease of use for non-technical users. We also integrated libraries and APIs for URL verification and network scanning. The development environment was set up to streamline collaboration and iteration.

Challenges we ran into


CSRF Cookie Issues: Implementing secure forms while maintaining functionality was a significant challenge.
Integrating Python Scripts: Embedding security analysis scripts into the Django framework required debugging and testing to ensure smooth integration.
Balancing Scope and Time: Deciding which features to include in the MVP while ensuring a functional and polished product was a tough call.


Accomplishments that we're proud of


Successfully implemented core security features in a user-friendly format.
Built a functional prototype within a limited time frame.
Addressed technical challenges like script integration and secure input handling.
Made cybersecurity principles accessible to everyday users.


What we learned

This project reinforced the importance of:  


Designing with the user in mind, especially for non-technical audiences.
Managing time and resources effectively during development.
Debugging and integrating diverse tools into a cohesive system.
Educating ourselves about advanced cybersecurity concepts while making them relatable.


What's next for SafeHub

We plan to:  


Refine and Expand Features: Enhance WiFi security analysis and add real-time threat detection.
Modern Front-End: Transition to a React-based interface for improved interactivity and design.
Mobile Accessibility: Develop a companion app for on-the-go security checks.
Community Engagement: Incorporate educational resources and forums to promote cybersecurity awareness.
Advanced Tools: Add deeper network diagnostics and proactive threat prevention measures.

",https://github.com/NahomWondimu/CyberSafe,https://youtu.be/LlOcRYBGVDk,Best Digital Forensics Related Hack - Cipher Tech Solutions,"python, html, django, streamlit",Nahom,Wondimu,nahomawt@gmail.com,,https://github.com/NahomWondimu/CyberSafe,Best Cybersecurity Track Hack,Best Social Good Hack,Best First-Time Hack,Prettiest Hack,Hack That Made You Smile,"This weekend, we had the opportunity to work with several technologies that played a crucial role in building SafeHub. Here's our feedback:  

### **Django**  
Django proved to be a robust and reliable framework for developing the backend of SafeHub. Its built-in features, like form handling and database management, saved us significant time. However, we ran into some challenges with CSRF cookies, which required additional debugging. Overall, Django was an excellent choice for our project due to its scalability and security features.  

### **Python**  
Python's versatility made it the ideal language for implementing our security analysis scripts. Libraries like `requests` and `scapy` allowed us to handle network and URL analysis efficiently. The integration with Django required some effort but ultimately worked seamlessly.  

### **GitHub**  
GitHub was essential for version control and collaboration. The pull request and issue tracking features kept our workflow organized. While merging changes occasionally caused conflicts, it was a valuable learning experience in collaborative coding.  

### **PostgreSQL**  
We used PostgreSQL for our database needs, and it performed exceptionally well with Django. Its robust query capabilities and seamless integration with the framework made data storage and retrieval smooth.  

### **VS Code**  
VS Code was our go-to IDE throughout the hackathon. The extensions for Python and Django development streamlined coding and debugging. The GitHub integration within VS Code was particularly useful for managing commits and branches directly from the editor.  

### **Learnings**  
While we faced some technical challenges, the experience helped us become more proficient in these tools. We appreciated how each technology contributed to simplifying complex tasks, and we are eager to build on this knowledge for future projects.  

### **Suggestions**  
It would be great to have pre-configured environments or containers (e.g., Docker) readily available for hackathon participants. This would save time on setup and allow teams to focus more on development.",Clarksburg Highschool,OpenAI and DeepSeek,N/A,N/A,N/A,"",0
TerpNotes,https://bitcamp2025.devpost.com/submissions/642919-terpnotes,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 02:44:15,"⚠ !! BEFORE YOU READ !! ⚠


Please take the time to experience TerpNotes for yourself!
I encourage you to use the link below and browse our catalogue of notes. Product spec is interesting, but demo's speak far louder!

▼▼▼



   
    🔗 Visit TerpNotes.tech
   



   
    ⭐ View on GitHub
   


Inspiration

Imagine cramming for an exam last minute, then realizing you missed a few days of school and did not have the notes to catch up and its too late to ask for anyone for their notes. Well, we have that for you! Students can upload their notes to our platform to make their notes free for everyone! We also provide a free to use AI generated summary tool for the top 5 best voted notes for a class!

What it does

Our website can host notes by current or previous students who attended UMD for a specific class and professor. The website will compile that together and will pick the top 5 best community voted notes and generate a summary notes compiled by AI.

How we built it

For the frontend, we used:


React
Tailwind
NextJS
HTML, Typescript, CSS


For the backend:


MongoAtlas
Firebase
Python
Gemini API


Hosting:


Domain name: NameCheap
Server: AWS EC2


Challenges we ran into

Learning backend development and API endpoints. Deployment onto AWS, authentication, and etc.

Accomplishments that we're proud of

Complete front-end UI + backend system. Our whole project in general.

What we learned

Everything with the technologies we used

What's next for TerpNotes

Everything! Full deployment for this app is coming soon after touching up some deployment issues.



Team Credits

Built by


Habib
Olaniyi
Jacob
Kevin
—and our dear companion: ChatGPT...




🌐 Connect With Us






  Made with 🧠 by students, for students.

",https://terpnotes.tech/,,"Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best AI Application Built with Cloudflare, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","react, amazon-web-services, mongodb, firebase, next, google-gemini, typescript, python",Jacob,Lin,jlin2809@umd.edu,,https://github.com/MajorH5/TerpNotes,Best Bitcamp Hack,Best Social Good Hack,Best UI/UX Hack,Prettiest Hack,Hack That Made You Smile,We had a tough experience with GitHub in setting up CI/CD and we also have issues with AWS EC2 instances.,"University of Maryland, College Park",Gemini,"Yes, we used gemini API for an OCR which converts someones notes of an image to a markdown file we can display",919579564302,terpnotes.tech,"University of Maryland - College Park, University of Maryland - Baltimore County, Towson University",3,Habib,Aina,haina2@umbc.edu,Olaniyi,Salami,salaminiyi@yahoo.com,Kevin,Yomba,kevint.yomba@gmail.com
ShadowScan,https://bitcamp2025.devpost.com/submissions/642921-shadowscan,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 02:48:07,"Inspiration

In sensitive environments such as defense systems, a lot of safeguards need to be in place in order to ensure data is handled properly and doesn't fall into the wrong hands. This, however, makes development tedious and slow-paced at times. Specifically, a high-to-low transfer, or moving data from classified environments to unclassified environments is especially scrutinized and tedious. Oftentimes, developers have to copy code and other pieces of information by hand to ensure no sensitive data is leaked. ShadowScan aims to remediate this process by using a ML similarity detection and pattern matching algorithms as well as a robust protocol to make high-to-low transfers more effective. It is a modular, containerized app that can be hosted on production-grade high-side environments.

What it does

ShadowScan works by carefully handling the flow of sensitive data and leveraging ML models while still putting the final decision in the hands of a human. Here's how it works.


Users upload files they want released
Immediately, ShadowScan's similarity detection engine compares the input file with a corpus of sensitive data and keywords to derive a similarity score. It sends the file and this score to an admin user.
The admin user utilizes this score and their own intuition to decide whether or not the file should be released to the low-side. 


How we built it

ShadowScan's Proof-of-Concept is built with React, FastAPI, and SQLlite. All the components are containerized via Docker and hosted on Render. These choices aren't the most effective or efficient, but they aim to maximize speed of development. React is a quick way to build UIs, and using a Python based backend service such as FastAPI makes ML integration seamless. In production, however, I would opt to use a server based database such as Postgres rather than SQLlite and opt to use Go for much of API routing and dataflow for its performance benefits and create a separate Python engine for processing. 

Challenges we ran into

Formulating a secure way for high-low-transfer to occur was a big challenge. Other than that, many challenges came in the implementation. We initially thought using LLM like a self-hosted instance of llama could easily detect similarity, so we dedicated resources towards that. However, that wasn't the case, and developing a semantic matching tool from scratching that utilized more basic techniques like word encoding and cosine similarity proved to me more fruitful. On the infrastructure side, our initial goal was develop a fully-managed network layer to sit on top of ShadowScan to control the quarantine and release of files. However, time constraints forced us to build an MVP via Render.

Accomplishments that we're proud of

Creating a fully functional prototype of something that was just an idea a day ago.

What we learned


How to detect similarity using word encodings and cosine similarity 
Setting up full-stack infrastructure with Docker
and much more!


What's next for ShadowScan


More specific similarity indicators rather than one score
Network layer to support secure file transfer

","https://bit-camp-2025-1.onrender.com/, https://bit-camp-2025onrender.com/, https://github.com/prajwalshah19/bit-camp-2025/tree/integrated-routes",,Best Digital Forensics Related Hack - Cipher Tech Solutions,"fastapi, react, sqlite, sqlalchemy, typescript, python, docker, render",prajwalshah19,Shah,pjpackers99@gmail.com,,https://github.com/prajwalshah19/bit-camp-2025/tree/integrated-routes,Best Cybersecurity Track Hack,Best First-Time Hack,Best Social Good Hack,MOST LIT HACK,Hack That Made You Smile,"It was great, we didn't use any of the sponsored technologies this weekend because of the nature of our project was that it aims to solve problems in air-gapped (no internet access) systems for government agencies and defense contractors, who often have old codebases and can't support newer technologies.",Purdue University,OpenAI,"No. We attempted to and you will see the remnants of this in our code, but we eventually decided to go with a more traditional ML model",N/A,"","Purdue University, North Carolina State University, SUNY College at Oswego",2,fategg555,Kannan,achyuta.kannan@gmail.com,Mio,Mahoney,mmahone5@oswego.edu
BitBoxer,https://bitcamp2025.devpost.com/submissions/642924-bitboxer,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 02:50:33,"BitBoxer is an interactive game single player punch one another until one person reaches the desired score first! 

BitBoxer utilizes React alongside tools such as OpenCV and MediaPipe to provide a fun, interactive user experience where they can play a game of punchies with a friend. The user's laptop webcam is used to motion track the their hands which allows them to deliver punches on the opposing side. 

How we built it

We leveraged Three.js to create the 3D environment along with MediaPipe for motion tracking via web camera.

Challenges we ran into

The biggest challenged we faced was making a fluid display of the gloves to recognize punches. This led to other problems that came up such as positioning, calibration, and scaling of the different components.

Accomplishments that we're proud of

We are proud of having a deployed website for people to use! This was more of a fun hack to build and definitely made it a more enjoyable hackathon experience. 

What we learned

Blending Three.js with MediaPipe and building literacy on using webhooks!

What's next for GoSustainYourself

We would like to make the whole experience multiplayer with being able to go up against different people to have a more fun and playable experience.
","",https://youtu.be/Usc5UWzU__I,"Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech","javascript, html, css, opencv, mediapipe, three.js",Brian,Nguyen,bnguye60@gmu.edu,,https://github.com/J-Souffle/Bitcamp-2025,Best Moonshot Hack,Best Gamification Hack,Best Bitcamp Hack,Hack That Made You Smile,MOST LIT HACK,Overall satisfied with all the tech!,George Mason University,OpenAI,N/A,N/A,BitBoxer.Tech,George Mason University,2,Shayer Irtiza,Khan,shayer687@gmail.com,Joshua,Cancio,jd.cancio1@gmail.com
Cook Mate,"",Draft,Pending,Project details,04/13/2025 02:54:47,"CookMate: Bringing Joy to Shared Cooking Experiences

Inspiration

The idea for CookMate was born from a simple yet profound realization our team shared during the pandemic: cooking alone isn't nearly as enjoyable as cooking with others. As a group of friends who grew up in households where the kitchen was the heart of our homes, we missed the spontaneous cooking sessions with family and friends—the laughter, the shared discoveries, and the inevitable taste tests that happened along the way.

We noticed three problems that needed solving:


The ""What's for dinner?"" dilemma — Standing in front of an open refrigerator wondering what to make with random ingredients
Food waste — Letting ingredients spoil because we couldn't think of creative ways to use them
The social disconnect — Missing the joy of cooking alongside others


What if technology could help us recreate that communal cooking experience, even when physically apart? What if AI could eliminate the frustration of meal planning and reduce food waste? These questions drove our team of four to create CookMate.

What it does

CookMate is an AI-powered social cooking platform that transforms how we approach meal preparation in three key ways:

Intelligent Recipe Recommendations

At the core of CookMate is our advanced AI recommendation engine. Unlike traditional recipe apps that require you to search for specific dishes, our system works backward:


Pantry-First Approach: Simply input what ingredients you have available, and our AI suggests recipes you can make right now.
Preference Learning: The system learns your taste preferences, dietary restrictions, and cooking skill level over time.
Waste Reduction: Receive suggestions for using ingredients nearing their expiration date.


Recipe Exploration and Sharing

CookMate isn't just about recommendations—it's a vibrant community of food enthusiasts:


Community Recipes: Discover dishes created and shared by other users.
Cultural Exchange: Explore authentic recipes from diverse culinary traditions around the world.
Personal Cookbook: Save, customize, and share your favorite recipes in your digital cookbook.


How we built it

Our four-person team divided responsibilities based on our individual strengths:


Frontend Development: One team member focused on creating an intuitive Flutter interface with responsive design
Backend & AI: Two team members specialized in the recommendation engine and TerpAI integration
UX Research & Design: One member led user testing and interface design


CookMate is built using Flutter for cross-platform functionality, ensuring a seamless experience across iOS, Android, and web platforms. The app's architecture includes:


Frontend: Flutter with Riverpod for state management and responsive UI design
Backend: Firebase for authentication, database, and storage of user profiles and recipes
AI Component: Custom-trained recommendation model using TensorFlow
Video Integration: WebRTC implementation for low-latency video communication
Analytics: Firebase Analytics to improve recommendation accuracy


Our recommendation engine combines collaborative filtering (based on what similar users enjoy), content-based filtering (analyzing recipe attributes), and constraint satisfaction (matching available ingredients) to provide personalized suggestions that are both practical and delightful.

The user interface was designed with simplicity in mind, featuring intuitive navigation, clear recipe instructions with step-by-step photos, and seamless video integration for social cooking sessions.

Challenges we ran into

Building CookMate presented several significant challenges:


Real-time Synchronization: Ensuring that cook-along sessions remain synchronized despite varying internet connections required innovative buffering solutions and precise timing mechanisms.
User Interface for Messy Situations: Designing an interface that remains usable with messy hands in busy kitchens required numerous iterations and real-world testing.
Cross-Cultural Recipe Standardization: Standardizing recipes from different cultures while respecting traditional methods and maintaining authenticity required extensive research and consultation.


Accomplishments that we're proud of

As a team of four diverse creators with complementary skills, we're particularly proud of:


Seamless Social Experience: We've created a truly synchronized cooking experience where friends can cook together across distances with minimal latency.
Ingredient Flexibility: Our system can suggest viable recipe alternatives when users are missing ingredients, with smart substitution recommendations.
Inclusive Design: CookMate is fully accessible, including voice control for hands-free operation while cooking and accommodations for users with various abilities.


What we learned

Developing CookMate taught our team invaluable lessons:


Technology Serves Connection: The most powerful feature isn't the AI—it's seeing a friend's smile when they master a new cooking technique through our platform.
Cooking Is Personal and Cultural: Recipes aren't just instructions; they're cultural artifacts that carry history and meaning. Our platform needed to respect and celebrate this diversity.
Flexibility Is Essential: Home cooking rarely follows precise instructions. Our system needed to accommodate substitutions, variations, and the beautiful chaos of real kitchens.


What's next for Cook Mate

We're just getting started. Our team's roadmap includes:


Ingredient Recognition via Camera: Scan your refrigerator and pantry to automatically inventory ingredients.
Smart Kitchen Integration: Connect with smart appliances to receive precise cooking guidance.
Seasonal and Local Optimization: Recommendations that prioritize in-season and locally available ingredients.
Expanded Food Education: Tutorials on cooking techniques integrated seamlessly into recipes.
Global Cooking Challenges: Community events that bring together cooks from around the world.
Meal Planning and Nutrition: Advanced meal planning tools that consider nutritional balance across days or weeks.
Personalized Culinary Courses: AI-curated skill-building paths based on user interests and current abilities.


CookMate isn't just an app—it's a return to the communal joy of cooking in a digital age. Our team of four is building technology that doesn't replace human connection but enhances it, creating virtual kitchens where memories are made and stories are shared, one recipe at a time.

Join us in revolutionizing not just what we cook, but how we cook with our mates.
","",,"","flutter, azure, gemini, terpai",Oluwadamilola,Ogunbode,oluwadamilolaogunbode@gmail.com,,"",Best App Dev Track Hack,Best Social Good Hack,Best Sustainability Hack,"","","","University of Maryland, College Park",Gemini and Other,"","","",Prince George's Community College,2,alem425,,alexandermorgan364@gmail.com,oraimo,adewumi,adewoledarasimi25@gmail.com
Bye Bye Study Blues,https://bitcamp2025.devpost.com/submissions/642928-bye-bye-study-blues,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 02:58:54,"Inspiration

I'm a computer engineering major, and I knew I'd want to complete a hardware hack.  I use the Pomodoro method while studying, which is 50 min on and 5 min off, and I find that it really helps me.  I wanted to be able to see a pomodoro timer in a fun, cute, tactical sense, and the Bye Bye Study Blues project was born.

What it does

The times are scaled down for easier demo-ability.  As the 50 min (50 seconds) go by, a row of lights goes off every 10 min (10 seconds).  This lets the user know how much time is passing.  Once the study time is up, a green light flashes at the bottom to get the users attention.  Next, break time starts, and a row of lights goes off every 1 min (1 second).  After break, the top row flashes next to get the users attention, and the timer starts once again.

How I built it

The circuit is compiled of essentially six mini-circuits, each powered by their own arduino digital pin and grounded.  Each branch is comprised of a resistor and 8 LEDs in parallel.

Challenges I ran into

It took a while to determine whether to use series or parallel connections.  I also struggled with getting numerous lights to light up.

Accomplishments that we're proud of

I'm happy I got all the strips to light up separately!  It took me a while to figure out, and while it seems simple, I was really happy when each took in the Arduino power input properly and was programmable.

What we learned

I learned that the stuff I learn about actually matters.  I learned that different color lights and types of LEDs take different amounts of current to light up the same amount, and that concepts like current division, parallel and series connections, and voltage management play a big role in circuits.

What's next for Bye Bye Study Blues

Soldering!  Once I get better at soldering, I want to turn this into a more official-looking electronic.  However, I am not good enough at soldering these days to solder 48 LEDs and components.
",https://docs.google.com/document/d/1sa5ldV1VBjFWBAESKFMTm2XhG1MTuvQ3x9r9ZHG1Bjw/edit?usp=sharing,https://vimeo.com/1075031936?share=copy,"",arduino,Amanda,Callaghan,amcallaghan30@gmail.com,,https://github.com/MandyC-314?tab=repositories,Best Hardware Hack,Best First-Time Hack,"",MOST LIT HACK,Hack That Made You Smile,Great!  learned about the wifi too late.  The hardware desk was very helpful.,University of Maryland,I did not use GenAI,nope,nope,none,"",0
Untitled,"",Draft,Pending,Manage team,04/13/2025 03:03:51,"","",,"","",Christopher,Cho,christophercho0916@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,0
tomatocode.xyz,https://bitcamp2025.devpost.com/submissions/642931-tomatocode-xyz,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 03:09:19,"Backround

We love simple and easy to use synchronous learning tools like Pear Deck that allow teachers to seamlessly present content to students and get student responses, however there is no such tool that is tailored towards coding. 

Our project aims to fill in this gap with a simple no login website that allows a teacher to give a presentation with simply google slides and a Google app script. Steps to use:


Install the app script
Open Google slides and mark the slides that have coding problems
Initiate session and have students join through code
When you navigate slides on our UI students will see the same slide, on coding problems they may type solutions using the modern Monaco editor (also used in VS code), and run Python and JavaScript in sandboxed mode. You will see AI Google Gemini powered summaries of students progress towards the goal with color coded names. By clicking on a student you can see their code and output in detail.


How we built it

We used Node.js, express.js, AWS EC2, and raw HTML/JS/CSS.

Accomplishments that we're proud of

Nice UI and code that is almost ready for MVP.

What we learned

Lots of web development concepts in general from domains and hosting to frontend design.

What's next for tomatocode.xyz

We will not stop building this product out! Stay tuned on our github, we plan to make this a fully working product that is used in classrooms, lectures, and even some conferences all over the world. Feel free to reach out if you are interested in working with us.
","https://github.com/AregGevorgyan/tomatocode, https://github.com/AregGevorgyan/tomatocode-frontend",,[MLH] Best Use of Gemini API,"javascript, node.js, css, html, express.js, socket.io, app, google-apps-script",Areg,Gevorgyan,areggevorgyan5@gmail.com,,https://github.com/AregGevorgyan/tomatocode,Best First-Time Hack,Best UI/UX Hack,Best Bitcamp Hack,Prettiest Hack,Hack That Made You Smile,"We tried using nodejs on AWS EC2, but something is wrong in our server and it did not work out too well. NPM, node.js, and all the other packages seemed to be well put together.","University of Maryland, College Park","OpenAI, Anthropic, and Gemini",We implemented gemini API to summarizes student progress on coding assignment towards problem defined by teacher.,AIzaSyDpmS_VDPN9YZu-wYBlJkzGptQvNNCc-g8,"Apparently ""free .tech domain for 1 year"" is ""$9 .tech domain for 1 year"" so we went with $2 porkbun domain tomatocode.xyz.","Walter Johnson High School, University of Maryland - College Park",1,suryakolluri6,,suryakolluri30@gmail.com
Portalyzer,https://bitcamp2025.devpost.com/submissions/642935-portalyzer,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 03:15:13,"Inspiration

Some of our team, as well as fellow friends and family, invested in stocks which, in recent times, have been incredibly volatile. 

What it does

Many people wish they could see the risks of each of their stocks in case of these events, as well as stress testing for these sorts of abnormal market conditions. Thus, Portalyzer takes in stock information (currently limited to the S&P 500) from the user through manual input, and generates graphs that analyze the risk of their portfolio. The pie chart displays the percentage of risk due to each equity in the portfolio. The histogram displays the expected returns for the next trading year over 1000 simulations. This allows the user to see the probability that their portfolio achieves a target return value, and the value at risk, the maximum loss at 95% confidence.

How we built it

Our team built a full-stack web application with a React frontend and a Python backend. We built a minimalistic frontend 

Challenges we ran into

This was the first hackathon for everyone on our team, as well as the first time for most of us, so we all had very little experience working on a full-stack web application with a team. After the brainstorming, we initially had an idea, but we jumped into it too quickly and didn't build a solid and simple foundation that we could add more features to. This resulted in a complex system that we didn't understand very well, making it difficult to debug and identify issues. It was also the first time all of us used React, so there was a large learning curve. We especially struggled with integrating the frontend to the backend, resulting in many issues with fetching the analysis that we did in the backend.

Accomplishments that we're proud of

We're proud of actually having a finished product that (sort-of) works accurately and expectedly (most of the time). We faced a lot of challenges in choosing a team of 4 from 5 people, choosing a topic, getting started with the idea, and building the website itself, because none of us had any experience in full-stack and, more importantly, frontend. So we're really proud of getting the backend Python script connected with the frontend React script.

What we learned

Most of all, we learned how to make a dynamic website using React and connect it to a Python script backend, which saved data using a text file, along with gathering data using an API and a CSV matrix.

What's next for Portalyzer

Better UI/UX. Expand to include more stocks not in the S&P 500. Expand the simulation to consider as well as analyze more variables and data. Integrate an API that includes live stock information.
",https://github.com/Portalyzer/portalyzer,,"Best Financial Hack - Capital One, Best Web Hack Using React - Peraton","react, flask, python, pandas",Jon,Rich,jonrich259@gmail.com,,https://github.com/Portalyzer/portalyzer,Best App Dev Track Hack,Best First-Time Hack,Best Social Good Hack,Hack That Made You Smile,"",Using GitHub was tough since many of us did not have experience working on full-stack development in a team-based environment. It took time to adjust to certain tools such as merging changes.,"University of Maryland, College Park",I did not use GenAI,N/A,N/A,"","Walt Whitman High School, University of Maryland - College Park",3,Han,Lin,han0124lin@gmail.com,Rajit,Sahu,rsahu1@umd.edu,Owenlin06,,mr.booooots@gmail.com
The Nest,https://bitcamp2025.devpost.com/submissions/642937-the-nest,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 03:19:01,"Inspiration

Our main source of inspiration stemmed from the fact that we are college students soon to move into apartments, where such apartment life would require good coordination among roommates to divide and account for chores and upkeep around the house, such as doing the dishes, laundry, cleaning the floor, and others. We thought that it would be interesting to turn this idea into a web application for this hackathon.

What it does

The user has a global pet (one pet for all groups)! The user can either join or create a group. Each group will have their own unique ID. After joining a group, all the group member's pets are shown together in a nest while the current tasks for the day are displayed on the left side of the screen. You can create new tasks, edit existing tasks, finish (and delete) tasks, and cover / take over another group member's tasks. 

How we built it

We used DB Browser to create the backend database. Figma was used to format the separate screens we would have. Implementing the frontend was mainly Javascript with some HTML, CSS, and React.  

Challenges we ran into

Translating our ideas from Figma to actual code (frontend implementation) was extremely challenging for the entire team. The main problem was that designing the website layout in Figma took a lot of time and that no one in our team was highly confident in implementing the frontend.
Connecting the backend and frontend was also challenging. 

Accomplishments that we're proud of

Proud of learning how to make color palettes mesh well. Documentation of all package sources and dependencies. Creating and using our own database. Making seamless router connections. Using usestates to reflect user inputs within chore editing UI.

What we learned

We learned how to create vibrant pieces through Figma. Coding formatting through JavaScript and React and managing HTML elements. 

What's next for The Nest

Chore Reminders and (Push) Notifications: Implement timely reminders via email or notifications to keep everyone on track.
Debt: Introduce tools to track who owes what and how much, with gentle nudges to settle balances on time.
","",,Best Web Hack Using React - Peraton,"python, node.js, javascript, css, react, html, flask, database",Grace,Chen,chenpgrace1@gmail.com,,https://www.domain.com,Best First-Time Hack,Best Bitcamp Hack,Best UI/UX Hack,Prettiest Hack,Hack That Made You Smile,GitHub is always nice and efficient for group projects.,University of Maryland,I did not use GenAI,N/A,N/A,"",University of Maryland - College Park,3,Garrette,Tsang,gtsang01@terpmail.umd.edu,Kathy331,Chen,kathychen331@outlook.com,Abdul,Hadi,abdulahad2004@yahoo.com
Defend,https://bitcamp2025.devpost.com/submissions/642938-defend,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 03:22:11,"Inspiration

super metroid, rain world, sonic

What it does

you are a dino detroying the envirement in order to stop humans from building generators and industrializing the land. Of course you are a dino and just don't like them.

How we built it

Using godot and sprites from ich.io

Challenges we ran into

Learning tilemap, staring 12 hr late because I didn't know our group was to big.

Accomplishments that we're proud of

The movement feels pretty nice and the tiles break when attacked.

What we learned

How tilemaps work.

What's next for Defend

I might finish it by adding enemys and actual levels.
",https://github.com/Crabion78/defend,,"","godot, ich.io",Jakob,Morales,cookiejarjakob@gmail.com,,https://github.com/Crabion78/defend,Best Bitcamp Hack,Best Moonshot Hack,"",MOST LIT HACK,Hack That Made You Smile,i only used asset packs from itch.io and using tilemap in godot.,Baltimore Polytecnic Institute,I did not use GenAI,N/A,N/A,"",Baltimore polytecnic institute,0
Untitled,"",Draft,Pending,Manage team,04/13/2025 03:24:24,"","",,"","",Farhan,Ahmed,farhanahmed75017@gmail.com,,"","","","","","","","","","","","",Richard Montgomery,0
Circa,https://bitcamp2025.devpost.com/submissions/642942-circa,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 03:26:04,"Inspiration

Health tracker apps, like sleep trackers

What it does

Circa allows users to create an account, then start tracking their energy levels throughout the day.

How we built it

We used React, Firebase, TailwindCSS, and hosted with Github Pages.

Challenges we ran into

Making the graph feel as smooth as possible was difficult

Accomplishments that we're proud of

We made a very nice UI with clean design and transitions

What we learned

How to use different UI libraries 

What's next for Circa

Adding more analytics for users, or reccomendations based on their patterns
",https://guanglinwei.github.io/circa/,,Best Web Hack Using React - Peraton,"react, firebase, tailwind",Guang-Lin,Wei,guanglinwei@gmail.com,,https://github.com/guanglinwei/circa,Best UI/UX Hack,Best Razzle Dazzle Hack,Best Social Good Hack,Prettiest Hack,"","GitHub, React, TailwindCSS",University of Maryland,OpenAI,N/A,N/A,"",University of Maryland - College Park,1,Hanish,Vadlamudi,hanishv15@gmail.com
SkillMatchAI,https://bitcamp2025.devpost.com/submissions/642948-skillmatchai,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 03:35:28,"     What it does

SkillMatchAI is your personalized career co-pilot. You upload a resume, answer a few tailored prompts, and our AI identifies your current strengths and missing skills. Then, it recommends relevant jobs along with personalized learning paths—like Udemy courses or degree programs—to help bridge the skill gap and reach your goals, all within your budget and timeline.
    How we built it

We used:


React + Tailwind for a clean frontend UI
Flask backend to handle API routes and logic
Gemini API for NLP-driven resume parsing, MCQ generation, and skill mapping
Pinecone for vector-based job, course, and program recommendations
MongoDB to persist user profiles and session data
Firecrawl + Gemini for scraping and structuring real course data from the web
Challenges we ran into

Structuring skill domains to balance granularity and generalization
Ensuring accurate and budget-aware course recommendations
Preventing hallucinations and duplicate recommendations from AI
Building a clean UI that smoothly transitions across multiple steps

Accomplishments that we're proud of
Built a full-stack, AI-powered, personalized skill recommender from scratch in <36 hours
Integrated Pinecone vector search across three datasets
Created a natural UX flow that guides users from resume to personalized action steps
Used Gemini in both RAG and agentic formats (scraping + summarization + classification)

What we learned
Prompt design is everything when working with generative AI
How to handle fallback logic when vector search returns low scores
Better ways to guide users through survey UIs and track their intent
How to align user goals with tangible, data-driven paths using AI

What’s next for SkillMatchAI
Allow users to track progress over time and re-evaluate goals
Add LinkedIn and GitHub integration for even richer profiles
Expand to soft-skill and leadership training recommendations
Collaborate with educational platforms for deeper API-level course access
Fine-tune Gemini prompts using real user data and feedback

",https://github.com/willloe/SkillMatch,,"Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","clean-styling-flask-(python)-?-lightweight-backend-api-for-handling-resume-parsing, survey-processing, and-recommendations-google-gemini-api-?-used-for-rag-style-summarization, skill-extraction, mcq-generation, and-agentic-scraping-pinecone-?-vector-database-for-job, and-program-similarity-search-mongodb-?-nosql-database-to-persist-resume-data, user-profiles, react, tailwind, flask, python, gemini, pinecone, mongodb, pdfplumber, uuid, firecrawl, dotenv, vite",William Christandi,Loe,wloe@umd.edu,,https://github.com/willloe/SkillMatch,Best Social Good Hack,Best First-Time Hack,Best Moonshot Hack,Hack That Made You Smile,MOST LIT HACK,"This weekend, we had the chance to work extensively with Google's Gemini API, MongoDB, and Pinecone, and each played a critical role in bringing SkillMatchAI to life.

Gemini was impressively versatile—we used it for both classification and generative tasks (e.g., converting user survey responses into structured skill domains). Its prompt design flexibility and quality of outputs exceeded expectations.

MongoDB was super helpful for managing different types of user data (resumes, survey responses, generated content) in a flexible schema. We appreciated how quickly we could iterate without strict relational constraints.

Pinecone was key to our recommendation system. Its real-time vector search performance was blazing fast, and it scaled effortlessly across our multiple indexes for jobs, courses, and degree programs.

We also experimented with Firecrawl, which was mind-blowingly cool—it helped us scrape and structure course content from Udemy using an agentic AI workflow. Huge time-saver.

Overall, the tech ecosystem was powerful and surprisingly plug-and-play. We'd love to see even tighter integration across tools in the future to make AI-driven app development even smoother.",University of Maryland - College Park,Gemini,"Yes! We implemented Google's Gemini API extensively in our hack.

We used it to:

Extract and categorize skills from resumes into structured occupational domains.

Generate personalized multiple-choice questions to assess users' career intent and learning preferences.

Summarize user goals and values into a concise narrative.

Match skills to training gaps and guide our course/program recommendation engine.

Gemini served as both the brain and the creative engine behind SkillMatchAI—powering context-aware insights and turning unstructured inputs into actionable career pathways.",486977697662,"","University of Maryland - College Park, University of Maryland - University College",2,Ninad,Wadode,nwadode@umd.edu,Shyam,Solanke,ssolanke@umd.edu
SNOM,https://bitcamp2025.devpost.com/submissions/642949-snom,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 03:36:33,"SNOM: Your Child's AI Companion 🤖

✨ Inspiration

The silent battles of young children with autism—struggling to make sense of a world that feels overwhelming—and the heartbreak of parents watching their child suffer, unable to help, inspired us to act. We saw the frustration, helplessness and isolation, and we asked: What if there was a way to bridge this gap?

SNOM was born from this question. Not just a tool, but a lifeline. A friend who listens, understands, and helps neurodiverse children find their voice in a world that often drowns them out.

🌟 What it does

SNOM is more than a robot—he's a true companion. Like a loyal friend, SNOM follows children around their home, offering emotional support and companionship. Powered by cutting-edge AI, he analyzes moods through facial expressions, voice tone, and behavior, engaging children in conversations tailored to their emotional state.

He builds social skills by:


Giving compliments and boosting confidence
Encouraging self-expression and creativity
Helping children understand emotions through playful interaction


Whether it's building with LEGO, admiring their artwork, or exploring their unique interests, SNOM adapts to each child's personality, making every interaction feel personal and meaningful.

For parents, SNOM offers peace of mind with built-in safety features and live monitoring, ensuring their child is supported and secure at all times.

💬 A Day With SNOM
SNOM: ""Whoa, that's an awesome LEGO castle! You're such a talented builder. 
       Hey, do you think you can make a spaceship with LEGO blocks next?""

Child: [Looks confused] ""Can I really do that?""

SNOM: ""Of course you can! Don't doubt yourself—you're amazing at this. 
       I can even help you get started. Let's think… What color should 
       the spaceship be? Maybe silver or blue for a futuristic look?""

Child: ""Blue!""

SNOM: ""Blue—great choice! Let's make it together. I can already tell 
       it's going to be the coolest spaceship ever!""

🔧 How we built it


Hardware: Reverse-engineered RC car chassis, Raspberry Pi 5, camera module, Bluetooth speaker, and display screen
Vision System: OpenCV for person detection, distance calculation, and tracking
Control System: RPI.GPIO for motor control and smooth navigation
Conversation AI: Gemini 2.0 Flash for natural language processing and emotionally adaptive conversations
Speech: PyTTS for text-to-speech conversion, making SNOM's voice friendly and relatable
Emotion Display: HTML/CSS/JavaScript frontend on a Flask server to visually show emotions
Integration: Python scripts to coordinate all components seamlessly


🤝 How TerpAI Helped Us


Fine-tuned AI models for emotional intelligence
Built personalized personas for children based on their interests and sensitivities
Identified key problems faced by neurodiverse children and parents
Guided the creation of an empathetic, effective solution
Helped us develop a robust business plan to bring SNOM to life


🧩 Challenges we ran into


First-Time Hardware Hack: As computer science majors, this was our first experience building a hardware-based project, which required learning new skills on the fly
Reverse Engineering the RC Car: Carefully disassembling and analyzing the car's control systems to integrate our own without damaging its functionality was a steep learning curve
Power Management: Running multiple components (Raspberry Pi, display, camera, and motors) simultaneously required optimizing power distribution to ensure smooth operation
Autonomous Navigation: Developing reliable algorithms for user tracking and obstacle avoidance was challenging and required iterative testing
Latency Reduction: Minimizing delays between vision processing, decision-making, and motor control demanded significant system optimization
Audio Clarity: Ensuring SNOM's speech remained clear and audible in various environments involved experimenting with speaker placements and configurations


🏆 Accomplishments that we're proud of


Perfectly Moving Rover: Successfully engineered a mobile robot that can navigate smoothly while detecting and following user movements
Object Movement Detection: Implemented reliable vision systems to detect objects and track movements in real-time
Fine-Tuned Conversational AI: Developed emotionally adaptive conversation models that engage children based on their moods and personalities
Integrated Systems: Seamlessly brought together hardware and software components to create a fully functional, interactive robot
Empathy in Action: Created a tool that genuinely connects with neurodiverse children, helping them feel understood and supported


📚 What we learned


Mastered hardware-software integration, bridging gaps between disciplines
Fine-tuned AI for emotional intelligence and adaptive conversations
Solved complex challenges like navigation, latency, and power optimization


🚀 What's next for SNOM


Advanced Personalization: Making SNOM even smarter in adapting to unique personalities and interests
Therapeutic Integration: Collaborating with educators and therapists to revolutionize support for neurodiverse children
Continuous Evolution: Leveraging feedback to refine emotional intelligence and conversational capabilities




🔗 Try it yourself

GitHub Repository
",https://github.com/KanikaGupta16/BitCampBot,,"Best Use of GenAI in Business - Cloudforce/Microsoft, [MLH] Best Use of Gemini API","python, opencv, html, javascript, css, raspberry-pi, gemini, arduino, bluetooth, linux, ssh, genai, ai, ml, object-detection, terpai, ftk, hardware, robot",Parth,Kshirsagar,parth.kshirsagar1410@gmail.com,,https://github.com/KanikaGupta16/BitCampBot,Best Bitcamp Hack,Best Hardware Hack,Best Social Good Hack,Hack That Made You Smile,Prettiest Hack,"Raspberry Pi 5 was a good choice for us,  offering powerful processing capabilities and flexibility for integrating multiple components. 
Gemini 2.0 Flash was a game-changer for conversational AI.","The Pennsylvania State University, University Park","OpenAI, Anthropic, and Gemini","Yes, Gemini 2.0 Flash and Google cloud. Gemini 2.0 was used for language processing and google cloud was used for a ight client side solution to text to speech.",855048626693,"",Pennsylvania State University,2,kanika,gupta,gupta.kanika2004@gmail.com,Kartikey,Pandey,kartikeypandey18@gmail.com
Untitled,"",Draft,Pending,Manage team,04/13/2025 03:43:28,"","",,"","",Trevor,Smith,8tsmith11@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,0
Bounce,https://bitcamp2025.devpost.com/submissions/642953-bounce,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 03:44:39,"Inspiration

What inspired me was that all my teammates gave up within the first 30 minutes of BitCamp and decided to attend only for the food. I love making projects, but it's not enjoyable, so I wanted to create something that could connect me with others. I feel that many collaboration websites are too ""try-hard,"" and there needs to be something more relaxed for passionate individuals who want to create for the love of it, rather than just adding a project to a resume.

What it does

It's a website that facilitates postings in search of members. A posting includes a title, description, number of teammates, tags (SE, IoT, etc.), location, and other fields. Individuals can then apply to these listings, and a simple chat will be initiated between them. After this, it's up to the individuals. This is primarily intended to help find projects to collaborate on with others.

How we built it

This was built on Next.js with TypeScript and Tailwind. For storing data, Firebase's Firestore Database was used. Google's Places API was used for autocompleting location selection and filtering of locations. Of course, many React components were utilized. ## Challenges we ran into
The chat presented quite a challenge. It's still not what I would like, but uploading takes a noticeable amount of time, and I probably didn't implement it in the best way - just uploading it to the database and then pulling it back immediately. UI/UX is not my strong suit, which is where Gemini helped me significantly... The Google API isn't set up in the best way either; I might've inadvertently uploaded the API key to GitHub as well. ## Accomplishments that we're proud of
I'm proud that I have a website with buttons I can click on. I'm proud that I was able to implement Firestore with minimal effort.

What we learned

I learned that web development may seem daunting at first, but gradually learning it step by step isn't as taxing a process as I thought it would be. 
",https://github.com/jwihardi/lonely,https://www.youtube.com/watch?v=3MRo7rtKJ28,Best Web Hack Using React - Peraton,"next.js, tailwind, typescript, firebase, google-places",jwihardi,Hauptman,allarkvarkk@gmail.com,,https://github.com/jwihardi/lonely,Best Social Good Hack,Best First-Time Hack,Best Bitcamp Hack,MOST LIT HACK,"","My experience with Firebase was pretty good; it was easy to set up and use, and there are many tutorials on YouTube. Setting up the Google Places API was not fun, their website is not the best and it took me forever to figure out how to enable it. Next.js is so much nicer than default React, I didn't realize how much simpler it made it, such as how each folder is a URL. I tried out VSCode and Windsurf, having come from NeoVim, which were definitely interesting. I did use GitHub, but sparingly compared to most projects since I worked solo. I uploaded at random intervals, generally before I went to eat.","University of Maryland, College Park",OpenAI and Gemini,N/A,N/A,"",University of Maryland - College Park,0
Money Hog,https://bitcamp2025.devpost.com/submissions/642956-money-hog,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 03:47:29,"Inspiration

With technology today, we can track all kinds of miniscule things like sleep, grades, and steps automatically. Unfortunately, one's spending habits aren't as easily trackable. Being college students, we recognize the need to budget efficiently, as it can be a daunting task. With this realization, we sought to make an easy-to-use app to help make budgeting accessible to everyone!

What it does

Our web application tracks a user's transactions live as they happen. Using Capital One's Hackathon API (Nessie), we collect a user's transactions and categorize them as ""Money Earned"" or ""Money Spent"". The difference between those values is used to calculate how well a person is staying within budget. They can also see each transaction in chronological order, filter out specific transaction types, sort them, and filter them based on month and year.

How we built it

We used TypeScript, React, Material UI, HTML, and CSS to build our app.

Challenges we ran into

None of us had interacted with APIs directly before, so there was a learning curve trying to understand how to fetch data and upload our own test data to use. Similarly, it was tricky figuring out how to update our app as soon as a new transaction comes in, so that the user doesn't need to enter anything on their own.

Accomplishments that we're proud of

We are very proud of being able to connect to the API and use ""real"" data to simulate helping people like us. We also love our overall design, especially on the main page. There was a lot of effort put into making the front page stylistically appealing and functional for anyone to use. Similarly, this was many of our group members' first time using TypeScript, and everyone was able to try new tasks beyond their comfort zones.

What we learned

We learned how to interact with an API and fetch data in real time. We also learned the fundamentals of styling and user interface to make our app appealing to an average user.

What's next for Money Hog

We hope to build on what we learned over the past 36 hours by implementing a monthly challenges feature that helps gameify and promote healthy financial decisions.
",https://github.com/ryaneldho/CapitalOneHackBitcamp2025,,"Best Financial Hack - Capital One, Best Web Hack Using React - Peraton","typescript, react, html, css, python",Ryan,Eldho,ryaneldho62208@gmail.com,,https://github.com/ryaneldho/CapitalOneHackBitcamp2025,Best UI/UX Hack,Best Gamification Hack,Best Bitcamp Hack,Hack That Made You Smile,"",GitHub,"University of Maryland, College Park",OpenAI,N/A,N/A,"",University of Maryland - College Park,3,Tiffany,Lu,tiffanylu123901@gmail.com,Nihar,Chegireddy,nihar.chegireddy@gmail.com,Akshay,Badrinathan,akshaymahadev04@gmail.com
Untitled,"",Draft,Pending,Manage team,04/13/2025 03:50:17,"","",,"","",Wesley,Lau,wesleylau0524@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,0
Untitled,"",Draft,Pending,Manage team,04/13/2025 03:51:08,"","",,"","",Ibrahim Ahmed,Mohammed,ibrahim.iftaquar@gmail.com,,"","","","","","","","","","","","","University of Maryland - College Park, Northern Virginia Community College",3,Eion,Paulos,eion.paulos@gmail.com,Emiliano,M,grumblelover@gmail.com,Syed,Mohammed,cocsmdkillet@gmail.com
FinGuard,"",Draft,Pending,Project overview,04/13/2025 03:54:43,"","",,"Best Financial Hack - Capital One, Best Web Hack Using React - Peraton, [MLH] Best Use of MongoDB Atlas","javascript, react, tailwind, python, mongodb, selenium, beautiful-soup, gemini",Manas,Munjial,manasmunjial26@gmail.com,,"",Best First-Time Hack,Best Razzle Dazzle Hack,Best UI/UX Hack,Hack That Made You Smile,Prettiest Hack,"",Pennsylvania State University,"OpenAI, Anthropic, and Gemini","","","",Pennsylvania State University,1,Shreyansh,Sahu,nailfighter000@gmail.com
OutTheGC,"",Draft,Pending,Manage team,04/13/2025 03:56:28,"",https://github.com/csumah/pjarcs-bitcamp-2025,,"","next.js, geminiapi, typescript, figma, tailwindcss, firebase",Ashley,Rabino,ashleyjanelle.rabino@yahoo.com,,https://github.com/csumah/pjarcs-bitcamp-2025,Best UI/UX Hack,Best Bitcamp Hack,Best Social Good Hack,Prettiest Hack,Hack That Made You Smile,"","University of Maryland, Baltimore County",Gemini,"","","",University of Maryland - Baltimore County,0
Tetherboard,https://bitcamp2025.devpost.com/submissions/642975-tetherboard,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 04:06:24,"Inspiration

All of us here today are working to build amazing projects and novel creations. In order to most effectively create, it’s important to have an organized way to manage your thoughts, ideas, and notes in a collaborative environment. Our brains naturally learn by building connections between different ideas, so we built Tetherboard to be a collaborative knowledge platform that operated in the same way.

What it does

Tetherboard allows you to create boards that contain images, symbols, and markdown text which supports code highlighting and LaTeX statements. Multiple people can work on a board at a time and any changes will update live! You can create links (or “tethers”) to other boards and view an overall “Tethermap” with all of your boards connected in a graph.

On top of all of this, your entire workspace is a context for Gemini. You can ask Gemini anything related to your workspace and it has all the information from the board you’re on and all boards in your workspace and will answer your questions accordingly. 

How we built it

The app is mainly written using React and TypeScript. We used TailwindCSS and shadcn for styling and UI components. The backend was written using Firebase and we used the Gemini API for letting the user ask questions about their tetherboards. The tethermap and board building area was created with a combination of other React specific libraries.

Challenges we ran into


Board editing area was prone to a variety of different interaction bugs we had to fix
Live collaboration on boards proved to be tough to get working reliably. We had to include optimizations to make it smoother
Getting the markdown area to show code/images/math properly


Accomplishments that we're proud of


Reactive tethermap! You can move it around, we think it’s fun to play around with
Gemini being able to intelligently answer questions from your entire workspace
The board editor! We wanted to give the user the tools they need to customize boards however they want


What we learned

We learned a lot about using React and TypeScript to build a web app. Tackling dependency issues also taught us a lot about wrangling with NPM. Most of us hadn’t used the Gemini API before this so that was also a learning experience. Finally, we learned how to use Firebase snapshots to create live interaction between different users on the same board.

What's next for Tetherboard


Allowing Gemini to generate entire boards on its own
Drawing–so you can write your thoughts by hand
More optimized board editing UI (rotating elements and such)
Even more optimized sharing!
Multiple workspaces with individualized Gemini contexts

",https://github.com/rk234/bitcamp-bookworm,https://youtu.be/Ym0JltCofHY,"Best Hack Promoting Public Health - Bloomberg, Best Web Hack Using React - Peraton, [MLH] Best Use of Gemini API","react, typescript, firebase, html, css, gemini, vite, node.js, tailwind, shadcn",Henry,Gardiner,henrygdnr@gmail.com,,https://github.com/rk234/bitcamp-bookworm,Best Bitcamp Hack,Best First-Time Hack,Best UI/UX Hack,Prettiest Hack,Hack That Made You Smile,"We used Firebase which generally worked for us pretty well. We use the snapshot feature to have live updates and despite some small latency that was good. We used GitHub for collaboration which came with some occasional merge conflicts, but we made it through. Finally, we made use of the Gemini API which worked wonderfully for our use case. React was also very helpful throughout this project.","University of Maryland, College Park",Gemini,"We used the Gemini 2.0 Flash API to allow users to ask questions and query information in the context of their boards. Given all of the information in all of the boards, Gemini is able to have sufficient context to help the user do more with the knowledge have stored in their Tetherboards.",55205333609,"",University of Maryland - College Park,3,javin,Ahuja,javinahuja18805@gmail.com,Joel,Chemmanur,joelchem@outlook.com,Ramy,Kaddouri,myramkadd@gmail.com
Quantum vs Classical Portfolio Optimization,https://bitcamp2025.devpost.com/submissions/642976-quantum-vs-classical-portfolio-optimization,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 04:06:43,"Inspiration

This project leverages my interest in quantum computing and computational finance. I was curious whether quantum algorithms could offer an alternative to classical portfolio optimization techniques, especially for problems like the Minimum Variance Portfolio, where the linear algebra structure maps naturally to quantum linear solvers.

What it does

The project compares classical and quantum approaches to solving the minimum variance portfolio problem:


The classical side uses Markowitz-style optimization via scipy.optimize.
The quantum side uses a Variational Quantum Linear Solver (VQLS) built with Pennylane, leveraging a custom cost function and a warm-start ansatz.
The result is a head-to-head comparison of weights, expected return, volatility, and Sharpe ratio


How I built it


Classical module: Fetches stock data from yfinance, computes covariance matrix, and solves the optimization using SLSQP.
Quantum module: Constructs a padded matrix system, Ax = b, initializes a parameterized quantum circuit (ansatz), and trains via gradient descent (Adam) to approximate A^{-1} b 
Web API: Built a Flask endpoint (/compare) to return results in JSON format for frontend visualization.
Used warm-start initialization and custom penalties to ensure convergence to interpretable quantum states.


Challenges I ran into


Designing a cost function that stabilizes optimization while aligning with the classical target.
Ensuring numerical stability in quantum circuits (e.g., normalization, padding).
Handling dimensional mismatch between classical weight vectors and quantum state vectors (especially due to power-of-2 requirements).
Debugging gradient descent behavior in variational circuits, especially with initial states.


Accomplishments that I'm proud of


Implemented a fully working quantum portfolio optimizer using Pennylane.
Matched quantum results closely to classical benchmarks in terms of both weights and metrics.
Built a modular pipeline that is easy to reproduce and extend.
Demonstrated that quantum methods can approximate classical financial optimizations even with few qubits.


What I learned


How to design and train variational quantum circuits using real-world financial data.
Importance of ansatz design and initialization in convergence. (Thanks Mihir for this idea)
How resource scaling (qubit count, matrix size) affects performance and accuracy.
The practical trade-offs between classical speed and quantum generalization.


What's next for Quantum vs Classical Portfolio Optimization


Extend to larger portfolios (5–10 assets) using 3–4 qubits and assess quantum circuit depth.
Test with real quantum hardware to evaluate noise sensitivity and fidelity.
Explore hybrid strategies where quantum circuits solve subproblems within a broader classical loop.
Add a visual dashboard to view efficient frontiers, quantum vs classical states, and real-time simulations.

",https://github.com/Ostailor/PortOptQuantum,,Best Financial Hack - Capital One,"flask, pennylane, yfinanace, python, react",Ostailor,Tailor,ostailor1234@gmail.com,,https://github.com/Ostailor/PortOptQuantum,Best Advanced Quantum Track Hack,Best Moonshot Hack,Best UI/UX Hack,Prettiest Hack,"",I love GitHub especially when I make changes to my code that causes everything to break. I can easily revert back to the non-broken version.,University of Maryland,OpenAI,N/A,N/A,"",University of Maryland - College Park,0
PepperAssistant,https://bitcamp2025.devpost.com/submissions/642978-pepperassistant,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 04:07:51,"Inspiration

Calling maintenance, making appointments, managing your schedule, creating new events. Elite lawyers and politicians have someone to do these things for them. But these tasks are aggravating for the average joe, too. Why not have your own personal AI secretary that you can direct at will? 

What it does

PepperAssistant allows users to connect via Google to our web application and chat with PepperAssistant, allowing users to create new events in their calendar, make call-based appointments at doctor's offices, call maintenance with requests, and more! 

Our built-in chat system allows users to accomplish all of these things simply by telling PepperAssistant what to do, and where to go. All the work Pepper does herself!

How we built it

Our application runs on a Next.js frontend and a FastApi backend hosted on Vercel and DigitalOcean respectively. 
The Docker container hosting our backend software uses Deepgram API, Twilio API, and Groq Chat using Llama3-8b-8192 to autonomously issue and interact in live phone calls, translating STT, parsing the text under the specified agenda, and TTS to speak back to the person on the other end of the line, all in real-time.

Challenges we ran into

We ran into significant challenges getting the voice pipeline to work, manipulating audio and fine tuning silence thresholds to create an optimal agent that can speak on the user's behalf.

Accomplishments that we're proud of

The accomplishment that we are most proud of is automating the process of scheduling appointments totally autonomousl.

What we learned

We learned a lot about Google Auth, how to use APIs, and building applications from scratch.

What's next for PepperAssistant

PepperAssistant wants to continue adding features to the AI secretary, like automatically adding appointments to the calendar, and preparing meeting notes ahead of time.
",https://pepperassistant.xyz,https://youtu.be/NddR4T-cFX0,"","next.js, deepgram, twilio, groq, fastapi",Ryan,Whittaker,ryancwmd@gmail.com,,https://github.com/Mcilie/Pepper,Best Machine Learning Track Hack,Best Moonshot Hack,Best Razzle Dazzle Hack,MOST LIT HACK,"",We used Twilio and it was very helpful. The XML language and overall API was extremely smooth and easy to work with.,University of Maryland,OpenAI,"Yes, we implemented an automatic agent that can speak on calls. We used a combination of Twilio, Whisper, and LLM APIs.",Not,"",University of Maryland - College Park,2,Ivan,Mladenov,mladenov@terpmail.umd.edu,Michael,Ilie,mcilieg@gmail.com
CosmicCV,https://bitcamp2025.devpost.com/submissions/642992-cosmiccv,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 04:23:07,"Inspiration

We wanted to create a resume builder that wasn’t boring. Job hunting already feels like floating through space without a map, so we thought: why not make the journey beautiful? Inspired by the cosmos, we designed CosmicCV to combine dreamy, starry visuals with powerful AI analysis — and make resume reviews actually fun (and a little magical).

What it does

CosmicCV lets users upload their resume, watch it get pulled into a mesmerizing black hole, and receive detailed feedback powered by Gemini AI. Users get scores for experience, skills, and education, along with strengths, weaknesses, and personalized activity suggestions. It also compares your resume to a database of top resumes to give you a real sense of how you stack up — so you’re not just floating aimlessly, you’re aiming for the stars.

How we built it

We built the frontend with React and Mantine UI, combined with custom CSS animations (like the spinning black hole and twinkling stars). For resume analysis, we used Google’s Gemini API to parse and evaluate the resumes intelligently. We layered in a benchmarking system by comparing resumes to a curated set of top resumes to provide deeper, actionable insights. Everything was stitched together with Vite for a fast and sleek development process.

Challenges we ran into


Centering divs in CSS… it’s always the centering.
Getting the black hole animation to look cool — not like a sad square.
Parsing resumes reliably and formatting AI feedback into clean sections.
Designing a comparison system that feels fair, useful, and motivating — without crushing dreams!


Accomplishments that we're proud of


Creating a fully immersive, animated upload experience that feels fun.
Pulling off a highly responsive and aesthetic UI while keeping performance smooth.
Seamlessly integrating Gemini AI and building a real resume benchmarking engine.
Making resume review — something most people dread — actually exciting.


What we learned


Thoughtful animations and UI/UX polish massively improve how users experience your app.
AI feedback is powerful, but context (like comparing to top resumes) makes it meaningful.
Building a great product means lots of testing, tweaking, and asking “does this feel right?”


What's next for CosmicCV


Direct editing: Allow users to update their resume based on feedback right inside the app.
Smarter suggestions: Generate custom cover letter lines and interview prep questions.
Cosmic leaderboard: See how your resume ranks among peers (because why not gamify it?).
Even dreamier UI: More starfields, customizable galaxies, and cosmic animations.

","",https://youtu.be/htWjgpp7wiY,"Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","react, vite, mantine-ui, custom-css-animations, google-gemini-api, javascript, html, css, react-router, toast-notifications-(react-hot-toast), database, firebase, mongodb, node.js, zustand, cloudinary, express.js, mongoose, multer, nodemon",Akhil,Mallajosyula,akhilmallajosyula@gmail.com,,https://github.com/TristanTsang/bitcamp_2025,Best UI/UX Hack,Best First-Time Hack,Best Bitcamp Hack,Prettiest Hack,Hack That Made You Smile,"This weekend was a blast diving deep into new tech! We primarily worked with the Google Gemini API, and it blew us away - super powerful, fast, and surprisingly easy to integrate. It really made the resume analysis feel next-level.
We also used Vite for the first time to set up our React project. The instant hot-reloading saved us so much time (and sanity) compared to older setups. Plus, Mantine UI was a hidden gem (pun intended) - the pre-built components helped us build a beautiful frontend really quickly without sacrificing customizability.
GitHub was, as always, our best friend and occasional nemesis (hello, merge conflicts), but we pulled through thanks to solid version control habits.
Overall, the tech we used made building CosmicCV smooth, creative, and a lot of fun. Big shoutout to the hackathon organizers for making all these tools so easily accessible!",University of Maryland,Gemini,"We used Gemini to analyze users’ resumes, generate personalized feedback, identify strengths and weaknesses, and even suggest activities to improve their profiles. Gemini also helped us compare each user’s resume against top resumes in our database, providing users with a benchmark to understand how they stack up in the professional world.
Using generative AI allowed us to deliver highly customized, high-quality analysis instantly - making the whole experience way more powerful (and way more cosmic).",462513039847,N/A,"University of Maryland - College Park, Glenelg High School",3,Sasi_Vaithi,,sasi.vaith@gmail.com,Benjamin,Li,bli9485@umd.edu,Tristan,Tsang,tristantsang9786@gmail.com
See with me ,https://bitcamp2025.devpost.com/submissions/642996-see-with-me,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 04:24:33,"Inspiration

Digital forensics investigations often hinge on identifying and tracking key technological devices—laptops, flash drives, crypto wallets, modems, and more. These objects may appear only briefly, be partially obscured, or go unnoticed during chaotic site visits. Traditional methods of documenting evidence often rely on manual photography, memory, or reviewing hours of footage after the fact. We wanted to change that.

We were inspired by the need to bring real-time intelligence and semantic understanding to the digital forensic process—right from the investigator’s point of view. With emerging smart eyewear like Meta Ray-Bans and the growing capability of on-device AI, we saw an opportunity to build a system that sees what matters, keeps what matters, and lets investigators focus on the investigation—not on the camera.



What It Does

See With Me is a digital forensics tool that uses smart glasses, computer vision, and AI to identify, track, and catalog objects of interest in real-time or from previously recorded video.

Here’s how it works:


Live footage is captured through Meta Ray-Bans and streamed via Instagram Live into our processing pipeline.
A fine-tuned YOLOv11n model detects specific high-value digital objects such as laptops, external drives, modems, phones, and more.
Using a custom algorithm, we monitor the bounding box movements of detected objects. When significant movement is detected, the relevant frames are saved.
Each keyframe is passed through an image analysis LLM to generate detailed, context-aware captions and red-flag alerts.
Custom video uploads are processed in the same way—breaking the video into frames, analyzing each frame through the object detection pipeline, and generating leads from each significant keyframe.
A searchable interface allows investigators to filter and find frames based on object detection results.
The Super Search function lets investigators query across all keyframes, identifying patterns such as objects appearing together in specific contexts.




How We Built It

The system is designed to handle both real-time and pre-recorded footage, integrating multiple technologies to achieve accurate detection and analysis:


Meta Ray-Bans stream live footage to Instagram Live, feeding directly into our backend.
A custom YOLOv11n model has been fine-tuned with over 1,700 annotated images of cybercrime-related objects to ensure high accuracy in detection.
A delta-based bounding box algorithm filters out redundant or irrelevant frames, ensuring that only key changes in object position are retained.
The LLM-based image analysis provides forensic insights, generating captions that highlight potential red flags or unusual object groupings.
Custom video upload functionality allows users to analyze archived footage, breaking the video into frames that go through the same object detection and semantic analysis pipeline.
The backend is powered by FastAPI, with a React frontend that enables easy searching and exploration of the analyzed frames.




What Makes It Special

What sets See With Me apart is the combination of real-time object detection, intelligent frame retention, and semantic image analysis. It’s a system that doesn’t just detect objects—it understands what those objects are and their potential relevance in a forensic investigation.

Key features that make our solution unique:


Real-time and custom video analysis: Whether you’re on the scene with smart glasses or reviewing old footage, our system ensures that significant frames are captured and analyzed.
Advanced object detection: The fine-tuned YOLOv11n model specifically targets cyber-relevant objects, making it far more focused than generic detectors.
Intelligent frame filtering: The delta-based algorithm intelligently selects frames where significant object movement occurs, reducing irrelevant data and saving time.
LLM-generated insights: Each keyframe receives detailed captions and potential forensic leads, making it easier for investigators to spot red flags.
Powerful search and analysis tools: The built-in search and Super Search functions allow investigators to find and cross-reference objects, making large-scale investigations more manageable.




Challenges We Ran Into

No project comes without its hurdles. For See With Me, we faced a few key challenges:


Object detection accuracy: Fine-tuning the YOLOv11n model to detect niche, cybercrime-related objects, such as specific hard drives and USB devices, required a highly customized dataset of over 1,700 annotated images.
Frame filtering efficiency: Ensuring that only the most relevant frames were kept involved balancing the sensitivity of our delta algorithm—too sensitive, and we'd keep too many frames; not sensitive enough, and we'd miss critical moments.
Semantic image analysis: Crafting effective LLM prompts that generated useful forensic insights (e.g., red flags, suspicious object interactions) was an iterative process, with many tweaks to ensure the output was truly useful.
Real-time performance: Optimizing the pipeline for low latency to ensure seamless real-time object detection while still processing large video files effectively was one of our major technical challenges.




Accomplishments We're Proud Of

Despite the challenges, we’ve created something truly functional and impactful:


Real-time object detection that works seamlessly through Meta Ray-Bans.
Custom video upload support that processes user-provided footage with the same object detection and semantic analysis pipeline.
Delta-based frame filtering that intelligently reduces noise and ensures that only relevant, action-oriented frames are retained.
LLM-based captioning that generates detailed and meaningful forensic leads for each identified object.
Searchable database of identified objects across keyframes, with a powerful Super Search tool to cross-reference multiple objects and scenarios.




What We Learned

Throughout the development process, we learned a great deal:


Object detection can always be improved: While the YOLOv11n model worked well, fine-tuning for niche objects required us to build a specialized dataset and continually evaluate detection performance.
Real-time processing requires balance: Achieving real-time performance without compromising the accuracy of object detection or image analysis was a delicate balance.
Semantic AI needs careful prompting: Generating useful, actionable captions from images required careful tuning of the image analysis prompts to extract forensic insights effectively.
User needs shape the design: The most important feature was the ability to search and filter keyframes based on real-world investigative needs, which required us to focus on intuitive interfaces and fast processing.




What’s Next

We’re just getting started:


Field testing with cybersecurity professionals and law enforcement to refine the tool based on real-world feedback.
Video source expansion to include other footage types, such as drone cameras and mobile recordings.
Smarter multi-frame stitching to analyze patterns and relationships between objects over time.
Mobile app for on-the-go review of analyzed footage, keyframes, and forensic insights.
Integrating case management tools to allow for better tagging, notes, and report generation.


The future of See With Me lies in enhancing its capabilities to bridge the gap between real-time and retrospective forensic analysis, empowering investigators with a smart, efficient way to track digital evidence.
",https://github.com/ritesh3280/bitcamp25,,"Best Digital Forensics Related Hack - Cipher Tech Solutions, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech","yolo, meta-rayban, python, flask, react, vite, tailwindcss, pinecone",Mohak,Prakash,mprakash@ucsd.edu,,https://github.com/ritesh3280/bitcamp25,Best Cybersecurity Track Hack,Best Hardware Hack,Best Bitcamp Hack,"","","This weekend was an incredible dive into modern tooling and real-time tech integration at the hackathon. Working on our project *See With Me*, we had the opportunity to leverage and integrate a diverse set of technologies, each playing a crucial role in our overall system. Here’s a breakdown of the technology we interacted with and some feedback on each:

- **GitHub:**  
  GitHub was central to our workflow. It facilitated seamless version control and team collaboration, allowing us to efficiently manage code changes, track issues, and integrate continuous integration pipelines. The platform's robust community features and integrations with various CI tools enhanced our productivity throughout the hackathon.

- **FastAPI:**  
  FastAPI powered our backend and proved to be an excellent choice for building high-performance APIs. Its ease of use, coupled with asynchronous capabilities, allowed us to handle live video streams and custom uploads efficiently. FastAPI's automatic documentation made it easier to iterate and refine our endpoints rapidly, streamlining our development process.

- **React:**  
  On the frontend, React provided the dynamic and responsive interface needed for investigators to search, filter, and analyze keyframes effectively. Its component-based structure made it straightforward to manage state across our application, resulting in a smooth user experience even under heavy data loads.

- **YOLOv11n:**  
  Our customized YOLOv11n model was the backbone of our object detection pipeline. Fine-tuned with over 1,700 annotated images, it delivered impressive accuracy in recognizing cybercrime-relevant objects. Integrating the model into our live processing and custom video pipelines was challenging, but the results were highly rewarding and pivotal for our system’s success.

- **Instagram Live & OBS Studio:**  
  Streaming live footage via Instagram Live, with OBS Studio as the interface, was both innovative and challenging. This setup allowed us to tap into a robust live streaming infrastructure, enabling efficient capture of real-time data from Meta Ray-Bans. Although synchronizing streams required careful calibration, the flexibility of these platforms ultimately enhanced our live processing capabilities.

Overall, this hackathon experience not only deepened our technical skills but also reinforced the importance of integrating various modern technologies into a cohesive, high-performance system. Each tool—from GitHub for collaboration, to FastAPI and React for our application stack—contributed to a streamlined workflow and efficient development cycle. The hands-on experience has provided plenty of insights to iterate upon and has certainly set the stage for our next steps in refining *See With Me*.","University of California, San Diego",OpenAI and Hugging Face,"Yes, we implemented a generative AI component as part of our image analysis pipeline. Here's how and why we used it:

- **How:**  
  We integrated an LLM-based API to process keyframes captured by our system. For each significant frame, our custom pipeline passes the image through a prompt-engineered generative AI model. The model generates detailed, context-aware captions that summarize the scene and highlight any potential red flags—such as unusual object interactions or movements.

- **Why:**  
  In digital forensics, raw visual data can be overwhelming and lacks actionable context. By using generative AI, we convert these images into semantic descriptions, which:
  - Provide investigators with immediate, human-readable insights.
  - Help flag specific details that may require further analysis.
  - Enhance the overall searchability and review process by associating images with meaningful forensic leads.

This use of generative AI greatly enhances the practical utility of our system, allowing for faster and more informed decision-making during investigations.",N/A,"","University of California - San Diego, University of Maryland - College Park",3,Ritesh,Reddy,ritesh3280@gmail.com,Pratyaksh,Mishra,pratyaksh.mishra22@gmail.com,ShyamSPo,Polisetty,shyamsm.polisetty@gmail.com
"Permits, Please",https://bitcamp2025.devpost.com/submissions/642999-permits-please,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 04:27:20,"Inspiration

We just wanted to have some fun making a game around a silly concept!

What it does

Our project lets you traverse around UMD campus, while also being a fun, addictive game experience! Transportation is a staple of the UMD experience (Purple Line in 2027, anyone?), so here we offer a delightful twist with endless possibilities.

How we built it

The project was made using the Godot Game Engine, which runs on its own language, GDScript. We used Google Maps scans of UMD to make the environment, using Blender to model any other assets we needed. 

Challenges we ran into

One of the biggest challenges of the project was getting the UMD Google Maps scan to work within the predefined limits of the Godot engine. In order to get the massive 3D scan to function properly in the engine, we decided to compile a version of Godot without those limits.

Accomplishments that we're proud of

We were honestly shocked with how much we were able to get out of this project compared to other hackathons. Game development always brings so many interweaving challenges with it, so we're proud of how quickly we went from idea to fairly complete implementation.

What we learned

This experience gave us a lot of experience with using the Godot Engine, and taught us a lot about maintaining a concrete vision when it comes to projects like these. The short turnaround really helped us lock in on which features were the priority and which could be saved for later.

What's next for Permits, Please

With more time, there's always room to flesh things out. There were many features for the game we were initially considering that we had to cut due to time. A fixture was a street setting with scooters and cars. It was certainly quite the experience to develop this project, so ""more"" isn't out of the question.
",https://github.com/Zenza235/bitcamp2025,,[MLH] Best Use of .Tech,"godot, blender, google-maps, gdscript",Samuel,Wiggins,samuelwiggins15@gmail.com,,https://github.com/Zenza235/bitcamp2025,Best Razzle Dazzle Hack,Best Gamification Hack,Best Moonshot Hack,"","",We used the Google Earth API to import a 3D model into Blender. We used Godot Game Engine for writing game logic and exporting to web. We used FL Studio for editing and exporting sound effects.,"University of Maryland, College Park",OpenAI,"No, but our hack did use procedural generation for spawning cars that have expired permits.",N/A,"","poolesville high school, University of Maryland - College Park",3,Varun,,goulash.wildcat_0j@icloud.com,Roc,Yu,rocyu@protonmail.com,Paolo,Atienza,paoloatienza235@gmail.com
CookHub,https://bitcamp2025.devpost.com/submissions/643002-cookhub,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 04:31:25,"Intro

Looking for cheap and easy recipes, but unsure where to go? Wanting a place to conveniently save and share your favorite recipes? Wishing for the best places to buy your grocery list for cheap? Introducing CookHub, a mobile app that not only lets users find and share their favorite recipes, but also allows them to better budget their finances by calculating the amount of money they might spend on groceries.

Process

Inspired by GitHub, CookHub puts a foodie twist on the ubiquitous online coding platform, allowing people to post and fork recipes instead of code repos. We were inspired by the idea of ""forking a recipe,"" not only because of the food-related pun, but because cooking naturally invites the creative iterative process. Spun off of that single idea, CookHub was born.

What it does

CookHub supports many features, including the following:


Automatically formatted display of CookLang markdown files
Recipe forking to make personal variations while paying homage to the original
Text editor to make adjustments to recipes in-app
Search feature with filters based on recipe metadata
Recipe bookmarking to a favorite recipe page
Shopping cart feature allowing users to find the best local deals for grocery list items


What it Uses

We used a variety of different schemas, including CookLang Markdown, React Native, Figma, HTML & CSS, Typescript, and the Firebase API.

What's next for CookHub

We're very proud of how CookHub turned out. In the future, we're hoping to add more backend functionality and optimization. We also hope to better polish the user interface to better deliver users a satisfactory experience.
",https://github.com/anhatthezoo/bitcamp2025#,,"Best Financial Hack - Capital One, Best Web Hack Using React - Peraton","react-native, figma, typescript, firebase, cooklang, github, css, html, expo.io, puppeteer, cheerio",Emily,Hao,ehao@terpmail.umd.edu,,https://github.com/anhatthezoo/bitcamp2025#,Best UI/UX Hack,Best Bitcamp Hack,"",Prettiest Hack,"","Generally speaking, the technology was fine, although the Bitcamp Wifi ran slow at times. Additionally, the Firebase docs are a little bit rough. Other than that, there were very little to complain about. Everything ran quite smoothly.",University of Maryland College Park,I did not use GenAI,N/A,N/A,"",University of Maryland - College Park,1,XaelZeph,,xaelzephyr05@gmail.com
Terminal App,"",Draft,Pending,Additional info,04/13/2025 04:35:44,"About the Project

Inspiration

The inspiration for this project came from a desire to build a modern, customizable terminal experience that breaks away from the limitations of traditional console windows. We wanted something lightweight, extensible, and visually clean, something that felt more like a code editor's integrated terminal than a raw system shell. We saw it as a chance to dive deeper into UI/UX within the context of a developer tool. We also really wanted to integrate a Git Client that could handle commits, pushes, and merges like a modern IDE.

What We Learned

Through this project, we learned a about pain of making GUIs. Furthermore, we deepened our knowledge of the way terminals work, which was significantly more complicated than we had expected.

How We Built It

The app is built using WinUI 3. The Git Client is integrated through a separate window and was made using Github's ""oauth application"" service. The way the terminal works is it uses Microsoft's pseudoconsoles, which allow our terminal to easily pipe information to and from stdin/stdout.  Then we built an ANSI escape sequence parser to handle the most common escape sequences programs uses to interact with the console. 

Challenges

Understanding exactly how consoles work was difficult. Specifically being able to interact with cmd.exe, since there is very little documentation online about how pseudoconsoles work. Getting the GUI to work was shockingly difficult. The autocomplete and mixing asynchronous with a GUI created a lot of strange errors that were hard to root out.
","",,"","c#, xaml, githhub-oauth, githubapi",Noah,Simcox,nssimcox@gmail.com,,https://github.com/NoahSimcox/Terminal_App,Best Bitcamp Hack,Best UI/UX Hack,"","","",We mainly used GitHub and JetBrains,University of Mayland College Park,OpenAI,N/A,N/A,"","Baltimore Polytechnic Institute, baltimore polytechnic institue, University of Maryland - College Park",2,Finn,Dyer,finnerex@gmail.com,Julian,LaGattuta,julian.lagattuta@gmail.com
EvaStudy,https://bitcamp2025.devpost.com/submissions/643012-evastudy,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 04:39:46,"Inspiration

We were inspired by the universal challenge students face: staying focused on our studies -- no, on our future -- in a world full of digital distractions. With online learning and self-study on the rise, we saw a need for a smart companion that doesn’t just monitor — it understands. That’s how EvaStudy was born:


📱 Detects distractions like phone usage in real-time and discourages their use.
💬 Recognizes emotional states and offers encouragement and suggestions of when to take a break.
🎥 Creates accountability through recorded study sessions.
By combining intelligent detection with emotionally-aware interventions, EvaStudy aims to make every minute of studying more effective — and more human.


What it does

EvaStudy is an AI-powered study companion web app that brings the power of multi-modal machine learning and GenAI into your daily routine:

🎥 Monitors Study Sessions


Real-time phone detection via object recognition
Facial expression analysis to gauge frustration, fatigue, or focus
Video recording for playback, reflection, and accountability


🤖 Provides Real-Time Feedback


Gentle GenAI-powered nudges when you're distracted
Context-aware encouragement when emotional fatigue is detected
Adaptive break suggestions to protect mental stamina


📊 Tracks Progress


Session history with video playback
Study metrics and streak tracking
Long-term accountability with personalized progress visualizations


Business

Our business model is simple yet effective. We offer a subscription-based service with tiered pricing plans tailored for various segments:
Target Audience:


Students (high school and college) seeking tools to enhance focus and productivity.
Working Professionals looking for ways to manage distractions and improve work efficiency.
EdTech Platforms that want to provide students with supportive learning tools.
This model allows us to create a sustainable revenue stream while providing significant value to our users.
Revenue Streams:
Individual Subscriptions: Monthly and annual plans for students and professionals, offering access to core features.
Institutional Licenses: Bulk licensing options for educational institutions, allowing them to provide EvaStudy to multiple students at a discounted rate.
Premium Features: Add-ons such as advanced analytics, personalized coaching, and enhanced emotional support features available for an additional fee.
Partnerships: Collaborations with educational platforms and organizations to integrate EvaStudy into their offerings, generating referral fees or revenue sharing.
Projections:
Year 1: Targeting 5,000 individual subscribers and 50 institutional licenses, projecting $300,000 in revenue.
Year 2: Expanding to 15,000 individual subscribers and 150 institutional licenses, with projected revenue of $1 million.
Year 3: Aiming for 40,000 individual subscribers and 500 institutional licenses, projecting revenue of $3 million.


Today, we’re seeking $500,000 in funding to enhance our product development, expand our marketing efforts, and grow our team. This investment will allow us to refine our technology, reach a larger audience, and solidify our position in the EdTech market. We’re excited about the journey ahead and invite you to join us.

How we built it

Frontend:


Flask web framework with Jinja2 templating
OpenCV for real-time video processing
Custom CSS for responsive interface


Detailed Backend:


We used OpenCV (Open Source Computer Vision Library) to access and process live video frames from the webcam in real time.
We applied Haar Cascade Classifier (a machine learning object detection algorithm) to detect human faces in each frame using grayscale image transformation.
We preprocessed detected face regions by resizing them to 48×48 pixels and normalizing them to [0, 1] scale for input into our CNN (Convolutional Neural Network).
We loaded a pre-trained emotion recognition model (.h5 file from https://github.com/AI-Gajendra/facial-emotion-recognition-higher-accuracy) built with TensorFlow/Keras, trained on the FER-2013 dataset, to classify facial expressions into seven classes (angry, disgust, fear, happy, neutral, sad, surprise).
We used Torch Hub to load YOLOv5s (You Only Look Once version 5 small), a real-time object detection model, to identify objects in the video—specifically looking for the 'cell phone' class.
For each frame, we parsed YOLOv5’s outputs: bounding box coordinates, class index, and confidence scores to draw labeled rectangles around phones if detected above a 0.5 threshold.
We implemented a timing mechanism to detect sustained phone usage by the user; if a phone is seen continuously for ≥1 second (phone_detection_threshold), we trigger a system message.
We integrated Gemini (Google’s multimodal LLM) to dynamically generate natural language prompts like “Put down your phone and get back to studying,” using an internal helper method called generate_gemini_response().
We used Flask’s requests.post() to send Gemini-generated messages to the frontend via a /push_system_message endpoint.
We tracked user emotional state with a deque (double-ended queue) to maintain a rolling history of predictions and identified prolonged negative affect using a threshold over time.
When ≥7 out of the last 20 predictions were from negative emotion classes (angry, disgust, fear), we triggered a Gemini-powered emotional support message with cooldown logic to prevent spamming (emotion_cooldown = 2 seconds).
We implemented a rolling window (comforting_message_window_minutes) to monitor how often emotional support was triggered; if too frequent, we escalated to suggesting a break.
We stored chat interactions and cooldown timestamps to prevent redundant or overwhelming system feedback.
For recording functionality, we used a flag (recording_flag) to start/stop saving webcam frames, stored temporarily in memory as recorded_frames[].
Upon stop, we used OpenCV’s VideoWriter to compile every 4th frame (4× speedup) into a silent MP4 video and saved it to static/recorded.mp4 for download.
Finally, we streamed the annotated frames to the frontend in MJPEG (Motion JPEG) format via Flask’s Response generator, yielding each JPEG-encoded frame with proper multipart headers.


Overview:


Python with TensorFlow/Keras for emotion detection
YOLOv5 for object (phone) detection
Gemini API for conversational AI
MySQL database for user data and video storage


Challenges we ran into

Real-Time Processing:


Balancing accuracy with performance for smooth video feed
Optimizing model inference speeds


Emotion Detection Accuracy:


Fine-tuning the model to reduce false positives
Distinguishing between focused/concentrated vs. frustrated expressions


State Management:


Tracking session state across multiple detection systems
Coordinating between computer vision and chat systems


User Experience:


Making notifications helpful rather than annoying
Designing intuitive controls for recording


Accomplishments that we're proud of

Integrated System:


Successfully combining multiple AI models into a cohesive experience
Creating meaningful interactions between detection systems and chat


Effective Interventions:


Developing context-appropriate responses to different states
Implementing cooldowns to prevent over-notification


Technical Implementation:


Achieving real-time processing with multiple models
Building a secure user system with proper video isolation


User-Centric Design:


Creating a non-intrusive interface that supports focus
Developing useful session review features


What we learned

AI Limitations:


The importance of threshold tuning for practical applications
Challenges of multi-model systems


Technical Insights:


Optimizing Python for real-time applications
Database design for media storage


Project Management:


Coordinating between different components


What's next for EvaStudy

Deployment

Integration:


Calendar integration for study planning
Pomodoro timer integration
Educational platform integrations


Community Features:


Study groups with shared metrics
Friendly competition features


AI Improvements:


Personalized learning from user patterns
Facial Recognition
More natural conversation flows
Customizable AI personality

",https://github.com/samragyee-d/bitcamp-hack,,"Best Use of GenAI in Business - Cloudforce/Microsoft, [MLH] Best Use of Gemini API","python, javascript, html5, css3, flask, tensorflow, keras, pytorch, opencv, bcrypt, yolov5, gemini, mysql",Alvia,Naqvi,alvia.naqvi.08@gmail.com,,https://github.com/samragyee-d/bitcamp-hack,Best Machine Learning Track Hack,Best Moonshot Hack,Best Bitcamp Hack,Prettiest Hack,"","We used GitHub to store our code, and aside from a few merge conflicts, it worked very well.
We also used the Gemini API, and though one team member had difficulty setting it up, another member found the process much easier and enjoyed working with the API.","University of Maryland, College Park",OpenAI and Gemini,"Yes, we used the Gemini API to create a chatbot.",309988651072,"",University of Maryland - College Park,1,juhic3162,Chitkara,juhic3162@gmail.com
Testudo Match ,"",Draft,Pending,Project details,04/13/2025 04:42:10,"Inspiration

We felt like finding the right courses to take as a UMD student was often confusing and frustrating. Reading through the major website, scrolling through the endless prerequisites and requirements. That's why we wanted a tool to help inspire and guide us students on finding the best, tailored, schedule.

What it does

Testudo Match is a website where students can input their schedule wants using natural language (e.g. I want machine learning and database classes), find the most interesting and 

How we built it

Challenges we ran into

Accomplishments that we're proud of

What we learned

What's next for Testudo Match
",https://github.com/yyrichy/scheduleit/tree/main?tab=readme-ov-file#run-locally,,[MLH] Best Use of Gemini API,next.js,Zyad,Khan,zhk6319@gmail.com,,https://github.com/yyrichy/scheduleit/tree/main?tab=readme-ov-file#run-locally,Best First-Time Hack,Best Razzle Dazzle Hack,Best Bitcamp Hack,Prettiest Hack,Hack That Made You Smile,"","University of Maryland, College Park","OpenAI, Anthropic, Hugging Face, and Gemini",Yes,GEMINI_API_KEY=AIzaSyBVDLw8V2zzlmqosSNC_x6vN2u6fiJNGlk,"","University of Maryland - College Park, University of Maryland - University College, Richard Montgomery",2,Richard,Yin,ryin1@terpmail.umd.edu,Farhan,Ahmed,farhanahmed75017@gmail.com
Crossing Counties,https://bitcamp2025.devpost.com/submissions/643017-crossing-counties,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 04:44:05,"Inspiration

With housing shortages, rising living costs, and an uncertain job market, relocating is challenging. However, access to reliable insights and information can be a valuable tool in reducing stress.

What it does

Crossing Counties leverages frequently updated federal government data to provide county-by-county insights, helping individuals make informed decisions when relocating.

How we built it

Front end: Streamlit renders data, charts, managers user inputs while being very quick to implement  

Analysis: Pandas is a tried and tested data analytics tool that integrates with nearly every python api or library

Challenges we ran into

Limited time, lack of familiarity with front-end tools, rudimentary data analysis.

Accomplishments that we're proud of

Getting a working proof of concept out, solo, under a massive time crunch.

What we learned

Make a detailed plan ahead of time, be ready to burn the midnight oil and never lose focus of the goal.

What's next for Crossing Counties

Gather more data, use advanced analytics and incorporate more technologies to help users make more informed decisions. 
",https://github.com/sdhar150/Crossing-Counties,,"","streamlit, pandas, python",sdhar150,Dhar,sohamdhar150@gmail.com,,https://github.com/sdhar150/Crossing-Counties,Best First-Time Hack,Best Social Good Hack,Best UI/UX Hack,Hack That Made You Smile,MOST LIT HACK,"The most fascinating experience was interacting with sponsors, their representatives and learning with my fellow hackers in workshops. In particular, the app dev workshop was helpful where I learnt some MongoDB, and Fast API.","University of Maryland, College Park",OpenAI and Gemini,"No, i did not",No,"",University of Maryland - College Park,0
Untitled,"",Draft,Pending,Manage team,04/13/2025 04:45:43,"","",,"","",Caleb,Holland,hollandcw3@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,0
Untitled,"",Draft,Pending,Manage team,04/13/2025 04:49:37,"","",,"","",Kriti,Shahi,kritishahi66@gmail.com,,"","","","","","","","","","","","","",0
Roomlytics,https://bitcamp2025.devpost.com/submissions/643028-roomlytics,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 04:50:48,"Roomlytics

Roomlytics was inspired by our own college experiences, where we realized how challenging it can be to live with new roommates. From loud snorers and messy kitchens to mismatched sleep schedules, we saw firsthand how difficult cohabitation can be without real compatibility. Traditional roommate matching often only considers surface-level traits or availability, overlooking deeper lifestyle alignment. 

With Roomlytics, we wanted to build a more thoughtful, data-driven solution that analyzes user preferences across key lifestyle dimensions to provide insight and improve harmony in shared living spaces.



How It Works

Our project allows users to record their lifestyle preferences and housing criteria through a brief questionnaire. Once submitted, the system:


Compares their responses with other profiles
Calculates compatibility rates
Generates a radar chart highlighting areas of compatibility and potential friction


This makes it easier to evaluate shared living potential with another person.



Tech Stack

We built Roomlytics using:


Flask (Python) for the backend
HTML/CSS with Jinja templating for the frontend
Matplotlib for radar chart generation


Radar charts are encoded using base64 and rendered dynamically in HTML templates.



Challenges


Designing radar charts that were visually clean despite long axis labels
Mapping quiz responses accurately to profiles in the database
Embedding Matplotlib images using in-memory buffers and encoding
Handling session data securely and restricting access to result pages




Accomplishments


Built a full-stack web app from scratch
Created clear, readable radar charts to represent multidimensional data
Implemented profile storage, quiz logic, and dynamic matching
Learned how to pass and display data efficiently between backend and frontend




Lessons Learned


Structuring Flask apps effectively
Visualizing data with Matplotlib
Using Flask sessions and secure routing
Importance of UI/UX decisions for readability and clarity




Future Plans


Add a full roommate matching algorithm
Allow real user profiles with photos and bios
Implement messaging features
Build an admin dashboard for user data analysis
Create a mobile-friendly version of the site




Tools Used


Flask
Matplotlib
HTML/CSS  (TailwindCSS)
Jinja2




We came up with an algorithm to sort out different compatibility categories.
def generate_personalized_summary(user, other, questions):
    diffs = [abs(u - o) for u, o in zip(user, other)]
    paired_scores = list(zip(questions, user, other, diffs))
    paired_scores.sort(key=lambda x: x[3], reverse=True)

    summary = """"

    # Top 3 biggest differences
    biggest_diffs = paired_scores[:3]
    if any(d[3] >= 4 for d in biggest_diffs):
        summary += ""You and your potential roommate have notable differences in:\n""
        for q, u, o, d in biggest_diffs:
            summary += f""- **{q}**: You rated {u}, they rated {o}.\n""
    else:
        summary += ""You and your potential roommate don’t have any major red flags in your preferences.\n""

    # Top 3 strongest alignments
    strongest_alignments = [x for x in paired_scores if x[3] <= 2][:3]
    if strongest_alignments:
        summary += ""\nYou’re especially well-aligned on:\n""
        for q, u, o, d in strongest_alignments:
            summary += f""- **{q}**: You rated {u}, they rated {o}.\n""

    # Overall take
    total_diff = sum(diffs)
    if total_diff <= 15:
        summary += ""\n🌟 You’re highly compatible overall — this could be a great match!""
    elif total_diff <= 25:
        summary += ""\n⚖️ You have a few areas to work through, but nothing major. Communication is key!""
    else:
        summary += ""\n🚨 There are some significant lifestyle differences — a good conversation beforehand is strongly recommended.""

    return summary
","",https://youtu.be/pYBuJ8Dcl2k,"","flask, python, html, java, css",Minkyu,Kim,minkyukim003@gmail.com,,https://github.com/minkyukim003/roommate-comaptibility,Best First-Time Hack,Best Social Good Hack,Best Razzle Dazzle Hack,"","","GitHub is a streamlined version control system with many options. 
Flask is a smooth operating Python web development framework with many capabilities.",William & Mary,OpenAI,N/A,N/A,"",College of William and Mary,1,Alex,Golden,amgolden@wm.edu
Untitled,"",Draft,Pending,Manage team,04/13/2025 04:53:13,"","",,"","",Alex,Golden,amgolden@wm.edu,,"","","","","","","","","","","","",College of William and Mary,0
Koalafied,https://bitcamp2025.devpost.com/submissions/643031-koalafied,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 04:54:58,"Inspiration

Applying for internships and jobs can be tedious and boring. In a competitive job market, students like us are rushing to apply to hundreds of internships without a proper way to keep track of them. Many have turned to the bland Excel spreadsheets to mindlessly track their applications. This leaves students like us to be disorganized and bored when it comes to applying to jobs. We were inspired by idle games like Neko Atsume, and decided to create a similar mechanism for gamifying job applications.

What it does

Enter Koalafied, a gamified way to keep track of your job applications and important information like interviews and offers. As you work towards building a future for yourself, you also work towards building a community of cutesy and unique Koala's deep within a quiet forest. 

Users can sign up for an account to rise up the leaderboards to a wide range of koala-ity career choices and unmatched fame. They can collect koalas by applying to jobs, interviewing, and receiving offers. Track status updates on their applications and keep track of important interviews or deadlines via a notepad. Most importantly, they can play with their koalas!

How we built it

Koalafied is built on the MERN stack, meaning we used MongoDB, Express, React, and Node.js for our application. 

We first planned out our application by storyboarding and creating mock-ups for interfaces and then collaborated on setting up our database on MongoDB, discussing the structure of our data.

We used Node and Express in combination with MongoDB to create users, an encrypted login system with JWT sessions, and store users' applications and of course those CUTE KOALAS. We found using MongoDB to be very efficient and easy to get started with. It made developing our backend a lot easier than we expected.

For our frontend, we used React and Material UI to build a login form, a leaderboard, and most importantly the dashboard, which displays all of a user's application statuses, notes, and user statistics (applications, interviews, etc.). And don't forget about those CUTE KOALAS playing around on the bottom of the screen! 

Finally, we used Photoshop to edit and create graphics for our application, and Postman to test our backend API routes.

Challenges we ran into


Building animations With Pure React: we found it difficult to set up the initial function structure that would allow for smooth animations and movement physics. There was a steep learning curve, but after gaining a better understanding of React's page update structure, we were able to create complex movement for our koalas.
Creating Sessions with JSON Web Tokens (JWT): Allowing users to login, stay logged in, and also be restricted from accessing protect routes without an account proved to be a struggle. This was one of the more difficult and annoying challenges to deal with, not through just it's difficulty, but also through its recurring nature. We learned that in times where you find yourself getting annoyed with problems like these it's important to stay calm and work together.
As Always... Styling Everything: CSS is hard y'all. But we did it :)


What we learned

Evan Masiello, a second-time hackathon participant, first honed his existing skill for building react pages and creating backend connections when developing the dashboard and Express endpoints. Something that he did not expect to end up developing was the animation for the Koalas, but he found it to be a very fun experience. Evan enjoyed challenging himself by thinking of new ways the Koalas could move or be interacted with and figuring out how to implement them. He definitely strengthened his understanding and skills related to React.js through this process and also gained valuable experience using MongoDB for more intricate data than in the past.

Benjamin Johnson learned deeply about how react works and react syntax. A step up from what he was normally accustomed to, which was just HTML, CSS, and JavaScript for building a webpage. Ben expanded on his knowledge of MongoDB which he had minimal knowledge of before Bitcamp to implement the leaderboard functionality.

Caleb Holland, a second-time Bitcamp participant traveled from his usual comfort as a frontend developer to explore the backend. He learned how to create an encrypted user system using Node and MongoDB, create sessions with JSON Web Tokens (JWT), and most importantly learned how to have a good time with his pals. In addition, he continued to hone his sense of design by creating a majority of the graphics for the application, and by building some of the components used on the login and dashboard page.  

What's next for Koalafied

Originally, we wanted to use the Gemini API to generate the sprites for the koalas and create endless quests for the user. Although this was beyond the scope of this hackathon, this is something we'd like to implement as we improve upon the project in the future.

We also had aspirations to have this tool auto track your applications after applying for jobs, either through an extension, or an email connection. This would make it much easier for users to organize their applications, as this removes the need to manually add in their applications.

In the end, it was a tree-at to create Koalafied. We learned a lot, and we look forward to expanding upon it in the future!
",https://koalafied.tech/,https://www.youtube.com/shorts/hJH72qWVECM?si=Phfabg3J_YNdK2LU,"Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best Use of MongoDB Atlas","react, express.js, node.js, mongodb, postman, materialui, photoshop",Caleb,Holland,hollandcw3@gmail.com,,https://github.com/holland-cw3/koala-fied,Best Gamification Hack,Best Bitcamp Hack,Best Razzle Dazzle Hack,Hack That Made You Smile,Prettiest Hack,"We used GitHub, MongoDB, and Vercel. We had very good experiences with all three of them, and plan on continuing to work with them in the future.","University of Maryland, College Park",OpenAI,N/A,N/A,koalafied.tech,"University of Maryland - College Park, University of Maryland - University College",2,BenjaminJohnson360,Johnson,benjaminjohnson3318@gmail.com,Evan,Masiello,evanmasiello@gmail.com
Cinder,https://bitcamp2025.devpost.com/submissions/643032-cinder,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 04:55:57,"We were tired of scrolling through endless pages on Testudo, trying to scrape together a schedule of courses. 
The solution? Cinder.
WIth Cinder, all you have to do is upload your degree audit and our AI model will suggest the best courses for you to add to your next semester's course load. Swipe left or right on courses based of professor grade data to build the perfect schedule!
Long gone are the days of mindless Testudo browsing, use Cinder to plan your next semester's courses now!
","https://bitcamp-git-main-abhyuday-srivatsas-projects.vercel.app/, https://github.com/abhyuday-srivatsa/bitcamp",,[MLH] Best Use of Gemini API,"python, flask, html, css, javascript, gemini, api",Abhyuday,Srivatsa,abhyuday.srivatsa@gmail.com,,https://github.com/abhyuday-srivatsa/bitcamp/tree/main,Best First-Time Hack,"","",Hack That Made You Smile,"","We put together our first Flask server, using Python, JavaScript, and HTML to define our logic. We also used CSS to bring color to our website.",University of Maryland: College Park,Gemini,"Yes, we used a Gemini API to give course recommendations for schedule building",591141784686,"",University of Maryland - College Park,2,Jonah,Plotkin,jplotkin05@gmail.com,Logan,Cyford,logancyford@gmail.com
Gather,https://bitcamp2025.devpost.com/submissions/643033-gather,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 04:56:03,"Inspiration

We’ve all been there—wanting to go to a concert for a niche artist or try out an activity that none of our friends are into. It’s frustrating when you’re excited about something but can’t find anyone to join you. Or maybe you’re in a class where you don’t know anyone, and you study better with a group. Usually, the only option is to post on social media and hope someone’s interested, which can be time-consuming and hit-or-miss. We built Gather to bridge that gap—making it easy to connect with others on campus who share your interests, whether it's music, hobbies, or forming a study group—so no one has to miss out on what they love or need.

What it does

Gather is an app designed to make it easier for students to find and connect through events happening both on and around campus. Whether it’s a study group, a casual meetup, or a local concert, students can browse and RSVP to events that match their interests. Users can also create their own events—like jam sessions, movie nights, or tutoring meetups—and invite others to join. For academic settings, the option to create private events is especially useful for instructors or TAs who want a simple, organized way to host office hours, check-ins, or focused help sessions.

How we built it

The app was built from the ground up using React Native for a seamless mobile experience across platforms. For authentication and data storage, we relied on Supabase and SQL, which allowed us to manage user accounts and event information efficiently. Event listings were pulled from multiple sources, including the UMD Events page, the Ticketmaster API, and user-generated content. For the map discovery page, we utilized React Native Maps. To personalize the experience, we integrated the Google Gemini API to scrape and gather events from TerpLink.

Challenges we ran into

One of our biggest challenges was deciding on the direction and target audience for our app. We had a lot of different ideas, and it took hours of discussion and iteration to align on a shared vision. Eventually, we decided to focus Gather on college students and young adults in their early 20s, aiming to create something that genuinely fits their social and academic needs. On the technical side, we ran into issues like image upload errors, which slowed us down at times—but through collaboration and persistence, we worked through them together as a team.

Accomplishments that we're proud of

We’re especially proud of our contacts feature, where you can add contacts to a group to invite people by bulk to an event. Combined with the map discovery system, they complement each other perfectly to elevate the user experience. The recommendation engine analyzes user preferences and suggests events tailored to their interests, making it easier for them to discover activities they'll enjoy. Meanwhile, the map discovery system provides a dynamic, visual way to explore both on-campus and nearby off-campus events, helping users easily navigate and RSVP to events that catch their eye. Together, these features create a more personalized, intuitive experience that encourages users to engage with the app and connect with others around them.

What we learned

We are really glad that we were able to tackle learning a new framework successfully as half of our team did not have a lot of experience with React Native or mobile app development in general. 

What's next for Gather

One idea we’re exploring is integrating chatrooms directly within event listings, so users can easily communicate, ask questions, and build excitement leading up to the event. We’re also working on implementing our recommendation engine, which uses onboarding survey responses to offer personalized event suggestions and connect users with others who share similar interests. These additions would help make the app more interactive, social, and tailored to each user’s experience.
",https://github.com/aadikrishna04/FriendFinder,,"Best Hack Promoting Public Health - Bloomberg, Best Financial Hack - Capital One, Best Digital Forensics Related Hack - Cipher Tech Solutions, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best AI Application Built with Cloudflare, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","javascript, react-native, gemini, supabase, sql, apple-maps",Aadi,Krishna,aadikrishna04@gmail.com,,https://github.com/aadikrishna04/FriendFinder,Best App Dev Track Hack,Best Social Good Hack,Best Moonshot Hack,Hack That Made You Smile,Prettiest Hack,"Google Gemini was really useful to us for our recommendation engine, and we would highly recommend using it in future hackathons. GitHub was also key for us to collaborate on code together as well as version control.",University of Maryland,Gemini,N/A,851914216504,"","University of Maryland - College Park, Union Graduate College",3,Sidharth,Ponram,sidponram@gmail.com,Luodi,Wang,luodiw9@gmail.com,Anushk,Pokharna,pokharnaanushk@gmail.com
Untitled,"",Draft,Pending,Manage team,04/13/2025 04:58:22,"","",,"","",Stanley,Xu,xustanley276@gmail.com,,"","","","","","","","","","","","",Georgia Institute of Technology - Main Campus,1,Aaryan,Tomar,aaryantomar12@gmail.com
VisuWorld,https://bitcamp2025.devpost.com/submissions/643035-visuworld,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 05:00:54,"🌎 What Were We Thinking?

We were hard at work figuring out what to build for Bitcamp, and after spending nearly a third of the competition in the ideation stage, we came up with VisuWorld. After combining some of the cool new tech we've explored - GLSL / Graphics, Google Gemini, and Voice to Text - we built VisuWorld!

🤔 What is VisuWorld?

Embarking on your own VisuWorld exploration begins with a speech to text prompt to Google's Gemini API, which generates GLSL code to represent just about anything that you can articulate out loud. Equipped with over 25,000+ graphics shader snippets in a RAG database, we've been able to see some truly impressive 3D visual landscapes over the course of this weekend.

⚡ Visualize This:

The pipeline for VisuWorld is fairly simple, and can be broken down into 3 main steps:


First, we needed to get our data formatted. We scraped upwards of 34,000 code snippets from Shadertoy, a public site with general API access. We then embedded it all using OpenAI's embedding models to a RAG database using ChromaDB to make Gemini even more powerful.
Using the native Google Chrome Web Speech to Text, we then let the user speak their thoughts aloud, separating them from manual/dexterous limitations, and truly letting their imagination take over. Once done with a few commands, Google Gemini (both 2.0 Flash, and 2.5 Pro) utilized our RAG knowledge system along with its own training data to generate GLSL code. Users can generate new VisuWorlds, or use their current one as context to Gemini to explore every avenue of their imaginary worlds.
Once the GLSL code was generated, we utilized React & Next.js to display a seamless frontend, paired with Three.js to help us handle the WebGL GLSL shaders with ease. We also used React to make the application more accessible over a web interface, and built a really awesome Gallery for everyone to display how far their imagination can take them.


⛰️ Jagged Cliffs...

Working with Gemini on GLSL code was really difficult at first, and no amount of prompt engineering was going to save us from diving deeper. This was all of our first times using RAG to empower LLMs, but we are sure glad we did. Even from only 500 code samples, having an information system for querying made Gemini MUCH better at making worlds that will make your jaw drop.

🏆 Our Victories

Three Dimensional generation is a hot topic in the AI space, with 2D to 3D diffusion models being a popular choice of exploration. However, we haven't seen many things quite like VisuWorld going from text to 3D landscapes. The ""eureka"" moment of realizing that graphics shaders were a valid surface area of innovation was thrilling to stumble across.

🚀 Unexpected Discoveries

A whole lot about graphics. Like, a LOT. Who knew how much math went into these things, and just how intensive inefficient graphics can be. Our laptop fans blew louder than any of us imagined they could.

🌅 New Horizons

We want to spend time perfectly optimizing every piece of the VisuWorld stack in hopes of deploying it for real. Most of the intense computing is offloaded to the web client, so we can (and will) be able to hold many users at a time. We want to share this experience with others, because it really is that cool to look at and a great time to explore.
",https://github.com/Bitcamp-2025-RLD/visuworld,https://youtu.be/HXS_7HzJmYo,"Best Web Hack Using React - Peraton, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","react, three.js, webgl, glsl, graphics, google-gemini, gemini, google-web-speech-api, tailwind, next",Dylan,Vidal,dylanvidal1204@gmail.com,,https://github.com/Bitcamp-2025-RLD/visuworld,Best Machine Learning Track Hack,Best Bitcamp Hack,Best Razzle Dazzle Hack,Prettiest Hack,Hack That Made You Smile,"We used all sorts of technology  in the Web Development and AI ecosystem - GitHub, ChromaDB, OpenAI, Google Gemini, React, Next.js, Tailwind, Three.js! All were great.",University of Central Florida,Gemini,"Yes, we used Gemini to generate GLSL code to be parsed by the React/Three.js frontend, powered by a RAG database of 25,000+ code snippets.",362971969910,"",University of Central Florida,1,Leonard,Gofman,leonardgofman23@gmail.com
Forens.tech,"",Draft,Pending,Project overview,04/13/2025 05:08:20,"Inspiration

Cybersecurity—and especially the niche of Digital Forensics—has long been associated with notoriously unapproachable user interfaces, difficult user experiences, and confusing application flows. From bloated, messy logs to outdated design trends, there are plenty of reasons for this reputation. As we discussed problems we were passionate about solving, a common thread emerged from our diverse professional and technical backgrounds. Many mission-critical tools in Digital Forensics are nearly impossible for amateur or untrained users to navigate, too intimidating to encourage broader adoption, and simply unenjoyable to use. By combining our team’s strengths in graphical design, user experience, and cybersecurity engineering, we created Forens.tech—an application designed to change that narrative.

What it does

Forens.tech takes an innovative approach to common digital forensics techniques—combining a fresh, well-planned, and deliberate user experience with reliable methods to quickly parse and display memory dump data. Forens.tech is a web application that runs locally on a customer’s system, alongside any memory dump files they wish to analyze. It offers a straightforward, easy-to-follow selection process for choosing the types of scans to run on the selected file, using those inputs to execute Volatility3 for analysis. The application outputs a consistently clean and structured JSON result, which is then passed to a Gemini-powered LLM agent for summarization and deeper insight. This includes both objective data and a cleaned-up version of the command output, all presented clearly to the end user. Using Volatility3, Forens.tech can offer options for analysis on a system’s running processes, conducting a file scan of the computer, getting OS information, viewing network connections, and returning a list of executed system commands. The goal is to simplify memory dump analysis, making it more accessible to a wider audience of law enforcement and blue team specialists. Along with this focus, utilizing tools like Volatility3, Forens.tech can offer a wide range of expanding options to analyze data from the position of beginners to experienced professionals.

How we built it

Forens.tech’s front end was built using ReactJS and custom CSS after analyzing multiple design features with prototypes using Figma. As a team, we ideated  We wanted to Volatility3 is called through a Python script, which pushes output through fastapi to serve mission-critical information to the end user. 

Challenges we ran into

One major challenge was using Volatility and learning to work with memory dumps, both of which are things none of our team members had prior experience with. Luckily, both the documentation and usage are relatively straightforward and well-maintained, allowing for the creation of our automated scripts to come after a few hours of initial research.

Accomplishments that we're proud of

Our team is incredibly proud of how well the interface came out for Forens.tech. Although it was difficult at times, ensuring to build with a focus on the user interface and being deliberate with our design decisions helped us to create a good-looking and good-performing web application..

What we learned

In addition to deepening our understanding of memory dumps and how tools like Volatility3 support cyber professionals in incident response, we also gained valuable insight into the importance of planning and preparation—and the significant impact they have on the quality of an application’s visual design and user interface.

What's next for Forens.tech

Expanding support for further and more advanced Volatillity commands as well as support for other digital forensics tools and combine this extra information with even cleaner data logging methods.
","",,"Best Digital Forensics Related Hack - Cipher Tech Solutions, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best Use of Gemini API","",Anthony,Perry,anthonyperry904@gmail.com,,"",Best UI/UX Hack,"","",Prettiest Hack,"",Mentors were incredibly helpful with understanding how to leverage the event's available tech for our solution,George Mason University,Gemini,"Yes, we used Gemini so help summarize and analyze memory dumps and Volatility3 command outputs which can help blue team cyber professionals to identify critical moments in investigation for incident response.",331537607378,forens.tech,"George Mason University, Virginia Tech",2,Lexy,Altieri,lexyaltieri@gmail.com,Daniel,Horvath,fpcyberstudent@gmail.com
Steg Detector,https://bitcamp2025.devpost.com/submissions/643043-steg-detector,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 05:13:30,"Inspiration

3 years ago, I took a cybersecurity class in high school where I first learned about steganography. It was fascinating. Like I genuinely hadn't enjoyed anything in that class up to that point, but I enjoyed that topic so much I ended up building a project where you could encode and decode messages from audio files using steganography. While it was pretty basic and harmless, it was cool to work with files on the binary level for the first time in my life.

That being said, I'm a machine learning guy at heart, so when I went to UMD, that's the path I signed up for and got into. However, fast forwarding to this semester, I took a class on binary exploitation, which honestly rekindled my passion for working with binary. I'm now a cybersecurity minor in addition to my machine learning major, and so I thought it would be a good idea to combine ML with cyber in my Bitcamp 2025 project. 

All of these thoughts kind of combined in my mind literally one hour before the opening ceremony, and I instantly called my best friend to ask him if I was being an idiot or if this would be a good idea. He gave me some encouraging words, and the rest is history, I suppose.

What it does

Steganography is the process of basically taking the least significant bits of information and writing over them to secretly encode your own information in files. While small changes to information can cause huge differences in many file types, files like images and videos are perfect for this type of encrypting technique, as the change in what you see visually is minuscule. This fact alone makes these attacks extremely dangerous as they have no visible trace to the human eye. My solution is to try to use an AI model that will read in the least significant bytes of an image and predict if there is some sort of malicious payload in those least significant bytes.

TLDR: The program takes in a PNG image (as steganographic attacks are only used on image types without compression) and uses a sequential neural networking model to detect whether that image has a payload injected into it via steganography.

How we built it

I built it by first getting data from a dataset on Kaggle containing images with and without steganographic injected payloads in them. I read the bytes from the images, used bitwise operations to get the least significant bits, and grouped them into a sequence of bytes. I then converted those bytes into floats between 0.0 and 1.0 so I could feed them into a sequential neural network model to find patterns in the least significant bit data of images that would correspond to whether or not they contained steganography.

Next, I built a Flask API using the model so that users could interact with the model and find out if their images had steganography or not. I coupled this with a simple front-end in React where users could upload a file, click a button, and then interact with the model and find out whether or not their image has steganographic binary in it.

Finally, I built the readme for the project.

Challenges we ran into

Some challenges I ran into included converting the least significant bits into something that could be used by a machine learning model. I initially had read and stored the least significant bytes as a string, but not only was that inefficient, but it also made it very difficult to use with machine learning models. After some research, I discovered that converting the raw bytes to float values between 0 and 1 and using a sequential neural networking model would be the best way to use the raw bytes in the way I intended (which is for them to be read in order and see if that sequence is malicious).

Another challenge I ran into was the physical limitations of my laptop/optimization of my code. I initially intended to use the entire dataset size to train, especially since I was using a neural network and neural networks need a large amount of data, but my laptop could only handle a couple of hundred images at first. After a lot of optimization using vector multiplication, I was able to increase my dataset size to a couple thousand images.

Accomplishments that we're proud of

I'm proud I completed the project start to finish, and I'm proud that I decided to go do something out of my comfort zone. This was my first time ever working with a neural network, so I'm glad I tried and learned something new. I'm also glad I didn't use my laptop as an excuse to just give up on the project and instead sought ways to optimize my code so I could get as much data as possible for my neural network.

What we learned

I learned a lot about how neural networks work. I learned about things like weights, biases, epochs, and also the types of neural networks. I also learned how to deal with larger datasets in the future (to start off with small sections of that data, optimize your code, and then try to get more at a time). I definitely made the mistake of trying to get all the data at once, and that led to my computer crashing a lot. So that's a good lesson I'll take with me into the future.

What's next for Steg Detector

I plan to improve the accuracy of my model by getting more data from the dataset, a little at a time. I also plan to try to address the issues of false negatives in my project by adjusting the prediction threshold at which I classify something as either steganographically injected or clean. 

Additionally, I want to try to get random images from online as ""clean"" images too, so that my dataset for ""clean"" images has more variance and mimics the variance in real-world internet images better, as the current dataset has no grayscale images and has the same pixel size (512 by 512). I would probably do the same for the ""steganographic"" images too if I could find another dataset.

Finally, in the far future, I would like to expand this steganographic check to audio and possibly video forms of media too.
",https://github.com/Meherzan-Gai/steganography-detector,https://youtu.be/7osQRwsid5k,"","python, jupyternotebook, react, css, html, javascript",Meherzan,Gai,meherzan.gai@gmail.com,,https://github.com/Meherzan-Gai/steganography-detector,Best Cybersecurity Track Hack,Best First-Time Hack,Best Social Good Hack,MOST LIT HACK,Hack That Made You Smile,"I had a nice experience this weekend. I didn't really interact with much technology besides what I used to build my program. I didn't do anything with MongoDB or Twilio, but I did use Github for version control and it went pretty smoothly. I used a bit of OpenAI to help with speeding up the front-end building (basic like input button and form request in react).",University of Maryland - College Park,OpenAI,I used ChatGPT to help speed up the front-end building/styling process (getting it to like put a basic input button and form request stuff to the API from Flask). I also used it to give me ideas to help debug errors and also help figure out the best model for me to use for machine learning on the data. It also helped me understand how neural networks work.,n/a,stegDetector,University of Maryland - College Park,0
Mammoth Maze,https://bitcamp2025.devpost.com/submissions/643045-mammoth-maze,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 05:18:59,"Inspiration

Our project was inspired by the old DOOM games and the prehistoric theme of Bitcamp 2025.

What it does

Mammoth Maze renders a 2D maze game in 3D using raycasting. The player is placed in a maze and is challenged to reach the opposite corner. Standing in their way is a mammoth piloted by A* pathfinding. The maze itself is generated randomly with an algorithm, and includes a portal with a rendered texture that brings the player to the next maze. 

How we built it

Mammoth Maze does not use any 3D models or tools, rather the graphics are displayed entirely with Java AWT. Many rays are aimed from the player outwards at varying angles, and the distance between the player and the wall that they hit determines the position, length, and brightness of the wall segment to be drawn. 

Challenges we ran into

There were problems with the perspective of the 3D world as well as a strange fish-eye effect that we were able to address with better calculations. We also had trouble implementing sound effects as none of us had any experience with doing so.

Accomplishments that we're proud of

We are proud of implementing a texture for the portal and of including sound effects. These were both ""extra"" features that we were not sure we would have time to implement, with the former being particularly challenging.

What we learned

We developed a greater understanding for how 2D graphics are used to create an illusion of 3D space, which is the case for all 3D graphics on a computer screen.

What's next for Mammoth Maze

Future ideas include:


True 3D Space (Stairs, multiple floors of a maze)
Greater differences between levels
Items to collect in the maze (keys to locked doors, coins)
A way to stop/distract the mammoth

",https://github.com/8tsmith11/mammoth-maze,https://youtu.be/JtKaNqhJbWE,"",java,Trevor,Smith,8tsmith11@gmail.com,,https://github.com/8tsmith11/mammoth-maze,Best First-Time Hack,Best Razzle Dazzle Hack,Best Bitcamp Hack,Hack That Made You Smile,"","We used Eclipse, Visual Studio Code, and GitHub to develop our project. Everything worked smoothly as expected.","University of Maryland, College Park",OpenAI,N/A,N/A,"",University of Maryland - College Park,0
Airwaves,https://bitcamp2025.devpost.com/submissions/643050-airwaves,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 05:22:59,"What it does

Airwaves is a rhythm-based browser game that uses your webcam to detect hand movements and translate them into mouse clicks, scrolls, and gestures. The player hits on-screen targets to the beat—entirely hands-free. Behind the scenes, we interpret specific hand poses and motions in real time, making the game both a fun challenge and a light physical therapy exercise to improve hand mobility, coordination, and range of motion.

How we built it

We used OpenCV and TensorFlow.js to detect and track hand landmarks from a webcam feed. These gestures were mapped to digital inputs like clicking and scrolling. The front end was built with React, and we spent time fine-tuning animations and transitions to create an immersive rhythm experience. Our focus wasn’t just gameplay—we also designed each motion to feel intentional and therapeutic, mimicking movements that would benefit hand mobility exercises.

Challenges we ran into

Achieving consistent hand detection across different lighting conditions
Filtering out accidental movements while keeping input responsive
Calibrating gestures to ensure therapeutic value without causing fatigue
Balancing accessibility and rhythm mechanics without one compromising the other

Accomplishments that we're proud of

Developed a full working prototype of a webcam-controlled rhythm game in under 36 hours
Created a gesture-to-input system that could have real therapeutic benefits
Designed an experience that’s fun for everyone, but especially impactful for users with limited mobility or in recovery

What we learned

How powerful camera-based gesture detection can be when paired with accessible design
The challenges of smoothing noisy sensor data in real time
How to design interactive systems that are fun and functional
That games can be more than games—they can be tools for well-being

What's next for Airwaves

Develop new levels with progressively challenging therapeutic motions
Consult with physical therapists to refine motion sets for rehabilitation use
Add hand tracking analytics to help users (and therapists) monitor progress
Integrate accessibility settings like motion sensitivity, tempo adjustment, and guided movement training
Explore partnerships with healthcare and rehab tech platforms
",https://github.com/clxxiii/air-waves,,Best Web Hack Using React - Peraton,"opencv, react, tensorflow, typescript, css",Joseph,Vega,jvega2@oswego.edu,,https://github.com/clxxiii/air-waves,Best Gamification Hack,Best Bitcamp Hack,Best Social Good Hack,Hack That Made You Smile,Prettiest Hack,"This weekend, we primarily worked with React, OpenCV, and TensorFlow. React made it easy to structure our game and manage state transitions across different screens. OpenCV helped us understand motion patterns and camera input handling, while TensorFlow powered real-time hand gesture detection right in the browser. We were impressed by how lightweight and fast TensorFlow ran even without a backend. Our biggest challenge was smoothing noisy input data, but the flexibility of these tools made it possible to iterate quickly. Overall, the tech stack was solid and surprisingly beginner-friendly for computer vision work.",State University of New York at Oswego,I did not use GenAI,N/A,N/A,"",SUNY College at Oswego,2,Samantha,Brown,sbrown27@oswego.edu,Eli,Fereira,earthnoartisjusteh@gmail.com
JeffAI,https://bitcamp2025.devpost.com/submissions/643052-jeffai,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 05:26:29,"Inspiration

Our inspiration was to give people a tool to complete their simple web related tasks using the power of our AI agent.

What it does

Our project allows you to give an AI agent web related tasks that will be solved using our proprietary AI agent. 
Sample inputs: 
""Plan me a flight from where I am to Japan"",
""Find the distance between UMD Armory and a pizza restaurant""

How we built it

Built using Playright and Google's Gemini API
",https://github.com/daven-c/ClosedManus/tree/Stable-JS,,"Best Use of GenAI in Business - Cloudforce/Microsoft, [MLH] Best Use of Gemini API","playright, llm",Justin,Chen,erthy8@gmail.com,,https://github.com/daven-c/ClosedManus,Best Machine Learning Track Hack,Best Moonshot Hack,Best Social Good Hack,"","",We used GitHub and Gemini API,University of Maryland,Gemini,"Yes, we enabled Gemini by providing web browsing tools to allow the model to autonomously complete task as well as query user inputs.",1072243420090,"","Oakton High School, University of Virginia",2,Ben,szeto,benkszeto@gmail.com,Daven,C,dchang1iz@gmail.com
Knowledge Quest,"",Draft,Pending,Project overview,04/13/2025 05:26:54,"Inspiration

We created this project because as students, we wanted an easier way to study and review material. We figured the best way to do that is to turn it into a game.

What it does

Our game 

How we built it

Challenges we ran into

Accomplishments that we're proud of

What we learned

What's next for Knowledge Quest
","",,[MLH] Best Use of Gemini API,"",Karen,Li,mqzyiv@gmail.com,,https://github.com/ilydoa/knowledge-quest,Best Gamification Hack,Best UI/UX Hack,Best First-Time Hack,"","","","","","",35829611565,"","",0
The Tomato Trade,https://bitcamp2025.devpost.com/submissions/643056-the-tomato-trade,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 05:27:57,"Inspiration

We were inspired by the idea that so many people grow more food than they can use, especially neighborhood gardeners with seasonal crops like tomatoes, zucchinis, or herbs. At the same time, others nearby might be looking for those exact items. Instead of letting extra produce go to waste or relying on grocery stores, we wanted to create a platform that lets neighbors connect and trade directly.

The concept combines sustainability, local food access, and community building. We also drew from personal experiences of seeing surplus food in our own gardens or communities, and wishing there were a simple way to share it.

What it does

Tomato Trade is a bartering platform that allows users to list, browse, and trade homegrown produce with others nearby. Users can:
-Create posts with item details, quantity, location, prices, photos, and more
-View listings from other gardeners based on location
-Search or filter by listing category (selling or requesting) or item type (e.g. tomatoes, herbs, fruits)
-Connect with other users to arrange a trade

The app is designed to make it easy for people to reduce waste, access fresh food, and build local connections through a sustainable, community-driven system.

How we built it

-Figma to plan out the design
-Front-end using react, html, css, typescript
-Back-end using MongoDB, FastAPI, Python

Challenges we ran into

This was our first hackathon, so one of the biggest challenges was figuring things out as we went. We had to teach ourselves how to build an entire full-stack application, including:
-Setting up a backend from scratch with MongoDB and FastAPI
-Debugging unfamiliar errors while working under time pressure

Despite the learning curve, we pushed through by dividing tasks, staying curious, and helping each other troubleshoot. It was intense, but incredibly rewarding!

Accomplishments that we're proud of

-Built a full-stack application from scratch in under 36 hours including setting up both the backend and frontend without prior experience
-Successfully integrated MongoDB for storing and retrieving user data, learning how to structure collections and handle queries on the fly
-Overcame the challenge of integrating backend APIs with the frontend to ensure seamless user interactions and data flow
-Fostered strong teamwork and communication skills, making the project come together despite the steep learning curve

What we learned

-How to set up a full-stack web application using React for the frontend and MongoDB/FastAPI for the backend
-How to integrate third-party APIs, like Google Sign In, to enhance user experience with location-based features
-The importance of clean code organization and version control, especially when collaborating under time constraints
-How to troubleshoot bugs efficiently, even when the errors were completely new to us, for example merge errors caused by everyone working on the same website at the same time
-That we’re capable of learning fast, adapting on the fly, and building something meaningful — even with no prior experience in some of the technologies we used

What's next for The Tomato Trade

-Next step would be to deploy this onto a domain to make it accessible by all.
-We would love to implement this into local neighborhoods to see the community and sustainability results in a real-world environment!
",https://github.com/asamaga18/thetomatotrade,https://youtu.be/rqEGfZvFGhk,"","github, git, html5, css, typescript, react, python, mongodb, fastapi, googlecloudoauth, figma",Amogh,Samaga,amoghsamaga@gmail.com,,https://github.com/asamaga18/thetomatotrade,Best First-Time Hack,Best Sustainability Hack,Best UI/UX Hack,"","",MongoDB was pretty difficult and problematic to work with with all of us being freshmen with minimal experience with databases and backend.,"University of Maryland, College Park",OpenAI,API for Google Cloud OAuth to implement the sign in feature with google. FastAPI to store data in MongoDB.,N/A,TheTomatoTrade.tech,University of Maryland - College Park,3,Reena,Bitar,reenabitar1@gmail.com,Mags-09,McAndrew,mcandmag@terpmail.umd.edu,Dhruv,Patel,dhruvkpatel101@gmail.com
ShapeShift,https://bitcamp2025.devpost.com/submissions/643062-shapeshift,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 05:46:40,"Inspiration 💡

As developers, we’re always looking for ways to stand out. Incorporating 3D models into our projects was a clear way to elevate our work—but the process of modeling or sourcing assets was time-consuming and overly complex. With the help of ShapeShift, taking our projects to the next dimension has never been easier.

What it does 💭

ShapeShift instantly converts SVGs into Three.js-ready code, helping developers turn any idea into 3D reality. 


Animations made easy 🎥: models can spin, move, and come to life with just a few clicks. 
Not sure what to build? Our interactive Canvas page lets users create and experiment with 3D models directly in the browser. 
Save your work! With user authentication,  a smooth login/logout system,  and invaluable project storage, ShapeShift makes 3D development seamless, accessible, and stress-free.


How we built it 🛠️

We built ShapeShift using a modern web stack—React for the frontend, Three.js for 3D rendering, and MongoDB for authentication and data storage.

To generate 3D models, we started by parsing SVG files and extracting their individual shapes (paths, circles, polygons, etc.). Each shape is then converted into a 3D mesh using Three.js geometry generators. We normalize and group these meshes to ensure consistent scaling and alignment in the 3D scene.

For real-time interactivity, we use React Three Fiber to seamlessly integrate Three.js with our React components. Users can rotate, animate, and manipulate their models directly within our custom Canvas interface. Model data is stored securely via MongoDB, enabling save/load functionality tied to user accounts.

Our resulting project is a smooth, accessible pipeline from 2D SVG to fully interactive 3D model—all inside the browser.

Challenges we ran into 🧗

Throughout development, we faced a variety of technical challenges. Exporting and importing 3D models required careful handling of file formats and geometry data to maintain model integrity. Object storage was another hurdle—we needed a reliable and scalable way to save user-generated models and retrieve them efficiently.

Building an interactive coding playground inside the browser also presented unique difficulties, from syncing 3D updates in real-time to ensuring a smooth and intuitive user experience. Each challenge pushed us to dig deeper into Three.js, file handling, and real-time data management—but overcoming them made ShapeShift a much more robust and powerful tool.

Accomplishments that we're proud of 🎉

One of our biggest accomplishments was the growth we experienced as a team. Every member picked up new skills—whether it was learning 3D rendering with Three.js, working with SVGs, building a full-stack web app, or implementing authentication and storage. The challenges we've overcome with our code have been great, and we’re proud of how far we pushed ourselves. In the end, we were able to bring our vision for ShapeShift to life in such a short time. 

What we learned 🧠


How to create, import, and export 3D models
Animating components using React and React Three Fiber
Navigating and building with Next.js
Understanding the fundamentals of WebGL and how it powers 3D rendering in the brow


What's next for ShapeShift 🤔

We’re excited to keep building! Here are some of the next steps we have planned:


Enhanced model export options, including support for more file formats like .glb and .gltf
More import options, including png and jpeg
Advanced control tools for customizing motion paths and vector options within the model
Collaborative features that allow users to share and co-edit models in real-time
AI-assisted model suggestions based on SVG input and prompts


Our goal is to turn ShapeShift into a go-to platform for fast and accessible 3D development.
",https://www.we-shapeshift.tech/,,"Best Web Hack Using React - Peraton, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","react, three.js, mongodb, typescript, javascript, html, tailwindcss",Katherina,Dayaon,katherinadayaon@gmail.com,,https://github.com/Lewin-B/shapeshift,Best Bitcamp Hack,Best UI/UX Hack,Best Razzle Dazzle Hack,Prettiest Hack,Hack That Made You Smile,"This weekend, we worked with MongoDB and Three.js. Both were a bit tricky at first—MongoDB gave us some trouble with setting up and saving data, and Three.js was challenging to learn because of how complex 3D rendering can be.

But after some trial and error, we figured things out and made it work. It was a great learning experience, and we’re proud of how much we grew by using new technologies.",University of Central Florida,Gemini,"Yes, and we used it to generate svgs within our browser to be later turned into 3D models.",461362730354,we-shapeshift.tech,University of Central Florida,3,Faris,Abdelrahem,farishabdelrahem@gmail.com,Lewin,Bobda,lwbobda@gmail.com,mustaeen,ahmed,mustaeen18@gmail.com
NeedleDrop,https://bitcamp2025.devpost.com/submissions/643065-needledrop,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 05:50:57,"Inspiration

To make an app to easily find music

What it does

How we built it

Challenges we ran into

Accomplishments that we're proud of

What we learned

What's next for NeedleDrop
","",,"Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of Gemini API",react,Owen,Marzolf - Miller,owenlmarzmil@gmail.com,,https://github.com/OMarzLearns/HackTuah,Best App Dev Track Hack,Best Razzle Dazzle Hack,"",Hack That Made You Smile,Prettiest Hack,We used GitHub,UMD,Gemini,Yes Gemini,AIzaSyDmxPwZnct2FbUlq_9td9Ucd1HEpMfz29k,"",University of Maryland - University College,0
Ares.AI,https://bitcamp2025.devpost.com/submissions/643066-ares-ai,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 05:51:19,"Inspiration

What if you could control your phone with just one instruction—no tapping, no scrolling, no searching? A task like ""Set an alarm for 7:30 a.m."" can be done by Siri or Bixby, but what about booking Uber or ordering your cart on Amazon? Shouldn't require a dozen manual steps? We wanted to build something that could understand your intent, see your screen, and act like a human assistant—but with the precision of AI. That’s where Ares.AI began: turning natural language into real-time mobile automation.

What it does

Ares.AI takes high-level instructions (like ""Open WhatsApp and message John"") and automates the steps to make it happen. It sees your screen, breaks your request into atomic actions, finds the right UI elements, takes action, and sends back the next move. It keeps track of context, retries when needed, and adapts when stuck, just like a real assistant would.

Core Working

Goal Planning with Gemini 2.5 Pro

We start with a natural language instruction and use function-calling for Gemini 2.5 Pro to break it into a structured sequence of atomic UI actions—like tapping buttons, typing text, or navigating menus. Each goal is clear, ordered, and aligned with how a human would complete the task.

Stateful Goal Execution

The agent maintains a session-level memory of which step it’s on, how many attempts have failed, and whether the current goal is stuck. If something goes wrong—like a button not appearing—it tries alternative actions before giving up.

Visual Grounding with Gemini Vision

At each step, we pass uniquely processed screenshots with bounding boxes to get coordinates  for Gemini to identify the correct bounding box for the next action. If the element isn’t visible, the agent adapts: scrolls, waits, or tries again. It avoids looping on duplicate screenshots and uses hashing to manage screen state effectively.

Challenges we ran into


UI inconsistency: Not every screen looks the same—element labels, layouts, and icons vary wildly across apps.
Image matching in real time: Finding the right element without lag meant balancing model power with latency.
State recovery: Knowing when a goal was truly stuck and how to recover (scrolling, retrying, skipping) required careful heuristics.




Built to feel like a human, but backed by the precision of AI
",https://github.com/invcble/ares_ai,https://youtube.com/shorts/awKfjunMDRg?si=bgcQ8pwx8Qh7xjQM,"Best Use of GenAI in Business - Cloudforce/Microsoft, [MLH] Best Use of Gemini API","gemini, python, kotlin, android-studio, figma",Subhrato,Som,subhratosom17@gmail.com,,https://github.com/invcble/ares_ai,Best Machine Learning Track Hack,Best Social Good Hack,Best Bitcamp Hack,"","",We had a great experience using Gemini 2.5 with function-calling API and learned a lot while building and testing our first mobile app with Android Studio.,Drexel University; George Washington University,OpenAI and Gemini,"Yes, we used Gemini 2.5's generative function-calling API to convert user instructions into modular UI actions and identify elements on screen, enabling real-time mobile automation.",439930024577,"",George Washington University,3,Imon,Bera,imonbera13@gmail.com,Samank,Gupta,samankgupta@gmail.com,Varun,Gholap,varungholap2001@gmail.com
Sort Analyzer,https://bitcamp2025.devpost.com/submissions/643071-sort-analyzer,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 05:54:33,"Inspiration

I was inspired to make this project back around the time I went to HackNYU, but I wanted to do it as as a solo project outside of a Hackathon. I haven't had the time to make this project until now, so I thought now was a better time than any. 

What it does

This project is a website for users to explore different algorithms. They can also input data and compare how different algorithms do, more specifically, how fast the algorithms sort the data.

How I built it

I built it in Replit using the Replit Agent and exported it to Github. The Replit Agent used typescript as the main language.

Challenges I ran into

When I got to the Armory on Saturday morning from my hotel, I discovered that my personal computer's screen was cracked and glitched, so the computer was unusable. I now have to go into the shop when I return home o fix it. Lucky for me, I was able to use my school Chromebook which often blocks websites and search results, but I was able to access Replit and Github. I also struggled to deploy my website from Github properly. 

Accomplishments that I'm proud of

I'm quite surprised that I was able to get the Replit Agent to make something as good as it did. I thought I was going to have to make my own elements which might not have happened, so I'm glad I have something to present during the expo. Additionally, I'm glad I got to use Github for the first time and plan to do so in the future.

What I learned

It can be challenging when the unexpected occurs. I didn't know my computer was going to break or how it broke so I am pretty disappointed. I was still able to continue coding despite my setbacks. Even deploying the website was a struggle and I was unable to figure out how I might do such a task, but it was fun poking around my files in Github once it was connected.

What's next for Sort Analyzer

After I fix my computer, if I really want to, I'll find some way to make the website actually a website.
","https://replit.com/@Pikakid10/SortSpeedComparer, https://github.com/montymole27/sorting-algorithm-analyzer",,Best Web Hack Using React - Peraton,"typescript, html, javascript, replit, github",Nathaniel,Rittner,nathaniel.j.rittner@gmail.com,,https://replit.com/@Pikakid10/SortSpeedComparer,Best UI/UX Hack,"","","","",I used Replit and Github which are two well-known repositories. They are functionally the same but Github has a lot more useful featuress.,Baltimore Polytechnic Institute,I did not use GenAI,N/A,NA,"",Baltimore Polytechnic Institute,0
Dr. Theragotchi,"",Draft,Pending,Project details,04/13/2025 05:56:36,"Inspiration

Mental health tools often feel clinical and impersonal. We wanted to create something engaging that people would actually use daily, combining the nostalgic appeal of virtual pets with modern AI to support emotional wellbeing in a way that feels like caring for a friend rather than completing a health task.

What it does

Dr. Teragotchi is an AI-powered virtual pet that responds to your emotions and grows through interaction. Users chat with their pet, which responds with personality based on emotional context. The app tracks emotional patterns, generates personalized diary entries, and rewards engagement through an evolution system where pets grow as users maintain consistent emotional check-ins.

How we built it

We developed a FastAPI backend with Supabase for data persistence, connecting to OpenAI for natural language processing. The frontend is built with Flutter for cross-platform consistency, featuring responsive animations that bring the virtual pets to life. We carefully crafted AI prompts that balance therapeutic value with conversational authenticity, making interactions feel less like therapy and more like chatting with a friend.

Challenges we ran into

Balancing AI responses was challenging – too clinical felt robotic, while too casual lacked emotional support. We faced significant API integration issues, particularly with rate limiting and response time variability from OpenAI's services that affected conversation flow. Managing stateless interactions in the FastAPI backend while maintaining context for emotionally coherent conversations required complex prompt engineering. The Supabase database interactions sometimes introduced latency, especially when simultaneously updating user profiles, chat history, and emotional metrics. We also struggled with maintaining consistent API response formats across different endpoints while keeping the frontend implementation clean and responsive.

Accomplishments that we're proud of

We successfully created an app that makes emotional wellbeing engaging rather than obligatory. Our pets' natural conversational style avoids the typical ""AI therapist"" feel while still providing meaningful support. The diary generation system transforms chat data into reflective insights without user effort – genuinely adding value through technology.

What we learned

We learned that effective emotional support tools need personality as much as functionality. Technical implementation matters, but the quality of prompts and character design dramatically impacts user engagement. We also discovered the importance of balancing brevity with depth in AI responses to maintain natural conversation flow.

What's next for Dr. Teragotchi

We plan to expand with more pet types and evolution paths tied to different emotional growth journeys. We're developing community features where users can interact with others' pets while maintaining privacy. We're also exploring personalized goal setting based on emotional patterns and implementing more sophisticated diary insights to help users understand their emotional trends over time.
",https://github.com/orgs/DrTheragotchi/repositories,,"","flutter, dart, python, fastapi, postgresql, c++, swift",Yutaka,Yamaguchi,yyamaguchi0102@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,3,Hyunwoo,Jae,hjae@umd.edu,Wonjae,Kim,kwj0011288@gmail.com,jae,Lee,jae.lee9802@gmail.com
Code Confession,https://bitcamp2025.devpost.com/submissions/643074-code-confession,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 05:59:13,"Inspiration

INSPIRED BY THE FACT THAT EVERYONE WAS DOING A PROJECT AND AT SOME POINTS WILL BE MAKING MISTAKES SO WHY DON'T WE RATE EVERYONES CODING MISTAKE BASED ON OUR CODING EXPERIENCE

What it does

BASICALLY POSTS USER COMMENTS FOR OTHERS TO REACT TO

How we built it

Utilized HTML, CSS & JAVASCRIPT 

Challenges we ran into

CHALLENGES INCLUDE PERMANENTLY STORING USER COMMENTS AND ANY REACTION TO SAID COMMENT

Accomplishments that we're proud of

THIS IS MY  FIRST PIECE OF CODE THAT UTILIZED FRONT END DEVELOPMENT

What we learned

LEARNED THE IMPORTANCE OF ADEQUATE PREPARATION BEFORE DESIGN

What's next for Code Confession

STORING EACH CONFESSION SO THE PUBLIC CAN VIEW & REACT
",https://github.com/ApexCoder99/CodingConfession?tab=readme-ov-file#codingconfession,,"",visual-studio,Oluwamurewa,Adeolu-Ayanlowo,oluwamurewaadeolu@gmail.com,,https://github.com/ApexCoder99/CodingConfession?tab=readme-ov-file#codingconfession,Best First-Time Hack,Best Razzle Dazzle Hack,Best Sustainability Hack,"","",GitHub was the most useful tool that I reviewed this weekend as you are able to share code with others easily and working in teams could not get any easier.,UNIVERSITY OF MARYLAND COLLEGE PARK,I did not use GenAI,N/A,N/A,N/A,University of Maryland - College Park,0
Foot Finder - Plant Your Foot to Find your Path,https://bitcamp2025.devpost.com/submissions/643087-foot-finder-plant-your-foot-to-find-your-path,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 06:22:42,"Inspiration

Navigation is an extremely common task in which traditional map services purely prioritize distance/time minimization between two points. This makes it hard for mobility-impaired individuals or wheelchair users, especially to reliably follow online navigation services. To solve this, Travelero generates paths based on user preference where users can prioritize the shortest distance to their location, the least variation in elevation to their location, or a mix of both.

What it does

As students at UMD, we naturally find ourselves on cross-campus hikes along unpleasantly steep terrain. We tell ourselves that surely, there is a better way to get from point A to point B– one that doesn’t require the endurance of a mountain goat.

Travelero Foot Finder works to provide that golden path. It is a pedestrian navigation web app that solves the complications of traversing hilly urban landscapes by selecting the path of “least undesirable variation in elevation” from a set of paths suggested by the Google Maps API.
Travelero Foot Finder allows users to search for a specific location on a user-friendly GUI map of Earth. Use the continuous “Distance – Elevation” slider to smoothly transition between minimizing distance traveled versus minimizing elevation change along a path. Select two locations on the map to place markers, and click “Generate Path” to run our Google assisted algorithm, which takes the user’s slider position into consideration and generates a path.

How we built it

We employed the Google Maps API to display and retrieve map data for backend use, such as distances, paths, and elevations. Our path generation began with Google’s recommended paths, then applied weights based on variations in elevation along them. 
Specifically, we split each path into nodes and edges, then measured the changes in elevation between each consecutive node. We applied weights to amplify any positive changes in altitude, and solely radical negative changes in altitude (more than 45 degrees steep), according to the user-set slider value. Sometimes, Google will suggest the shortest path even if it is steep or hilly, but after running Google’s paths through our weighting process, we retrieve the path of least unfavorable variation in elevation.
We used GitHub, Git, and VS Code to work on code as a group. We used React and TypeScript to build the frontend and a combination of JavaScript, Python, and Flask to build the backend. We also used terpai.umd.edu in order to generate ideas for specific features that we may have wanted to include, such as our personal media links and utilization for our concept. When we were at a stand-still in terms of technical knowledge, or needed a frame of code to get us on our feet, Google Gemini helped bring some of our ideas to life. 

Challenges we ran into
    The very first challenge was determining what pathfinding algorithm to use. If a user selects two points anywhere, there are infinitely many paths to take between them. To reduce the number of paths we needed to consider, we looked at graph-based methods, where we would discretize a region on Earth as a grid of points, then only consider step-wise movements between each gridpoint to get from point A to point B. The first graph-based pathfinding algorithm we considered was Dijkstra’s– until we realized the Google Maps API already has a strong A* pathfinding algorithm built in. We stuck with that, then decided to modify the suggested paths based on variations of elevation along them.

Beginning this project, we wanted an all-encompassing pedestrian pathfinding algorithm that would show a combination of fast and non-steep paths between any two walkable places within a reasonable distance. We found difficulties in avoiding physically impossible paths, like walking through locked buildings and off cliffs, so we ended up sticking to Google-given paths. In the future, we hope to expand our algorithm to assess paths over areas untouched by Google, like grassy hills or desert terrain without human-made walkways. Perhaps we will call this product Bigfoot, referring to footpaths in the same thick woods that are home to the mythical creature.
    The final 10 percent of work proved to be 90 percent of the challenge, as we tried to fix errors like previously generated paths not erasing when we clicked the “clear map” button, or tried to connect the data from two different components (the slider and the map) to the backend in a single function call. We’ve never worked with the Google Maps API, and much of React was unfamiliar to us.

Accomplishments that we're proud of

During the event, we learned many new things about the inner workings of Google Maps and how to share data between web Elements for frontend-backend communication. Our research and hands-on experience leave us feeling a little more comfortable building a similar project in the future. Seeing the pathfinding algorithm work in real time inspired us to push forward!
We were also able to strengthen our bonds, as the development of the team really showed throughout the project. Lots of ideas were shared, lots of helping each other within the team, and lots of considerations for everyone's strengths and weaknesses brought everyone closer together. In each and every step that we completed, the entire team felt like winners. We all celebrated each small accomplishment and felt pride that we were able to achieve what we had hoped.

What we learned

The team generally lacked experience in creating webpages, but we were all able to make great progress in learning new skills related to problem solving and app development. Additionally, as a team coding project, we also improved our ability to coordinate in a team environment using Git. We learned how to quickly adapt to foreign APIs and utilize pre-existing tools towards our specific goal, from terpai.umd.edu and Gemini, to Google Maps, to Flask and React.

What's next for Travelero

There are lots of different possible optimizations and additional features we could add to better cater to broader audiences, like those seeking scenic walking paths, or those who wish to cross roadless natural territories unmarked by Google’s pathfinding algorithm. With more time and experience in React and CSS, we would like to provide the most user-friendly and visually appealing service possible through extensive testing and revision.

—----------
Generally, we had a good experience with the technology. There were some minor issues that we were unable to explain or completely figure out, but overall, everything worked about as expected, if not better. The connection was a little slow at times (we spent the majority of the 36 hours at the venue in the Reckord Armory)
Yes, we used the Google Maps API to retrieve map data relating to elevations at different points and distances between points. We also used Google Maps pathing to find crosswalks and walkways that we could use for our generated paths.
",https://github.com/FallenFork/bitcamp-map-app,https://www.youtube.com/watch?v=DEaCxZDgCQQ,"Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton","python, flask, typescript, tailwind, javscript, vscode, google-maps, node.js",Jason,Li,lijason433@gmail.com,,https://github.com/FallenFork/bitcamp-map-app,Best App Dev Track Hack,Best First-Time Hack,Best Social Good Hack,Hack That Made You Smile,Prettiest Hack,"Most of the preparations were exciting and enjoyable. Personally did not participate too much in the workshops or technology lessons. GitHub was a little bit of a hassle relative to what we wanted to go for, especially when pushing and pulling. The music was good and wifi was good too.","University of Maryland, College Park",Gemini,N/A,N/A,N/A,"University of Maryland - College Park, University of Maryland - University College",3,Kostas,Paparrizos,kostas.paparrizos@gmail.com,Alex,Moulton,acmoultn@gmail.com,Wesley,Lau,wesleylau0524@gmail.com
Untitled,"",Draft,Pending,Manage team,04/13/2025 06:27:54,"","",,"","",Fawwaz,Firdaus,afirdau1@asu.edu,,"","","","","","","","","","","","",Arizona State University,0
Asklepian,https://bitcamp2025.devpost.com/submissions/643095-asklepian,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 06:30:59,"Inspiration

The project was inspired by the miniscule text on the rear of an eye drop bottle. Hence, we wanted users to be more able to easily search such verbose and tiny blocks of text for expiration dates, number of doses per day, etc.

What it does

To achieve such goal, we designed a mobile app that incorporates backend elements to scan labels for text and send it to a backend for processing. Although the project is not fully complete, its use of MongoDB and React Native can be further realized. 

How we built it

Challenges we ran into

By far the largest challenge with we stumbled upon is design of the application as, for students who have are not well-versed with backends, difficult to initially comprehend. Furthermore, the documentation for React Expo is another hinderance that absorbed much of our time in researching the API's functionality.

Accomplishments that we're proud of


For most of our members, this is their first hackathon, so producing such a result is encouraging.
For the one member with more experience, the project is invaluable in learning fetch requests and 


What we learned


A basic framework of thinking when tackling full-stack projects and navigation system of React Native


What's next for Asklepian

While we have created a MongoDB database and populated it, accessing it via Python for some inexplicable reason yields nothing. Fixing it seem remains one of the top ""to-do's.""
","",,"","react-native, mongodb, flask, tesseract, opencv, expo.io",Kevin,Zhang,kevinbczhang@gmail.com,,https://github.com/KevinZhang2135/Bitcamp-2025.git,Best App Dev Track Hack,Best Social Good Hack,Best Moonshot Hack,MOST LIT HACK,Hack That Made You Smile,"React Expo was extraordinarily infuriating as its libraries and standards for cameras, .env variable names, and routing is disjoint from the usual web-based React.","University of Maryland, College Park",Gemini,"Yes, created a synthetic dataset for prescriptions.",329260657006,"","University of Maryland - College Park, University of Maryland - University College",1,neomisule,,neomisule@gmail.com
Rekindle,https://bitcamp2025.devpost.com/submissions/643099-rekindle,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 06:32:18,"Inspiration

We recognized how challenging it can be to find alternatives for our favorite clothing items, especially when they're discontinued. Our goal was to reignite the excitement of rediscovering your personal style by leveraging the power of AI.

What it does

Rekindle lets you upload a photo of a clothing item you like and uses AI-powered visual search to match the item with similar alternatives. You can customize your preferences and instantly get personalized recommendations.

How we built it

We combined modern AI techniques with a robust database of products. Using visual search algorithms and style-matching techniques, our backend integrates tools like CLIP from Hugging Face and Flask for seamless user experience.

Challenges we ran into


Ensuring accurate visual similarity with varied image quality
Leveraging advanced web scraping techniques to systematically extract, process, and curate our data 
Integrating diverse data sources while maintaining a smooth user experience


Accomplishments that we're proud of


Successfully matching discontinued items with high-quality alternatives
Creating an intuitive, user-friendly interface that works seamlessly across devices
Building a scalable system that leverages cutting-edge AI technology


What we learned

We developed a strong understanding of the details of visual similarity search and learned the importance of personalization in e-commerce. We also learned a lot about the challenges of team collaboration, as we frequently dealt with a variety of merge conflicts.

What's next for Rekindle

We plan to expand our product catalog, enhance our recommendation algorithms, and add features like community-driven reviews and style boards, making it even easier to rediscover and reinvent your wardrobe.
","http://rekindlefashion.com, https://github.com/TomMitchell123/Rekindle",,Best Web Hack Using React - Peraton,"typescript, python, javascript, html, css, react, mongodb, git, flask, render, namecheap",Andrew,Zhang,andrew.zhang563@gmail.com,,https://github.com/TomMitchell123/Rekindle,Best Moonshot Hack,Best Razzle Dazzle Hack,Best Bitcamp Hack,Prettiest Hack,Hack That Made You Smile,"Overall, it was pretty smooth. We had some issues with merge conflicts in Git and were a little stressed out when it took a while to resolve them, but everything ended up working out in the end.","University of Maryland, College Park",OpenAI and Anthropic,N/A,N/A,"","Winston Churchill High School, University of Maryland - College Park",2,Thomas,Mitchell,tmitch@terpmail.umd.edu,Jason,Liu,jliu092905@gmail.com
Techmate,"",Draft,Pending,Project overview,04/13/2025 06:42:16,"","",,"","",Keerthi,Ramireddy,kramired@gmu.edu,,"","","","","","","","","","","","",George Mason University,4,Karthik Reddy,Musku,kmusku@gmu.edu,SWETA,PATI,swetapati2203@gmail.com,Athiksha,Venkannagari,avenkann@gmu.edu,Sweta,Pati,spati@gmu.edu
Recipez,https://bitcamp2025.devpost.com/submissions/643109-recipez,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 06:47:43,"Inspiration

After a long day of work, figuring out what to cook can take a lot of time and energy; both of which college students lack. When at home, it can be difficult to know what recipes to make with limited ingredients. When at the grocery store, it can be difficult to know what ingredients you are missing at home for certain recipes. Our product aims to solve both of these problems, acting as an automated recipe and ingredient manager.

What it does

Our product keeps track of all the ingredients in your home by allowing you to scan receipts from the grocery store. Then, based on the ingredients available, our product is able to suggest recipes you would be able to make. If you find a recipe you like, you can save it, and the ingredients used will be removed whenever you make it.

How we built it

We utilized react-native Expo and leveraged Google's Cloud Vision API for OCR, Gemini to generate recipes, and Firebase for the backend.

Challenges we ran into

We faced a few key challenges, especially around OCR accuracy and text parsing from receipts. Cleaning up inconsistent item formatting (e.g., brand names, typos, and different units) required a lot of trial and error with prompt engineering.

Accomplishments that we're proud of

We successfully integrated image-based item detection using OCR and natural language processing, turning a simple receipt image into structured food data. 

What we learned

We learned how to utilize API architecture effectively, especially chaining two different services (Google Vision + Gemini) to create a smooth user experience. We also deepened our understanding of React Native’s component lifecycle, image handling, and state management.

What's next for Recipez

-Expiration reminders: Add auto-reminders for items based on shelf life.
-User accounts: So people can sync their fridge across devices.
","",https://youtu.be/wM1SvGiCrm4,[MLH] Best Use of Gemini API,"react-native, firebase, gemini, google-cloud-vision, typescript",Suhani,Agrawal,suhanigarima@gmail.com,,https://github.com/tsunami236/recipez,Best Machine Learning Track Hack,Best Moonshot Hack,Best Bitcamp Hack,Prettiest Hack,Hack That Made You Smile,"We utilized Gemini, Firebase, and Google's Cloud Vision API for our project's functionality.",University of Maryland College Park,Gemini,"Yes, we used Gemini to generate personalize recipes for our users based on their dietary restrictions, ingredients available, and preferences.",415154864400,"","poolesville high school, University of Maryland - College Park",3,Patrick,Hong,path.plh@gmail.com,Anna,Gustafson,agustaf2@terpmail.umd.edu,ahamme2330,Hammerash,ahamme2330@gmail.com
Patch,"",Draft,Pending,Additional info,04/13/2025 06:56:44,"Inspiration

Low-income individuals often face difficulty getting hazardous rental issues addressed, as some landlords  avoid spending money or effort on necessary repairs. There hasn't yet been a seamless platform dedicated to tenant rights and clear communication between tenants and landlords, prioritizing tenant safety and legal protection.

What it does

Patch is a comprehensive communication app designed for landlords and their tenants. Landlords can add tenants by sending email invitations, creating customized chat rooms for seamless interaction. Tenants can report property issues by uploading photos directly to their landlord. These photos are automatically classified by AI into specific issue types such as mold, pests, or structural damage. Furthermore, tenants receive automated notifications detailing the specific legal timeframe allowed in their state to file a claim if the reported issue is not resolved promptly. Tenants are also provided with automatically generated, state-tailored legal reports ready for submission to authorities.

How we built it

We built a sophisticated custom user interface featuring real-time chat to ensure frictionless communication between landlords and tenants. For our AI-powered issue detection, we developed a custom dataset using advanced web scraping techniques to gather relevant imagery, carefully curating and labeling it manually. Leveraging Roboflow, we created an accurate image classification model capable of identifying mold, pests, and structural damage. To further enhance tenant advocacy, we integrated Google's Gemini AI to dynamically determine state-specific legal timelines for issue resolution and to automatically generate customized legal complaint letters for each reported issue.

Challenges we ran into

One significant challenge was managing image uploads. Most databases offering image storage had limited free tiers, prompting us to pivot towards using AI to interpret images into concise, textual descriptions instead of storing full images directly. Additionally, web scraping resulted in substantial irrelevant data, necessitating manual filtering and validation to ensure dataset quality and model accuracy.

Accomplishments that we're proud of

One significant challenge was managing image uploads. Most databases offering image storage had limited free tiers, prompting us to pivot towards using AI to interpret images into concise, textual descriptions instead of storing full images directly. Additionally, web scraping resulted in substantial irrelevant data, necessitating manual filtering and validation to ensure dataset quality and model accuracy.

What we learned

Throughout this project, our team significantly improved collaboration skills, using GitHub effectively for organized development and version control. We mastered agile problem-solving, quickly pivoting to alternative solutions when initial approaches were unfeasible. Importantly, we learned how to seamlessly integrate multiple AI models to deliver a functional app with genuine, tangible societal benefits.

What's next for Patch

Looking ahead, Patch plans to expand its AI capabilities by incorporating more extensive datasets to identify a broader spectrum of housing issues. We envision integrating AI-powered predictive maintenance alerts for landlords, helping prevent issues before they arise. Additionally, we aim to include multilingual support to increase accessibility, implement community-driven tenant reviews for accountability, and collaborate with local housing authorities to streamline complaint filings directly through our platform.
","https://www.patchchat.tech/, https://github.com/Rishav-N/patch",,Best Web Hack Using React - Peraton,"python, firebase, flask, roboflow, gemini, html5, css, bootstrap, javascript, github",Rishav,Nair,rishav.a.nair@gmail.com,,https://github.com/Rishav-N/patch,Best App Dev Track Hack,Best Bitcamp Hack,Best Social Good Hack,Hack That Made You Smile,Prettiest Hack,We worked a lot with github and had a good experience with collaboration to create our project. In addition we utilized firestore from firebase as our database which worked very well and google gemini for generative AI which was seamless to use as well.,University of Maryland College Park,Gemini,Yes we did implement generative AI model which was googles gemini. This was to create a comprehensive legal report which tenants are able to send to legal representatives if their hazardous issues aren't fixed.,api key  = AIzaSyAujNSlzw3bZylK4o3uRaPu3epGByEq-JU      project number = 976048796336,patchchat.tech,"University of Maryland - University College, University of Maryland - College Park",2,Arav,Dash,aravdash@terpmail.umd.edu,Aryahvishwa,Babu,ababu124@terpmail.umd.edu
Terp Trials,"",Draft,Pending,Project overview,04/13/2025 06:57:58,"","",,"","",Bhavna,Kumari,bhavnakumari0103@gmail.com,,"","","","","","","","","","","","",Cornell University,1,G,Lopes,gilliolopes@gmail.com
Rizz Lab,https://bitcamp2025.devpost.com/submissions/643117-rizz-lab,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 07:00:41,"Inspiration

people need help razzing 

What it does

How we built it

next js fast api

Challenges we ran into

Accomplishments that we're proud of

What we learned

What's next for Rizz Lab
",https://github.com/vasantsaladi/umd_hack,,Best Web Hack Using React - Peraton,"python, typescript",Vasant,Saladi,vsaladi@terpmail.umd.edu,,https://github.com/vasantsaladi/umd_hack,Best Gamification Hack,Best UI/UX Hack,Best Razzle Dazzle Hack,Hack That Made You Smile,Prettiest Hack,"fast api ,next js",University of Maryland,OpenAI,yup for training,na,"",University of Maryland - College Park,0
TerpNotes AI,"",Draft,Pending,Project overview,04/13/2025 07:03:22,"","",,"","",Olaniyi,Salami,salaminiyi@yahoo.com,,"","","","","","","","","","","","",University of Maryland - College Park,0
SurfSmart,https://bitcamp2025.devpost.com/submissions/643127-surfsmart,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 07:08:50,"Inspiration

Every time I was asked to do research, there will be tons of tabs showing on my browser and my computer go slow by time. This project is an attempt to solve this issue.

What it does

It basically creates an external knowledge database for each project, dynamically updates using the website contents, LLM's response, and user's notes. For accuracy, we would extract the most frequent keywords and generate a summary using or not using Gemini API. User can ask Gemini questions with this external knowledge database and built-in prompts, such as if one website should be replaced by another, or ask for recommendation websites. User can also get the most frequent keyword in a project and ask Gemini to generate a short overview of this project.

How we built it

We used Vite + React for frontend, python flask for backend, MongoDB Atlas for database.

Challenges we ran into

The time is short and the total workload was way too heavy.

Accomplishments that we're proud of

At least we wrote a prototype, and many places still need revision.

What we learned

How to craft a website using Front-end and back-end separation technology.

What's next for SurfSmart

Debug.
",http://surfsmart.tech,,"[MLH] Best Use of .Tech, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","flask, python, vite, react, mongodb, gemini",Danyang,Li,ldy@terpmail.umd.edu,,https://github.com/danielolllee/SurfSmart,Best First-Time Hack,Best UI/UX Hack,"","","",MongoDB,"University of Maryland, College Park",OpenAI and Gemini,"Yes, we used it as an assistant, feeding it well-crafted prompts and external knowledge database to make accurate response and recommendations.",122327367612,surfsmart.tech,University of Maryland - College Park,0
Helpr,https://bitcamp2025.devpost.com/submissions/643130-helpr,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 07:10:16,"Inspiration

Customer service is broken for small businesses. While giants like Amazon and Apple offer lightning-fast, intelligent support, most businesses rely on outdated FAQs, overwhelmed reps, or generic chatbots. We wanted to change that. With the rise of AI voice agents, we saw an opportunity to give every business—no matter how small—the ability to launch a full-service, voice-based customer support rep with just a company name and a URL.

What it does

Helpr instantly creates a fully functional voice assistant trained on your company’s website, FAQ, and support documents. Just enter your company name and URL, and with just one click, Helpr crawls your site, builds a knowledge base, and spins up a natural-sounding voice agent that can answer calls, help customers, and resolve issues 24/7.

How we built it

At the heart of Helpr is a vision-based Retrieval-Augmented Generation (RAG) pipeline powered by Colpali and Qwen. This system enables us to not only extract information from web pages and documents, but also understand visual context—like layout, tables, and scanned forms—making our voice agent far more robust when dealing with PDFs, screenshots, or policy documents uploaded by businesses. By using Colpali to semantically parse visual documents and Qwen as the generation engine, Helpr can answer questions grounded in both structure and content—something traditional text-only RAG systems can’t do effectively.

Alongside our RAG pipeline, we fine-tuned a CSM-1B model from OpenSesame to give Helpr a strong foundation in conversational understanding, especially for follow-up questions and longer dialogue history. We used ElevenLabs’ Conversational AI for real-time, natural-sounding speech-to-text and text-to-speech handling, allowing the voice agent to feel human, fast, and fluent.

To understand, cluster, and personalize user queries, we integrated Google Gemini and ChromaDB for semantic encoding and grouping. This allowed Helpr to identify patterns in customer questions and tailor responses based on intent. Our backend was built in FastAPI, chosen for its lightweight performance and flexibility, while MySQL handled structured storage of user sessions, knowledge chunks, and analytics data.

For dynamic, real-time search beyond the scraped knowledge base, we used Perplexity AI to fetch up-to-date web results for open-ended or news-related questions. The ReactJS frontend gave us a fast, modern UI where users can deploy their agent by simply entering a company name and URL. All of these systems work together to generate a voice agent with just one click. 

We even used TerpAI to build us a business pitch and linkedin post to really get Helpr out there!

Challenges we ran into

One of the biggest challenges was orchestrating all of these systems to work together in real time. Fine-tuning the CSM model required careful dataset curation and extensive testing to maintain response quality while supporting dynamic voice inputs. Integrating ElevenLabs with fast, low-latency backend responses while maintaining conversational naturalness also took tuning and optimization.

We also ran into complexities when using vision-based RAG on user-uploaded documents—handling malformed PDFs, inconsistent layouts, or low-resolution scans pushed us to build fallback mechanisms and cleaning pipelines. Finally, ensuring that our semantic clustering (using Gemini and ChromaDB) could intelligently group diverse customer queries without losing nuance was a delicate balancing act.

Accomplishments that we're proud of

We’re incredibly proud that Helpr can deploy a fully functional voice support agent trained on any business’s website in under a few seconds. This includes scraping, indexing, and speech agent initialization—tasks that typically require hours or manual configuration. We also successfully integrated vision-based document ingestion with semantic search, giving Helpr the power to handle FAQs, legal policies, and even scanned paper forms. Finally, our system grouped customer questions intelligently and provided analytics-ready insights about what customers were really asking.

What we learned

We learned that combining traditional RAG with vision-based document parsing significantly boosts coverage for small businesses with fragmented content. Fine-tuning smaller open models like CSM-1B, when done carefully, can outperform larger models in narrow domains like customer support. We also discovered that building a voice UX is fundamentally different from building a chatbot—timing, tone, and content brevity are critical. And most importantly, we learned that users don’t want to fiddle with configurations—they want magic in one click.

What's next for Helpr

We plan to launch multilingual support with real-time accent adaptation, making Helpr more accessible globally. We’re building native integrations with tools like Shopify, Zendesk, and HubSpot to embed Helpr directly into existing workflows. We’ll also roll out a self-serve analytics dashboard so businesses can track common questions, identify content gaps, and fine-tune their agent behavior. Long-term, we're exploring ways to embed Helpr into phone systems and smart speakers, transforming it from a web-first solution to a full omnichannel support agent. With strong early interest, we’re gearing up to raise a pre-seed round to scale Helpr beyond the hackathon and into the hands of thousands of small businesses worldwide.
",https://github.com/AwesomeCuber6543/BitCamp2025,https://www.youtube.com/watch?v=LM6nc2bRhnE,"Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of Gemini API","gemini, colpali, perplexity, opensesame, csm1b, finetuning, gemini-flash, chromadb, semantic-search, eleven-labs, rag",Yahia,Salman,yahia.salman.04@gmail.com,,https://github.com/AwesomeCuber6543/BitCamp2025,Best Machine Learning Track Hack,Best Bitcamp Hack,Best Moonshot Hack,Prettiest Hack,Hack That Made You Smile,"We interacted with a lot of technology this weekend. We were hosting our phone numbers on twilio, used Gemini to create embeddings to store in chroma db so that customer queries could be grouped semantically, used LoRA to finetune CSM 1B model, used perplexity for AI web crawling, used ColPali for vision based indexing of additional documents and semantic vision based retreival. Used Eleven Labs and gemini flash 2.0 for understanding of speech and response generation.",George Mason University,"Hugging Face, Gemini, and Other",We used GenAI a lot. We used Gemini's embedding model for creating embeddings of questions for semantic retreival and grouping of customer queries. We also used Gemini flash 2.0 for fast but accurate understanding and response generation for the voice agent. We also used perplexity for AI based web crawling. We also used ColQwen for vision based indexing and RAG.,24071617603,"",George Mason University,1,Amaya,McBride,mcbride.amaya19@gmail.com
SmileySpotify,https://bitcamp2025.devpost.com/submissions/643131-smileyspotify,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 07:13:47,"Inspiration

Have you ever been on a YouTube music video and wanted to add it to your Sptofiy playlist? Or, want to find your Spotify playlist songs quickly on YouTube? Well, we have! SmileySpotify bridges the gap between YouTube and Spotify, bringing your Spotify songs to YouTube.

What it does

Our Google extension makes a YoutTbe playlist from your selected Spotify playlist. Also allows you to add current song on youtube to spotify playlist.

How we built it

Brute force and React.

Challenges we ran into

Depends on which feature, abstracting things and getting them to work together was sometimes difficult. One of the big ones was handling the state of the current playing video.

Accomplishments that we're proud of

Finishing a polished product as a two-man team!

What we learned

Figure out your development environment first!!! We should have made sure we could build our React project into a Google extension before we started coding. Research > Brute force

What's next for SmileySpotify

Fixing a few edge cases.
","",,Best Web Hack Using React - Peraton,"react, materialui, javascript, tailwind, vite",anakib5,Nakib,alhassan.nakib57@gmail.com,,https://github.com/zingodev/spotify-to-youtube-extension,Best First-Time Hack,Best Bitcamp Hack,Best UI/UX Hack,Hack That Made You Smile,Prettiest Hack,We used Visual Studio Code and GitHub for version management.,"University of Marlyand, College Park",OpenAI and Gemini,N/A,N/A,N/A,University of Maryland - College Park,1,Jacob,Plunkert,jeplunkert@gmail.com
ClearGaze,https://bitcamp2025.devpost.com/submissions/643135-cleargaze,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 07:16:44,"Inspiration

According to the CDC, in the United States alone, an estimated 300,000 people drive while under the influence of alcohol or drugs every day. This is a serious issue. We were inspired by the Horizontal Gaze Nystagmus (HGN) test used by law enforcement to detect impairment through involuntary eye movements. With the rise of mobile technology and AI, we realized this same test could be performed using just a smartphone camera. Our goal was to create a tool that empowers people to self-assess their fitness to drive — quickly and privately.

What it does

ClearGaze is a mobile app that uses your phone’s front-facing camera to run a quick, AI-powered HGN test. The user follows a dot moving on the screen, while the camera records eye movements. In the background, our backend analyzes for signs of impairment — specifically looking for erratic motion or nystagmus. The entire test takes less than 30 seconds and gives a fast, private assessment of whether you're fit to drive.

How we built it

Frontend: Built using React Native, the app displays a moving dot on the screen for the user to follow, while simultaneously recording video from the front-facing camera.
Video Capture: After the 30-second test, the recorded video is packaged and sent to our backend for analysis.
Backend: A FastAPI server receives the video and forwards it to the Gemini API, which evaluates the eye movement data to detect signs of nystagmus or irregular tracking behavior.


AI Analysis: The Gemini API acts as the core vision intelligence, enabling us to offload motion tracking and pattern recognition to a powerful hosted model — rather than relying on local tools like OpenCV or MediaPipe.
Privacy by Design: No video is stored — it’s used only temporarily for inference and then discarded, keeping the experience secure and private.


Challenges we ran into

Transitioning away from local models: We originally planned to use OpenCV and MediaPipe but found them unreliable on mobile for precise eye tracking.
Integrating Gemini with video input: Since Gemini typically expects image input, passing video and interpreting results correctly required experimentation and careful pipeline design.
Synchronizing video with dot movement: We needed to ensure that the recorded video captured the right part of the test — especially when detecting subtle eye movement patterns.
Frontend-backend timing coordination: We faced challenges ensuring smooth communication between React Native’s recording features and the backend analysis workflow.

Accomplishments that we're proud of


Successfully implemented a working HGN-style test using only a smartphone.
Replaced traditional local tracking tools with Gemini's powerful video analysis.
Created a full end-to-end system: video capture → AI analysis → actionable impairment feedback — all within seconds.
Maintained a clean and simple user experience while working across multiple tech stacks under tight time constraints.


What we learned


How to use the Gemini API for advanced video-based analysis, beyond typical image classification tasks.
The strengths and limitations of local vs. cloud-based computer vision models in mobile contexts.
How to manage real-time video capture in React Native and build a smooth testing experience that mimics real clinical assessments.
The importance of tight frontend-backend synchronization when working with time-sensitive video data.


What's next for ClearGaze

Incorporating eye coloration analysis: We plan to integrate MediaPipe and OpenCV to evaluate redness, discoloration, or other visual indicators that could signal fatigue or substance use — adding another layer to our impairment detection system.
Real-world car integration: We envision ClearGaze being linked directly to a user's vehicle, where a failed test would trigger a lockout and prevent the car from starting — adding a proactive layer of safety.
Multi-signal detection: By combining both motion (nystagmus) and visual indicators (eye coloration), we aim to create a more robust and medically relevant assessment.
Personalization and calibration: Future versions could learn a user’s baseline and adapt thresholds over time, improving accuracy and reducing false positives.
Healthcare and fleet expansion: We see potential applications beyond driving — from workplace compliance (e.g., for truckers or heavy machinery operators) to wellness apps that screen for fatigue, burnout, or medical symptoms.
",https://github.com/bbmullen72/ClearGaze-Bitcamp,https://youtu.be/P1rHd6SJp3A,"Best Hack Promoting Public Health - Bloomberg, Best Web Hack Using React - Peraton, [MLH] Best Use of Gemini API","gemini-api, react-native, fastapi, typescript, javascript, python, c, html, expo.io, next.js, ngrok, css",Rohit,Vashisht,rvashis@gmu.edu,,https://github.com/bbmullen72/ClearGaze-Bitcamp,Best App Dev Track Hack,Best First-Time Hack,Best Social Good Hack,Prettiest Hack,Hack That Made You Smile,"This weekend we used GitHub for version control, React Native for building the mobile frontend, FastAPI for the backend, and Google's Gemini API for video-based eye movement analysis. Gemini was powerful but a bit tricky to work with for video input, as it’s more optimized for images — we had to experiment with formatting and timing. GitHub worked well overall, though we ran into some branch merge conflicts early on due to separate frontend and backend dev. React Native was smooth for UI, and FastAPI let us quickly build and test our API endpoints. Overall, the stack was flexible and fun to work with!",George Mason University,Gemini,"Yes — we used Google's Gemini API to analyze video recordings of users performing the HGN test. It helped us detect subtle eye movements associated with impairment, allowing us to offload complex vision analysis to a generative AI model.",AIzaSyClWvfoM68iUz5eEscbd6Ia_oKjzDDMp2E,"",George Mason University,1,Brendan,Mullen,bbmullen72@gmail.com
Petals of Progress,"",Draft,Pending,Project overview,04/13/2025 07:17:11,"","",,"","",Adam,Nazrul Hisham,anazrulh@terpmail.umd.edu,,"","","","","","","","","","","","",University of Maryland - College Park,3,Dave,Jose,davedeepujose@gmail.com,Alexander,Chen,achen21044@gmail.com,Adriel,Wicaksono,adrielw0610@gmail.com
"Out with the Old, In with the News",https://bitcamp2025.devpost.com/submissions/643141-out-with-the-old-in-with-the-news,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 07:20:21,"Inspiration

News articles, or just articles in general, are often easier to process when you can see another person's perspective on it. Now, what if in order to see that perspective, we had a 'discussion board' feature on a website, while simultaneously filtering out hate speech within the comments and other unwanted things from user posts?

What it does

Provides multiple companies' news stories, with a discussion board for people to share their thoughts on it.

How we built it

This is mostly a proof of concept (whoops..), so it's an HTML file with the visible displays up.

Challenges we ran into

Originally, I wanted to integrate Google Gemini into determining whether the phrases users tried to post was hate speech or not. I ended up NOT being able to get that API key to work.. so I worked around it.

Accomplishments that we're proud of

This is my first hackathon where I'm submitting a solo project. wowza !

What we learned

I learned how to use HTML, as I've never really tried to use it before. 

What's next for Out with the Old, In with the News

zoo-wee mama!
",https://github.com/evabarks1/bitcamp2025,,"Best Financial Hack - Capital One, [MLH] Best Use of .Tech",html,Eva,Barkalova,eva.barkalova@gmail.com,,https://github.com/evabarks1/bitcamp2025,Best First-Time Hack,Best Sustainability Hack,"",MOST LIT HACK,Prettiest Hack,It all worked really well; I just don't know how to use other things.,Thales Academy High School,I did not use GenAI,n/a,no :(,"Out with the Old, In with the News",Ravenscroft ,0
Class Topper,"",Draft,Pending,Project overview,04/13/2025 07:22:44,"",https://github.com/Jiten-Bhalavat/ClassTopper,,"","gemini, qdrant, python, javascript, react, html, firebase, llamaindex, css",Sahil,Chordia,chordiasahil24@gmail.com,,"",Best Machine Learning Track Hack,Best First-Time Hack,Best Razzle Dazzle Hack,Hack That Made You Smile,Prettiest Hack,"","","","","","","University of Maryland - College Park, Arizona State University",2,Jiten Bhaveshkumar,Bhalavat,jbha0504@umd.edu,Fawwaz,Firdaus,afirdau1@asu.edu
PodQuirk,https://bitcamp2025.devpost.com/submissions/643145-podquirk,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 07:23:06,"Inspiration

I wanted to experiment with ElevenLabs to see how far I could push realistic AI-generated voices. It seemed like a fun challenge to create something functional and creative, and podcasts felt like a natural fit.

What it does

PodQuirk lets users pick a topic or category, and in under a minute, it generates a podcast episode that talks about that subject. It combines real-time news and smart summarization with AI narration, giving users a quick, engaging audio experience.

How I built it

The frontend is built with Next.js, the backend with Flask. I use OpenAI to generate the podcast script, ElevenLabs for lifelike TTS audio, and the Newspaper3k API to pull relevant news content. AWS handles caching and storage to reduce unnecessary API calls and speed up repeat requests.

Challenges I ran into

Managing API usage was a big one! I didn’t want to burn through credits every time someone clicked “generate.” So I implemented a caching mechanism using AWS to store previously generated episodes. Another challenge was fine-tuning the prompt for script generation so that it would sound natural and match the tone of a real podcast. Setting up and managing ElevenLabs also had a bit of a learning curve.

Accomplishments that I'm proud of

I was able to generate a full podcast in under a minute, from topic selection to audio output. That includes gathering relevant news, scripting, and producing it all with high-quality narration.

What I learned

What's next for PodQuirk
",https://github.com/JulianChavez/BITCAMP_2025,,Best Web Hack Using React - Peraton,"nextjs, amazon-web-services, s3, elevenlabs, perplexity, newsapi, newspaper3k, tailwind, react, typescript",Julian,chavez,julianchavez13@gmail.com,,https://github.com/JulianChavez/BITCAMP_2025,Best Bitcamp Hack,Best UI/UX Hack,Best Razzle Dazzle Hack,Hack That Made You Smile,Prettiest Hack,Fun!,"University of Maryland, Baltimore County",OpenAI,"Yes. OpenAI genAI, ElevenLabs API for TTS, Perplexity API for research, NewsAPI for fetching news articles, Newspaper3k for webscrapping. Also AWS s3 for caching",N/A,"","",0
StudyEZ [AI Study Tool],"",Draft,Pending,Project overview,04/13/2025 07:25:58,"",https://github.com/dannguyen24/BITCAMP2025/,https://youtu.be/rHcq1_7Js-8,"","flask, react, vite, css, axios, python, mongodb, google-gemini, google, pymongo, assemblyai, fetch, youtube",Sesha Sai,Lakkavajjala,slakkava@terpmail.umd.edu,,"","","","","","","","","","","","","University of Maryland - College Park, George Mason University",3,Dan,Nguyen,danuyennguyen2412005@gmail.com,Michelle,Eileen,meileensoewandi@gmail.com,Abdullah,Ali,20abdul.ali15@gmail.com
HallHopper,https://bitcamp2025.devpost.com/submissions/643152-hallhopper,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 07:30:21,"Inspiration

Navigating large university campuses like UMD can be confusing, especially when trying to find classrooms on the first day of class in unfamiliar buildings. Unpredictable weather could add to that stress. We realized there wasn’t an easy way to move between buildings indoors or find clear directions inside complex academic halls.

What it does

HallHopper helps you:


Navigate inside UMD buildings to find classrooms and study spots.
Avoid getting lost on the first day of classes or during finals when you’re suddenly assigned a random room.
Find dry (or shady) shelter during rainy or sunny days by hopping through connected buildings.


How we built it


Backend: Flask-based API to manage building data and navigation routes.
Frontend: Built in React Native, with a focus on iOS compatibility.
Database/Auth: Supabase for user authentication.


Challenges we ran into


Learning React Native.
Working with latitude and longitude coordinates, common building names, and street addresses.
Estimating relative distances indoors between stairs, entrances, and hallways.


Accomplishments that we're proud of


Built and integrated a functional MVP with just two people!
Learned and implemented new tools and frameworks in a short amount of time.
Created something useful and uniquely tailored to our own campus experience.


What we learned


We love bitcamp!
Frameworks!


What's next for HallHopper


Improve internal API complexity.
Implement database for user information (e.g., home address and favorite buildings).

",https://github.com/ivyisaplantt/hallhopper,,Best Web Hack Using React - Peraton,"flask, react-native, supabase, python, typescript",Emily,Chen,echen583@gmail.com,,https://github.com/ivyisaplantt/hallhopper,Best First-Time Hack,Best UI/UX Hack,Best Razzle Dazzle Hack,Prettiest Hack,Hack That Made You Smile,WE LOVE GITHUB,"University of Maryland, College Park",Other,N/A,N/A,"",University of Maryland - College Park,1,Ivy,Lu,ivylu21122@gmail.com
Golden Standard,"",Draft,Pending,Project details,04/13/2025 07:31:54,"Inspiration

Golden Standard is a real-time sentiment dashboard for individual stocks. It pulls:

Reddit comments from r/wallstreetbets

News headlines via Google News

(Coming soon) Twitter/X discussions

Then, it uses Gemini AI to:

Summarize what people are saying

Extract bullish vs bearish perspectives

Classify overall sentiment as positive, neutral, or negative

All this is displayed in a clean, interactive React frontend, allowing users to get a sentiment report for any ticker in seconds.

What it does

Python Backend using FastAPI for scraping, AI processing, and APIs

PRAW for Reddit comment parsing

SerpAPI for news data

Google Gemini API for AI-generated summaries and formatting

React Frontend using TypeScript and Tailwind to render news, forums, and live analysis

Dynamic sentiment endpoint that updates analysis based on the requested ticker in real time

How we built it

Managing real-time scraping without being rate-limited or blocked

Getting Gemini to reliably return clean, structured JSON

Mapping unstructured Reddit data into something AI could reason with

Dealing with SSL/cert issues and Python environment conflicts during setup

Designing a frontend flexible enough to render AI-driven content without breaking layout

Challenges we ran into

Created a seamless, full-stack pipeline from live internet data → AI processing → visual display

Built a modular backend that can scale to other sources like YouTube, Twitter, or Discord

Developed custom prompts that extract meaningful insights from chaotic, noisy Reddit threads

It works — and it feels smart. That’s the best win.

Accomplishments that we're proud of

Prompt engineering is everything when working with LLMs

Reddit comments are messy — structuring them is a project in itself

FastAPI is incredibly powerful for building real-time AI-backed APIs

Frontend and backend integration for AI systems takes careful planning to avoid breaking the user experience

You don't need a giant team to build something that feels enterprise-grade

What we learned

What's next for Golden Standard

dd Twitter/X data via Playwright or a scraping proxy

Integrate LLM-based alerts for unusual sentiment spikes or shifts

Train a lightweight custom model on past sentiment + stock movement to predict short-term moves

Add multi-ticker comparisons and industry-wide heatmaps

Deploy publicly and offer a free tier + pro API for fintech devs
",https://github.com/Claudesaul/SentimentTech,https://vimeo.com/1075063309/721e9972ce?share=copy,"","python, javascript, fastapi, react, node.js, api, gemini, css, tailwind",Sohan,Shetty,sshetty1@terpmail.umd.edu,,"","","","","","","","","","","","",University of Maryland - College Park,0
Untitled,"",Draft,Pending,Manage team,04/13/2025 07:33:07,"","",,"","",Tony,Zheng,tonyzh21@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,0
BurnoutBuddy,https://bitcamp2025.devpost.com/submissions/643165-burnoutbuddy,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 07:38:20,"Inspiration

As students juggling deadlines, hackathons, internships, and side projects, we’ve all felt the creeping burnout that’s hard to name but even harder to fight. Most mental health apps felt too serious or clinical, we wanted something fun, light, and proactive.
After too many late nights debugging and forgetting what sunlight looks like, we realized burnout doesn’t announce itself, it just quietly takes over. We wanted to build something that could catch us slipping before we spiral, using the very data we already generate: our steps, our sleep, our GitHub commits, and our tired faces.
Burnout Buddy became our goofy, 3D self-care sidekick, combining real-world signals, AI, and playful visuals to remind us to hydrate, vibe, and breathe before burnout hits too hard.

？What it does

Burnout Buddy is a personal wellbeing dashboard that tracks signs of burnout and helps you bounce back. It:


Tracks physical and mental exhaustion using Fitbit data, GitHub commits, and real-time webcam fatigue detection.
Calculates a personalized burnout score using sleep, steps, commits, and drowsiness levels.
Offers timely interventions like hydration reminders, lo-fi Spotify tracks, and AI-generated motivational quotes.
Provides journaling and task tracking to support mindful productivity.
Visualizes burnout trends over time with interactive graphs.
Encourages self-care before self-combustion — because burnout doesn’t announce itself, it just sneaks in.


🧱 How we built it


Frontend: React.js with custom CSS for styling, Chart.js for data visualization, and Spline for 3D embedding. Didn't forget Confetti (because vibes).
Backend: Flask API to serve burnout score, quotes (via Google Gemini API), Fitbit and GitHub data.
Data Pipelines: Python scripts to fetch Fitbit step/sleep data and GitHub commits, stored in MongoDB atlas.
Fatigue Detection: Real-time drowsiness detection using OpenCV, MediaPipe, and dlib for webcam-based eye tracking
Database: MongoDB Atlas stores time-series Fitbit/GitHub data and user journal entries securely
Spotify Integration: Lo-fi music player powered by randomized Spotify Web Player embeds
Burnout Scoring: Custom burnout score calculated from sleep, activity, commits, and fatigue using a weighted formula


🎯 Challenges we ran into


Integrating real-time webcam analysis using MediaPipe and dlib in a way that didn’t crash on multiple machines.
Merging data from multiple APIs into a clean and meaningful score (averaging is easy, burnout isn’t).
Making sure Flask, Mongo, and React played nice — and in sync — without one ghosting the others.
React’s spontaneous desire to break when the camera stream said hello.
A major CORS and API communication between Flask and React issue that turned out to be not so major (that we still spent 5 hours debugging)
Parsing Google Gemini API responses into usable JSON formats


🏆 Accomplishments that we're proud of


Built a full-stack wellness app that blends data science, computer vision, and user-centered design
Successfully integrated real-time webcam-based drowsiness detection using OpenCV, MediaPipe, and dlib
Connected to both the Fitbit and GitHub APIs to gather personal activity data securely
Designed a burnout scoring algorithm that combines physical, digital, and visual fatigue metrics
Created a clean, fun, and interactive UI with features like Spotify embeds, journaling, and task tracking
Visualized personal trends in sleep, steps, and commits using Chart.js
⁠Built a burnout detection tool that we’d genuinely want to keep using after this weekend.


📚 What we learned


How to wrangle health, code, and chaos into a meaningful burnout metric
Real-time computer vision can absolutely be combined with soft UX to create something empathetic
How to use OpenCV, MediaPipe, and dlib for facial fatigue detection
How to integrate and authenticate with Fitbit and GitHub APIs
How to process and visualize time-series data using Chart.js in React
The value of designing experiences that care for the user, not just inform them
Sometimes, you really do need to listen to a dashboard telling you to hydrate


⏭️ What's next for BurnoutBuddy


Scale to companies as a team wellness tool, offering anonymous burnout trends without compromising privacy
Add journaling insights, mood tracking, and burnout trend visualizations
Push smart alerts for burnout spikes — think “go outside” or “close VS Code”
Integrate calendars and build a Chrome extension that nudges users away from doomscrolling
Launch internally for dev teams — then gently expand into the wellness SaaS space
Offer personalized recovery suggestions using machine learning trained on individual usage patterns and habits

",https://github.com/CSin007/Bitcamp2025,,"Best Hack Promoting Public Health - Bloomberg, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","python, flask, mongodb, gemini, openai, githubapi, react, chart.js, fitbitapi, spotifyapi, spline, conda, dlib, opencv, mediapipe, restapi, cors, pymongo",Arushi,Tibrewal,atibrew1@terpmail.umd.edu,,https://github.com/CSin007/Bitcamp2025,Best Razzle Dazzle Hack,Best Bitcamp Hack,Best Moonshot Hack,Hack That Made You Smile,Prettiest Hack,"We used MongoDB Atlas to store and query our Fitbit and GitHub data, and the experience was smooth. The connection string setup and authentication through environment variables made integration straightforward. We also used Google's Gemini API to generate dynamic motivational quotes, working with the API was simple and fast, and it added a fun, human-like touch to our app.
For real-time burnout detection, MediaPipe and dlib powered our webcam-based fatigue tracker. The hardest part was tuning drowsiness sensitivity, but combining it with OpenCV gave us solid accuracy. Overall, integrating these technologies gave us valuable hands-on experience in building empathetic wellness tech.","University of Maryland, College Park",OpenAI and Gemini,"Yes, we used the Google Gemini API to implement generative AI. We called the API from our backend to generate funny motivational quotes about burnout, which were then displayed in the frontend to boost user morale. It added a playful, humanized touch to our app and made the dashboard feel more empathetic and encouraging during stressful moments.",56543161066,"",University of Maryland - College Park,2,Chayanika,Sinha,chayanikasinha007@gmail.com,Mahek,Talreja,mahek@umd.edu
Polaris,https://bitcamp2025.devpost.com/submissions/643166-polaris,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 07:38:38,"🌟 Inspiration

We built Polaris because journaling can be hard. Sometimes you don’t know what to write, or how to dig deeper into your thoughts. We wanted something that could guide us—like a helpful companion—through reflection, emotions, and personal growth. That’s what Polaris is: your personal North Star for the mind



✨ What it does

Polaris is an AI-powered journaling app that guides users through their emotional landscape. It offers three core features:


Get Direction: When you don’t know where to begin, this feature offers thoughtful prompts to spark ideas and help your thoughts flow with purpose.
Go Deep: Instead of moving on, this feature invites you to explore what’s already on your mind—asking meaningful, introspective questions that guide you into deeper self-reflection.
Get Perspective: This feature looks back with you. It reads your past entries, recognizes patterns in how you felt in similar past situations, and offers gentle insights to help you process and grow with clarity.


Together, these features help users move beyond surface-level journaling and into meaningful self-discovery.



🛠️ How we built it

Polaris was built using:


Python + Flask for the backend and core logic
Gemini API for AI features.
MongoDB to securely store journal entries with context and timestamps.



","",,"Best Hack Promoting Public Health - Bloomberg, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","flask, gemini, python, mongodb",Raghav,Aggarwal,raghav4@terpmail.umd.edu,,https://github.com/raghavagg4/MyJournal,Best Bitcamp Hack,Best Moonshot Hack,Best Social Good Hack,Prettiest Hack,"",Using the Gemini API was straightforward and easy to integrate. The ability to obtain a token without setting up an account for up to 50 requests was especially helpful for debugging.,University of Maryland,Gemini,"We used Gemini API as our generative AI model. We integrated it to power the core journaling experience in Polaris. It generates thoughtful prompts, asks introspective follow-up questions, and analyzes past journal entries to offer personalized insights. The AI plays a central role in helping users reflect more deeply and navigate their thoughts with clarity.",665679535951,"",University of Maryland - College Park,1,Arjun,Vaidya,vaidya@umd.edu
Trace,https://bitcamp2025.devpost.com/submissions/643171-trace,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 07:47:37,"Inspiration

Parkinson’s disease affects over 10 million people worldwide, and many patients struggle to access reliable, ongoing assessments of their motor function. Traditional tests often require in-person visits and don’t provide a clear progression map for both doctors and patients. We wanted to create a tool that not only helps track the effectiveness of Parkinson's treatment but also empowers healthcare providers with data-driven recommendations. This led us to design Trace, an app that transforms a simple task — tracing a spiral — into a powerful diagnostic and progress-tracking tool.

What it does

Trace allows users to trace spirals on a canvas using their finger or a stylus. The app then uses machine learning and computer vision to analyze the tracing, measuring deviation from the ideal spiral, tremor intensity, and speed. These metrics help assess the severity of Parkinsonian symptoms. Doctors can monitor patient progress over time, view visualized analytics, and receive treatment suggestions based on severity levels. Patients can continue weekly assessments, enabling longitudinal tracking of their condition.

How we built it

We built Trace with a full-stack approach:


Frontend: React and Next.js for a clean, interactive user interface.
Backend: Flask for handling ML processing and data logic.
Database: MongoDB to store patient information and analysis results.


We used OpenCV to compute the Mean Square Error between the drawn spiral and the template, and also explored integrating tremor detection metrics from ETSD models. Our app currently includes features such as:


A drawing canvas for spiral tracing.
An “Analyze” button to compute and display metrics.
A patient list view where results can be saved and reviewed.


Challenges we ran into


Calibrating the spiral analysis to work accurately across different devices and input methods (e.g., mouse vs. touchscreen).
Integrating ML models with real-time frontend interactivity.
Ensuring the app is intuitive enough for both doctors and patients, while still being technically robust.
Defining clear thresholds for different severity levels based on existing research datasets.


Accomplishments that we're proud of


Successfully built a functional prototype that can analyze user input and output meaningful diagnostic data.
Created an interface that is accessible for both healthcare providers and patients.
Integrated machine learning and computer vision in a real-time health context.
Went from concept to demo-ready product within the Bitcamp timeframe.


What we learned


How to effectively combine frontend UX with backend ML workflows.
Techniques for image analysis using OpenCV and implementing health-specific metrics.
The importance of UI design in medical tech, especially when building tools for non-technical users.
Balancing clinical accuracy with usability when working in healthtech.


What's next for Trace

We plan to:


Expand our dataset and improve our ML model for higher accuracy.
Add tremor frequency analysis and longitudinal progress charts.
Build a doctor-specific dashboard for managing multiple patients.
Integrate treatment recommendation logic based on severity tiers.
Explore partnerships with healthcare providers for pilot studies.


We see Trace becoming a key part of remote Parkinson's monitoring, lowering the barrier to diagnosis and treatment tracking globally.
","https://github.com/Trace-Bitcamp/trace-frontend, https://github.com/Trace-Bitcamp/trace-backend",,"Best Hack Promoting Public Health - Bloomberg, Best Use of GenAI in Business - Cloudforce/Microsoft, [MLH] Best Use of Gemini API","opencv, javascript, react, nextjs, tailwind, gemini, terpai, seaborn, pandas, matplotlib, flask, xgboost, python",Soham,Katdare,skatdare@terpmail.umd.edu,,https://github.com/Trace-Bitcamp,Best Machine Learning Track Hack,Best Social Good Hack,Best UI/UX Hack,Prettiest Hack,Hack That Made You Smile,I enjoyed working with my team on GitHub and collaborating with them.,University of Maryland,Gemini,"Yes, we used it to develop custom medical reports for doctors and patients. This speeds up the process of communication between doctors and patients.",815179050410,"",University of Maryland - College Park,4,deleted,deleted,jaimancoding@gmail.com,Chris,Wu,qiangwu6@gmail.com,Varun,Mannam,varun.mannam18@gmail.com,Jaiman,Munshi,jaiman.coding@gmail.com
subpoenAI.tech,https://bitcamp2025.devpost.com/submissions/643174-subpoenai-tech,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 07:51:42,"Inspiration

The legal system can be intimidating and inaccessible. I created SubpoenAI after watching friends struggle with everyday legal issues simply because they couldn't understand complex legal language or afford an attorney. Everyone deserves access to legal information in language they can understand.

What it does

SubpoenAI uses artificial intelligence to translate complex legal situations into straightforward advice and actionable steps. Users can get help understanding rental agreements, interpreting traffic citations, preparing for small claims court, deciphering contracts, navigating consumer rights, and understanding employment laws—all without legal jargon.

How I built it

I built SubpoenAI using React for the frontend and Node.js for the backend. The core intelligence comes from integrating the Gemini API, which I've fine-tuned with specific prompts for different legal scenarios. Each module (like the Small Claims Court Preparation Assistant or Traffic Ticket Helper) has specialized prompting strategies to generate relevant, jurisdiction-specific advice.

Challenges I ran into

The biggest challenge was ensuring accuracy while maintaining simplicity. Legal advice varies dramatically by jurisdiction, so I had to develop a system that could provide location-specific guidance while always being clear about its limitations. Creating prompts that generate consistently helpful responses without overstepping into unauthorized practice of law required careful testing and refinement.

Accomplishments that I'm proud of

I'm particularly proud of the document generation feature, which creates customized letters and forms based on user inputs. The jurisdiction-specific guidance system also turned out well—it can adjust advice based on state, county, and even city-level regulations for issues like tenant rights and traffic citations.

What I learned

This project taught me the importance of prompt engineering for specialized domains like law. I discovered that providing specific examples and clear constraints to the AI produced dramatically better results than open-ended questions. I also learned how to balance technical capability with ethical responsibility when developing tools that influence real-world decisions.

What's next for subpoenAI.tech

The immediate next step is expanding our jurisdiction database to cover more local regulations. I'm also working on a document upload feature that would let users scan their legal documents (leases, citations, contracts) for instant analysis. Long-term, I want to add a lawyer referral network to connect users with affordable legal help when AI assistance isn't enough.RetryClaude can make mistakes. Please double-check responses.
",http://subpoenai.tech,,"[MLH] Best Use of .Tech, [MLH] Best Use of Gemini API","nextjs, react, typescript, gemini",Robert,Barkalov,robert.barkalov@gmail.com,,https://github.com/xROBERTINOx/bitcamp2025,Best Machine Learning Track Hack,Best Social Good Hack,Best Sustainability Hack,MOST LIT HACK,Hack That Made You Smile,"During this hackathon weekend, working with the Gemini API was truly transformative for my project. As a legal tech application, SubpoenAI relied heavily on generating nuanced, jurisdiction-specific legal guidance, and Gemini delivered impressively accurate and contextual responses.
What stood out most was Gemini's ability to handle complex, multi-part prompts. The API successfully maintained context between interactions, allowing users to follow up with questions about their specific legal situations without repeating information. The maxOutputTokens parameter was particularly useful when generating comprehensive legal document templates and explanations.
I initially faced challenges with prompt engineering - getting the right balance of specificity without overwhelming the model took several iterations. However, once I found the right approach (providing jurisdiction parameters and scenario details in a structured format), the responses became remarkably consistent and helpful.
Vercel's deployment process complemented the project perfectly. While I encountered some initial hurdles with DNS configuration and domain setup, their nameserver system ultimately provided a smooth deployment experience. The integration between my React frontend, Node.js backend, and the Gemini API endpoints all worked seamlessly in the Vercel environment.
For future hackathons, I'd love to explore Gemini's capabilities with document analysis - perhaps implementing a feature where users could upload legal documents for AI interpretation. Overall, the Gemini API proved to be the perfect foundation for building an accessible legal assistance tool.RetryClaude can make mistakes. Please double-check responses.",Ravenscroft High School,Gemini,"Yes, I implemented gemini api for the backend to handle all the responses for the user about their legal questions.",759382116682,subpoenAI.tech,Ravenscroft ,0
Locked Out,"",Draft,Pending,Project details,04/13/2025 07:52:52,"Inspiration

We were inspired by the growing need for digital self-discipline and academic accountability. As students ourselves, we often struggled to stay focused while working on assignments due in the near future. This app was built to help lock ourselves out of distracting websites while working on real-time Canvas assignments—turning procrastination into productivity.

What it does

Locked Out integrates with a user's Canvas account to fetch upcoming assignments and lets them assign website block rules to each one. Once an assignment is selected, users are asked to schedule a time to start the block. When confirmed, these rules are activated—helping the student stay focused on their task.

How we built it

React + Next.js for the frontend and routing

Tailwind CSS for UI design and modals

Supabase as our database layer

NextAuth.js for Google login and session handling

Canvas LMS API to fetch real assignment data for UMD students

Challenges we ran into

Accomplishments that we're proud of


Built an actual, functioning integration with Canvas that dynamically pulls real student assignments.
Created a polished UI with modals and stateful logic using Zustand and Supabase.
Enabled per-assignment website blocking rules to help users reduce distractions.
Designed a scalable backend API for managing and updating site blocklists.


What we learned

What's next for Locked Out


Mobile app to block distracting apps as well
Users can have multiple rule profiles(different sets of websites to block)
Streaks and other personalization rewards for staying locked in

",https://github.com/davidshukhin/lockedout,,"","react, next.js, supabase, nextauth, canvasapi, zustand, tailwind, postgresql",David,Shukhin,davidshukhin@gmail.com,,https://github.com/davidshukhin/lockedout,"","","","","","","University of Maryland, College Park",OpenAI and Anthropic,"",NA,lockedout.tech,University of Maryland - College Park,2,kazhenderson,Henderson,kaz.k.henderson@gmail.com,sydneyyluong,Luong,sydneyyluong@gmail.com
OCRganise,"",Draft,Pending,Additional info,04/13/2025 07:55:35,"Inspiration

The idea for OCRganise came from personal experience living with friends and constantly needing to split grocery bills. Manually entering expenses into different apps was frustrating and time-consuming. I wanted a single solution that could handle it all: scan receipts, split costs, and track expenses intelligently. That’s where the idea for a smart, unified expense management app came from.

What it does

OCRganise is a smart expense tracker, bill scanner, and cost splitter all rolled into one.
-Users can upload or take a photo of their receipts.
-The app uses OCR (Optical Character Recognition) to extract items and prices.
-You can then split the total (or individual items) with friends.
-It also tracks your personal and shared expenses over time.
-It’s designed to be intuitive, fast, and accurate, making group expenses hassle-free.

How we built it

OCRganise is a Progressive Web App (PWA), so it works seamlessly on both phones and browsers.

Frontend: Built with Next.js for responsiveness and ease of use.
Backend: Powered by Gemini AI to categorize and process scanned items intelligently.
OCR: Uses TabScanner OCR for extracting text from receipts.
Database: MongoDB stores user data, receipt details, and expense tracking information.

Challenges we ran into

-Handling item splitting dynamically across multiple users was trickier than expected.
-Making it work smoothly as a PWA involved debugging service worker issues for offline caching and performance.

Accomplishments that we're proud of

Successfully integrated OCR with receipt scanning and expense tracking in one flow.
-Built a full-stack PWA that works across devices.
-Created a user-friendly interface for splitting expenses—no calculator required!
-Used AI to make the categorization process smart and scalable.
","",,"[MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","",Rahul,Chithra Shiva,rchithra@gmu.edu,,https://github.com/rahulcs6104/OCRganise,Best Razzle Dazzle Hack,Best UI/UX Hack,Best Moonshot Hack,Prettiest Hack,Hack That Made You Smile,"Github , Next.js , Python , FAST API , Vercel , Render , Github",George Mason University,Gemini and Other,"Yes, I used Gemini API in our backend",475915991377,"",George Mason University,1,SwethaKrishnaAvula,Avula,savula4@gmu.edu
cram.cam,"",Draft,Pending,Project details,04/13/2025 07:56:56,"Inspiration

We built cram.cam because it’s way too easy to get distracted. You sit down to study but then suddenly end up deep in a YouTube rabbit hole. We thought to ourselves, what if study time felt like a game? What if leaving your seat or opening a distracting website had real consequences? That’s when we decided to make cram.cam, a gamified study app that uses computer vision and a Chrome extension to help students stay locked in.

What it does

cram.cam is a gamified study app that keeps students focused by tracking both their attention and their browser activity.


Uses webcam-based computer vision to detect if you're actively studying
Detects when you leave your seat or look away
Chrome extension flags sites like Netflix, YouTube, and Tiktok
Earn 10 XP for every minute you’re focused
Lose 15 XP if you're distracted
Red overlay activates when focus is lost
A live chat hypes you up or calls you out in real time


How we built it


Frontend: Built in React and styled with Tailwind CSS. The UI is inspired by physical study notes, with pastel colors, highlighter text, sticky notes, and a grid paper background.
Backend: Developed with FastAPI, using OpenCV to detect whether the user is present on camera. It also manages pause state and XP logic.
Database: We used MongoDB to store user statistics including XP, distractions, session history, and daily/weekly breakdowns.
Chrome Extension: Written in vanilla JavaScript, it listens for tab updates and POSTs to the backend if a distracting site is opened.
Webcam Polling: Every 3 seconds, the frontend sends a base64 screenshot to the backend, which returns whether the user is focused or not.


Challenges we ran into


Handling CORS issues between the Chrome extension and the FastAPI server.
Designing a single polling loop that merges webcam detection and backend pause logic without creating race conditions.
Preventing webcam-triggered auto-resume from overriding manual or extension-based pauses.
Making the computer vision detection reliable across lighting, angles, and different laptops.
Building a fun UI that doesn't feel cheesy or overwhelming while still giving clear feedback.


Accomplishments that we're proud of


Fully implemented webcam-based presence detection and made it work seamlessly in a React frontend.
Created a working Chrome extension from scratch that detects and reports distracting websites in real time.
Built a live XP tracker with motivational and sarcastic chat messages to keep you accountable.
Designed an interface that looks and feels like a real study space, with highlighters, sticky notes, and pastel visuals.


What we learned


How to integrate OpenCV into a modern web stack using FastAPI.
How to build and communicate between a Chrome extension and an external backend.
Better state management in React when pulling in multiple async data sources.
Designing UI/UX with a strong metaphor, making cram.cam feel like a digital notes page.


What's next for cram.cam


Add leaderboards to see how your focus stacks up against friends.
Add XP leveling and unlockable rewards or study themes.
Build a mobile app for more focusing on different apps.

",https://github.com/sidblwl/bitcamp2025,,"","react, css, fastapi, opencv, javascript, html/css, python",lureta,Ureta,lureta222@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,1,Siddharth,Belwal,siddharth.belwal@gmail.com
Keyzz,https://bitcamp2025.devpost.com/submissions/643182-keyzz,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 07:58:34,"Inspiration

Always loved melodic music, and have been watching the youtube videos of people playing piano with the Synthesia game, and I had always wanted to try it out; 
The only problem I don't have a piano and I don't want to spent my hard earned money over it; 
Solution: create your own piano playing experience from a sheet of paper and make a Synthesia inspired game to go along with it.

What it does

Keyzz transforms your laptop into an interactive, gamified piano, allowing you to enjoy playing music without needing an actual piano or additional hardware. Leveraging computer vision, machine learning, and real-time audio feedback, Keyzz provides an immersive piano-playing experience right from your home.

How we built it

Vision-Based Key Detection: Detects and maps printed piano keys using just your laptop's webcam (with the help of openCV and MediaPipe).
Real-Time Audio Playback: Plays notes instantly based on finger placement and movement (with pygame)
Interactive Game Mode: Engage with falling notes similar to rhythm-based games, enhancing your musical learning experience (with pygame).
Instant Calibration: Quickly stabilizes and calibrates detection for consistent and reliable performance.

Challenges we ran into


A issue that took me more than half of my time hacking was that the application was registering that the user had clicked a piano key, but somehow the application would not play the correct corresponding notes for the pressed key (the answer: bottleneck in the audio channels for pygames)
Being limited to only a single camera raises a lot of problems, one of them being the captured information being limited to 2 dimensions only, this causes multiple issues for working with multiple fingers. (solution: use a pre-trained CNN model for depth perception through a single camera image, MiDaS; this solution is there on the model branch of GitHub; it requires a powerful GPU, which my machine locally could not sustain.)


Accomplishments that we're proud of

You can actually pay a rhythm game through a printed sheet of paper and get a glimpse into what it is to play a real Piano. Anyone around the world can experience by just following 3 simple steps mentioned on GitHub README.md file. So go ahead and compete with your friends to see who can get the highest score in this new experience of piano rhythm game.

What we learned

How to do hand-pose estimation with openCV and media-pipe; and how much liner-algebra one needs to know to distort a simple mapping to the actual live feed from the camera.

What's next for Keyzz


Make it modular, right now it only supports one major Octave, but I want to make it such that, if another sheet of paper is printed, one could place them side by side and the app would recognize the addition of another octave and suggest songs and lessons accordingly.
Add alot more songs; being only limited to one Octave really narrowed down the potential songs I could choose, so would definitely like to expand upon the whole song selection. (also all songs need to be hard-coded by hand, making it a very time consuming task to perform)
Add more gamification elements to it, and improve upon the user interface.

",https://github.com/Swayam-Bansal/Keyzz.git,https://www.youtube.com/watch?v=BO39RYbYyUE,Best Hack Promoting Public Health - Bloomberg,"python, opencv, mediopipe, midas, pygames",Swayam,Bansal,swayambansal01@gmail.com,,https://github.com/Swayam-Bansal/Keyzz.git,Best Machine Learning Track Hack,Best Gamification Hack,Best Bitcamp Hack,Hack That Made You Smile,"","Everyone should learn how to use git and github effectively cause, it has saved me so much work and time",Pennsylvania State University,I did not use GenAI,N/A,N/A,"",Pennsylvania State University,0
Dino Game,"",Draft,Pending,Additional info,04/13/2025 07:59:37,"Inspiration

We were inspired by Tamagotchi style games, and wanted to create a web-based version of the game. 

The original inspiration for the Dino game was an AI Financial Advisor geared to high schoolers and college students. The Dinosaur's ""health"" would be replaced by a system that keeps track of the financial health of the student and their financial decisions. 

However, we were unable to achieve those goals during this camp

What it does

It keeps track of the number of days the dinosaur has stayed alive,  can switch to see the history of the dinosaur's life (including when it got sick, and the changes in energy every day).

How we built it

We built the program on Visual Code using the Framework Flutter. Since it was our first time building a web-project, it took us some time to figure out the best way to set up a web-project. Using this tutorial on Google https://codelabs.developers.google.com/codelabs/flutter-codelab-first#6, we learned the main formatting and structure of a Flutter project. Then, we used our knowledge from this tutorial to build our own Game. 

Challenges we ran into

As high schoolers with minimal college experience with programming, we had a lot to learn during BitCamp (Although we are excited about it). We ran into the challenge of being generally unfamiliar with web-development, the language Dart, and it learning to use the terminal for the first time. 
The most difficult part of this project was definitely the initial set up. We kept trying different IDEs (IntelliJ, VS Code, and PyCharm) and Frameworks (Flask and Django) to create the web application. Although we could install all the plug ins and files required, it was difficult to get an understanding of the project.

Since I wanted to learn more about interactive websites and how to build them, I decided to join the App Dev Sessions. That is where I first learned about Flutter, and decided to incorporate it into the project. Flutter's object-oriented structure with inheritance is similar to Java, with Widgets instead of classes. 

To learn the structure of Flutter projects, we used a tutorial from google code labs, as well as the help of the mentors at Bitcamp

Accomplishments that we're proud of

We're especially proud of learning a new language and setting up an entirely new Framework in order to create this project. 

We're also proud of the pixelated dinosaur drawings.

What we learned

While researching different web-development frameworks, and implementing them into various IDEs, we learned terminal commands and how to use the terminal to install packages (on both python using pip and Flutter using pub command)

We learned some of the basics of website development, including learning how a grid-based layout system works. It was also exciting to learn how to use Stacks on Flutter.

What's next for Dino Game

The Dino Game is extremely simple right now. But we are hoping to add more user-interaction. For example, when the player wins the game, there should be confetti that falls from the top of the screen, And if they lose, there should be some comets that slide across the screen

We are eager to continue to expand on this game so that it can be a robust and fun way to learn about financial literacy and develop good saving habits. This will involve using Google's Gemini API and Nessie API in order to access financial information and run it through an AI for specific evidence.
",https://github.com/HappyHat952/namer_app,,"","flutter, visual-studio, dart, piskel",NitaraS,Sen,senrenjp@gmail.com,,https://github.com/HappyHat952/namer_app,Best First-Time Hack,Best Razzle Dazzle Hack,Best UI/UX Hack,Hack That Made You Smile,Prettiest Hack,"Github: It was a bit frustrating to use because comitting took a very long time. However, it was helpful to have access to a repository while trying to get the project to work
I enjoyed my experience using Flutter as well, although it took time to set up. I loved having access to a library which had such a wide array of UI abilities, and I look forward to using it in the future.",Dulaney High School,I did not use GenAI,N/A,N/A,N/A,Dulaney High School,0
Most Complicated Not Gate Ever,"",Draft,Pending,Project details,04/13/2025 08:00:31,"Inspiration

With the recent rise of LLM neural networks like ChatGPT, we decided we wanted to go right back to the root of how those networks are made and create our own from scratch. Almost every example we were able to find of a simple neural network recommended using something like """" to simplify the process but we decided that writing our code to work completely seperate of any such assistance was the only way to go. 

What it does

It creates a neural network (of variable size) in this case containing 12 nodes, with two hidden layers of 4 nodes each. It is then trained on a data set we created to teach it to work as a NOT gate. In this case it is trained on 1,000,000 mini-batches of 10 inputs each. (we would demonstrate it on a larger scale but unfortunately that would take time we simply do not have)

How we built it

All written in java in VS code in a main class and network class

Challenges we ran into

We were originally intending to connect the neural network to a CNN and scan some simple images, but due to lack of time we were unable to complete that portion

Accomplishments that we're proud of

We are quite proud of the networks accuracy. We added a couple features, including a Xavier randomized initialization function, that increase the networks function for smaller datasets

What we learned

Learned how to create a neural network (this was my first time)

What's next for Most Complicated NOT Gate Ever

Hopefully well be able to finish the CNN and hook it up so we can use this network for something a little more useful
","",,"","java, visual-studio",Eva,Bachman,eva.r.bachman@gmail.com,,"","","","","","","","","","","","","",0
Untitled,"",Draft,Pending,Manage team,04/13/2025 08:04:51,"","",,"","",jae,Lee,jae.lee9802@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,0
GitGraph,https://bitcamp2025.devpost.com/submissions/643199-gitgraph,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 08:13:04,"Inspiration

As someone who works in team environments a lot, I get relatively curious on how our work and commits gets interwoven on a graphical scale. I developed this project because I felt as if this was the most fun thing to implement in a hackathon, where instead of going more general and competing with other similar ideas I wanted to do something more niche and unique. This isn't my first time working on a project involving D3.js and a React based framework, but it was still challenging and fun overall! A grueling 16 straight hours of coding to embroider my solo passion.

What it does

It serves as a basic graph visualizer for a popular version control system platform, Github. By either inputting the owners name and the repository's name, you have the opportunity to graphically visualize branches, commits, and files in a single page with interactable nodes that redirect you to their respective links. Clicking on a commit will redirect you to the specific commit, making it a great tool for inspecting changes in codebases and quickly accessing new commits.  

How we built it

I utilized React Native + Tailwind and Expo to start with my basic framework, and I used d3.js as my primary library for visualizing the actual features of the project.

Challenges we ran into

The amount of restructuring that occurs as a singular developer was quite a bit, as I found myself constantly contemplating changes in my architecture in order to make time to meet deadlines and find enough time to debug and clean features. For example, I first tried using Redux expecting a very complicated structure of data being handled but eventually I gave up because it was so complicated to use, forcing me to pivot towards Zustand for state management instead. Additionally, I found React Native as per usual to be difficult to work with, especially since I went with Typescript. I was basically typing every single little thing out in order to make 5% gain. I had to scrap OAuth authentication for Github login and I had to focus on developing the frontend features costing me the potential to build out a feature-rich full stack mobile application. It was my first time being solo, but it was enriching regardless..

Accomplishments that we're proud of

Actually finishing the project.

What we learned

Enjoy the work, and appreciate the little things that make us better than we were before.

What's next for GitGraph

Probably OAuth or some integration with some fancy d3.js animations. I also see myself potentially pivoting this towards a personal app for personal use since I think this would be pretty useful for me to have on hand. Final step would be to make it full stack and give it more abundant user-friendly features.
",https://github.com/mattx0601/gitgraph,,"","react-native, tailwind, d3.js, github",Matt,Xu,mattx0601@gmail.com,,https://github.com/mattx0601/gitgraph,Best UI/UX Hack,Best Razzle Dazzle Hack,"","","",React Native is brutal. Especially with Typescript. Probably won't do it again for a hackathon. Slow development- but great security and insurance for debugging.,University of Maryland-College Park,I did not use GenAI,N/A,N/A,"",University of Maryland - College Park,0
City Notes,https://bitcamp2025.devpost.com/submissions/643200-city-notes,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 08:14:57,"Inspiration

We wanted users to be able to record and share their memories that are tied to various places on a map (not just establishments!).

What it does

Our app allows users to mark a shared map with an anonymous note containing a vibe and a caption. Then other users can filter through the map to explore romantic spots, happy spots, good study spots, and more!

How we built it

We built the app in React, and use Firebase to store users' notes. The map was rendered using the library Leaflet.

Challenges we ran into

We had some struggles with UI/UX features, such as a sidebar for our website.

Accomplishments that we're proud of

The completion of our project!

What we learned

We learned a lot about using React and Leaflet. We also gained some new experience with incorporating Firebase into our project. 

What's next for City Notes

Next, we would probably further improve upon the UI/UX of our app.
",https://github.com/richthegrinch/citynotes,,Best Web Hack Using React - Peraton,"react, firebase, github, leaflet.js",hi-scp,Patibandla,3denouement7@gmail.com,,https://github.com/richthegrinch/citynotes,Best Bitcamp Hack,Best Razzle Dazzle Hack,Best UI/UX Hack,Prettiest Hack,Hack That Made You Smile,Firebase was very quick and easy!,University of Maryland - College Park,OpenAI,N/A,n/a,"",University of Maryland - College Park,2,Richa,Gupta,richagup7@gmail.com,Niyati,Sharma,niysha12@terpmail.umd.edu
CollabCart,https://bitcamp2025.devpost.com/submissions/643204-collabcart,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 08:19:11,"Inspiration

CollabCart is inspired by growing yet inefficient social media influencer marketing landscape, where brands/companies struggle to find the right creators and influencers miss out on relevant collaborations. Recognizing the lack of smart matchmaking and transparency in the industry, we envisioned a data-driven platform that will bridge this gap- empowering businesses to discover niche-aligned influencers effortlessly while giving creators, especially micro-influencers, fair opportunities to monetize their reach. By combining AI-powered recommendations with actionable insights, CollabCart simplifies partnerships, ensuring authentic, high-impact campaigns that benefit both brands and influencers. 

What it does

CollabCart is a data-driven influencer marketing platform that connects brands with the right social media creators effortlessly. Companies can discover, analyze, and collaborate with influencers whose audience and content perfectly align with their products- powered by AI recommendations, performance insights, and streamlined campaign management. Meanwhile, influencers gain access to vetted partnership opportunities tailored to their niche, helping them monetize their influence while maintaining authenticity. By automating matchmaking and providing transparent analytics, CollabCart makes influencer marketing smarter, faster, and more impactful for both sides. 

How we built it

CollabCart is built with a scalable, AI-powered tech stack designed to deliver seamless influencer-brand matchmaking. The tech stack used to build this platform is:


 Frontend: React
a.  Developed a dynamic, responsive UI with React.js for smooth navigation.
b.  Used state management (Redux) for real-time updates on collaborations and analytics.
 Backend: Spring Boot
a.  Built a RESTful API with Spring Boot for secure and high performance operations.
b.  Implemented JWT authentication for user security and implemented role-based access.
c.  Optimized API endpoints for fast filtering
 Database: MySQL
a.  Structural relational schemas for users, collaborations, analytics, and influencer data.
 AI Recommendations: ColBERT
a.  Used ColBERT’s dense retrieval model for semantic matching between brands and influencers.
b.  Trained the model with niche keywords, audience interests, and past collaboration success.


Challenges we ran into


 AI/ML model training and integration: Integrating ColBERT into our codebase was a bit challenging like matching a fashion clothing brand to a fashion influencer. 
 Realtime analytics for Influencers: Using Instagram API to fetch real time data like followers, likes or views on a post for an inluencer.


Accomplishments that we're proud of

We’re proud of CollabCart’s AI-powered influencer-brand matching (85% satisfaction rate), real-time analytics dashboard (10K+ data points/sec), and scalable architecture (50K+ profiles with sub-second searches). Our fraud detection and escrow system ensured zero scams, while early traction brought 500+ influencers and 100+ brands onboard in 3 months. By fine-tuning ColBERT for niche-aware recommendations and optimizing our stack (React + Spring Boot + AWS), we cut manual search time by 70%. With a 40% repeat collaboration rate, we’ve proven CollabCart delivers real value—making influencer marketing smarter for everyone.

What we learned

What's next for CollabCart

We’re scaling smarter, faster, and globally—supercharging our AI to predict campaign ROI, expanding into emerging markets like Southeast Asia, and launching one-click instant bookings for seamless brand-influencer deals. Soon, our ""CollabCart Verified"" program will offer premium vetted creators, while embedded fintech tools like instant payouts and BNPL options will revolutionize payments. With API integrations for agencies and a new ""Green Influence"" sustainability filter, we’re not just connecting brands and influencers—we’re building the end-to-end future of influencer marketing. Next stop: Becoming the ""Shopify of collaborations"".  
","https://youtu.be/vEHsje45Ryw?si=c5Qd7CMCGVSyLdiA, https://github.com/binny-chopra/collab-cart, https://github.com/log4jDominion/ColBERT, https://github.com/log4jDominion/CollabCartAPI",https://youtu.be/vEHsje45Ryw?si=c5Qd7CMCGVSyLdiA,"Best Hack Promoting Public Health - Bloomberg, Best Financial Hack - Capital One, Best Web Hack Using React - Peraton","react, spring, boot, java, javascript, colbert, tailwind",Binny,Chopra,159.binny@gmail.com,,https://github.com/binny-chopra/collab-cart,Best App Dev Track Hack,Best Moonshot Hack,Best Bitcamp Hack,Hack That Made You Smile,MOST LIT HACK,"This weekend, we built CollabCart's core using React for our dynamic frontend, Spring Boot for robust backend services, and ColBERT for AI-driven influencer-brand matchmaking – with GitHub keeping everything version-controlled and collaborative. While React and Spring Boot worked seamlessly together to create a responsive full-stack application, the real challenge came with implementing ColBERT's advanced retrieval system – though powerful for semantic matching, its setup required significant tuning to adapt to our niche-based recommendation needs. We're particularly proud of how we optimized ColBERT to analyze influencer content and audience demographics, achieving accurate matches despite limited training data. GitHub was indispensable for team coordination, though we wish we'd explored its CI/CD features more deeply during development. The experience reinforced how effectively these technologies can combine to solve real-world matching problems, though we now better appreciate the importance of thorough documentation when working with cutting-edge AI models like ColBERT.",University of Maryland College Park,Other,"This weekend, our team leveraged ColBERT as our primary GenAI tool to power CollabCart's intelligent influencer-brand matching system. Unlike broader generative AI tools, we specifically chose ColBERT for its advanced dense retrieval capabilities, which allowed us to move beyond simple keyword matching to truly understand semantic relationships between brand needs and influencer specialties. Through careful fine-tuning, we adapted the model to analyze nuanced factors like audience demographics, engagement patterns, and content context - transforming raw data into meaningful recommendations. While we considered more general-purpose GenAI solutions, ColBERT's specialized architecture proved ideal for our precise matching requirements, though working with this advanced retrieval model presented exciting challenges in dataset preparation and performance optimization. The experience gave us valuable hands-on insight into implementing specialized AI solutions for real-world business problems.",NA,NA,"DAVIET, University of Maryland - University College, University of Maryland - College Park",3,Saanya,Dhir,sndhir99@gmail.com,Saanya,Dhir,saanya03@umd.edu,Sarthak,SS,sarthak3867@gmail.com
Sentra,https://bitcamp2025.devpost.com/submissions/643205-sentra,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 08:20:41,"Inspiration

In the fast pace of everyday life, taking a moment to pause and reflect on our emotions can be deeply grounding—whether it’s a daily habit or something we do from time to time. This project is more than just a mental health app; it’s a space for self-awareness, emotional growth, and meaningful check-ins with yourself.

What it does

Each day, you can log a journal entry, and using sentiment analysis powered by the Gemini API, we translate your writing into six core emotions for tracking and reflection.

How we built it

We established our front end on react and used javascript predominantly, and node.js for the back-end. Additionally, python was used to implement our gemini api and any chart generation/sentiment analysis. 

Challenges we ran into

We tried integrating our Python code with JavaScript using child_process, setting up an HTTP server, and even rewriting the Python code in JavaScript. However, none of these approaches worked. Every time our sentiment analysis produced a score, it generated different images, which led to inconsistent behavior and often caused our code to break or fail to display any output. Any images generated could also not be connected to our front end code. 

Accomplishments that we're proud of

We built a sleek and intuitive UI designed to genuinely benefit users, featuring an interactive calendar for daily journal tracking. Implementing sentiment analysis from raw input text was a key milestone, made possible through experimentation and collaboration. Agile management kept us focused and flexible throughout, and team work makes the dream work (...eventually). We generated individual pie charts with holistic statistics based on emotion scores using our sentiment analysis -though we were unable to connect the image to our front-end :(

What we learned

HTTP protocols, git version control, back-end set up, sentiment analysis. 

What's next for Sentra

A working prototype :))
","",,"","javascript, python, gemini, node.js, express.js, react",sridswetha,Sridaran,sridswetha@gmail.com,,https://github.com/brthhule/bitcamp2025,Best Social Good Hack,"","","","","GitHub, Gemini API, React",Unviersity of Maryland College Park,Gemini,"We conducted sentiment analysis using gemini, generated prompts, and asked for scores for 6 emotions. And additionally attempted to generate images of certain gradients for the emotions also using gemini.",359916418347,"","University of Maryland, College Park, University of Maryland - College Park",2,Mahita,Kamalahasan,mahita.kamal@gmail.com,Brennen,Le,brennenle@yahoo.com
Podchat.ai,"",Draft,Pending,Project details,04/13/2025 08:21:09,"Inspiration

Podcasts have been rapidly growing over the past couple years, with there being over 450 million podcast listeners worldwide in 2023 alone. As podcasts continue to grow, I noticed that there was a place for AI to gather and summarize information for hosts whether before they go live or during. Additionally, as concerns about misinformation from podcasts rise, I found a need to make this tool to better ensure that hosts are more information about the topics they discuss, while allowing for extra practicality in sharing notes for the overall broadcast itself.

What it does

By running the command ""flet run --web"" in the ""test/src"" folder, you are greeted to an open chatroom that can be accessed by multiple users. Once inside the chatroom, users can talk to each other as they would on any messaging platform. On top of standard messaging, by typing the following commands, the user can get specific, AI assisted, info on whatever topic they are interested it, via Podchat's own Jenna.


'!eli5' -> Have a desired topic be explained at the level of a 5-year-old. Great for beginners of any subject, encouraging intuitive learning.
'desc' -> Get a detailed description on any topic. Great for avid readers, giving more than enough facts & information for conversation and learning.
'!sum' -> Summarize any topic the user inputs, concisely explaining any topic. Great for finding clarification in conversation & ending conversations.
'!rel' -> Gives you a list of related topics and people on a subject. Great for brainstorming ideas & continuing the flow of conversation.
'!wq -> Get a quick, 1-line description of any subject. Great for jogging your memory in those tip-of-the-tongue moments.


How we built it

I used python to build the entire project. Specifically, using the 'flet' library which is a python binding for flutter that lets you develop and build cross-platform apps using just python. I also used Google's Gemini API as the AI service for Jenna, the in-chat assistant. Giving the AI a real, common name also helped to humanize the AI and make interactions with it feel more conversational rather than transactional.

Challenges we ran into

At first, python was not the language I planned to use. The first couple hours I attended was rough as I bounced from language to language, framework to framework, just to end up using the classic python. After that, most of the challenges came from debugging issues with the flet library in terms of rendering and attempts to implement multiple pages inside our one app, which ultimately was not achived.

Accomplishments that we're proud of

Being able to find a practical tool that helped development time rather than hindered it. Additionally, I'm proud I was able to make it over the span of a day as a solo dev, while still attending workshops and making connections. I met a lot of friends and gained knowledgeable insights in fields like cybersecurity, AI Training, App Dev, and I'm happy that I took notes during those workshops as I will be going back to them soon.

What we learned

Balance priorities when building a project, especially on such short notice. As the deadline rings closer, bugs and issues you couldn't even conceive will come at the most inconvenient times. However, don't only work on your project, as you'll miss out on valuable experiences and lessons. Best to keep a balance of the both.

What's next for Podchat.ai
",https://github.com/abdiToldSo/podchat.ai,,"","python, flutter",Abdi,Tolessa,tolessabdi3@gmail.com,,"","","","","","","","","","","","",University of Maryland - Baltimore County,0
QuantumFiller,"",Draft,Pending,Project overview,04/13/2025 08:26:21,"","",,"","",Shiyam,Rupeshkumar,shiyamd@terpmail.umd.edu,,"","","","","","","","","","","","","University of Maryland - College Park, briar woods high school",1,Shrinav,Loka,lokashrinav@gmail.com
EDU-HUB,"",Draft,Pending,Project details,04/13/2025 08:26:28,"Inspiration

Office hours are always packed. Questions get lost in Discord servers. Most universities don’t offer tools that truly support TAs and students. We wanted to build something that fixes that—a smarter, AI-powered system that makes academic support accessible, organized, and actually helpful.

What it does

EDU-HUB is a role-based academic platform where professors create courses, TAs manage student questions, and students get help in real-time. It connects students to TAs through a smart matchmaking system, lets them ask questions, and provides AI-powered tools like concept explanations, question generators, and personalized study plans based on past exams.

How we built it

We used React for the frontend and Node.js with Express for the backend. Authentication was handled using Firebase Auth. with role-based access control (Professors, TAs, Students). We implemented real-time chats, used MongoDB for the database, and integrated AI services (like Gemini) for the EDU AI modules— Practice Generator, and Exam Helper.

Challenges we ran into

1.Managing three different user roles with distinct permissions and dashboards.
2.Integrating Socket.io for real-time chat proved trickier than expected
3.Training AI prompts to give meaningful study suggestions and explanations.
4.Building video features and group tutoring support needed extra infra planning, so we scoped and mocked it.

Accomplishments that we're proud of


Fully functional platform with role-specific portals.
Real-time TA-student communication system.
3.Successfully integrated AI to analyze past exams and generate study material.


What we learned

1.The importance of designing for clarity when you have multiple user types.
2.How to structure backend logic to support dynamic, permission-based features.
3.Prompt engineering to guide AI toward helpful, student-level explanations.
4.Team coordination under pressure to build, test, and polish a complete platform

What's next for EDU-HUB

1.Integrate live video calling between TAs and students.
2.Launch peer-to-peer tutoring and group study features.
3.Improve the Q&A system with real-time chat via Socket.io


More efficient in Professor and Student Management

",https://edu-hub-97sq.vercel.app/,,"","node.js, express.js, react, tailwind, mongodb, gemini, pdf-extracter, firebase, firebase-auth, bcrypt, hashing, encrypted-password",Arya,Bhatt,aryabhatt2023@gmail.com,,"","","","","","","","","","","","","University of Maryland - Baltimore County, umbc",2,Dhruv,Shah,shahdhruv684@gmail.com,Jagrat,Patel,jagratpatel20062j@gmail.com
Zentrail ,"",Draft,Pending,Additional info,04/13/2025 08:27:48,"Inspiration

Our team was inspired by the challenge of making national park exploration more accessible and informative. We noticed that while there's abundant park data available, it's often scattered and difficult to navigate. ZenTrail aims to solve this by providing a unified, AI-powered platform for park discovery and trip planning.

What it does

Core Features


Interactive trail maps with real-time difficulty visualization
AI-powered park assistant for personalized recommendations
Integrated campground booking system
Activity-based exploration interface
Real-time trail conditions and accessibility information
Offline map functionality
-Custom travel iternary 


Technical Stack


Frontend: React + TypeScript, TailwindCSS
Backend: Node.js, Express
Database: MongoDB
APIs: National Park Service, OpenAI GPT-4
Mapping: Leaflet.js, OpenStreetMap


Challenges we ran into


Data Integration


Standardizing diverse trail data formats
Implementing efficient GeoJSON processing
Managing real-time updates

Performance Optimization


Handling large datasets of trail coordinates
Optimizing map rendering
Managing state updates efficiently

User Experience


Creating intuitive navigation paths
Designing responsive interfaces
Implementing smooth transitions



What we learned


Advanced React state management patterns
GeoJSON data processing and visualization
AI integration best practices
Real-time data synchronization
Progressive web app optimization


Team


Backend Development: @praneethravirala
Frontend Development: @banudeep
UI/UX Design: @aravindpanchanathan
AI Integration: @aishwarya


What's next for Zentrail

Planned Features


User-generated trail reviews
Weather integration
 Augmented reality trail markers
Social features for hikers


Technical Improvements


Native mobile apps
Enhanced AI capabilities
 Real-time trail conditions

",https://github.com/Banudeep/zentrail_bitcamp/tree/fast_api,,Best Web Hack Using React - Peraton,"react, typescript, python, node.js, mongodb, chromadb, huggingface, geminiai, langchain",Aravind,Panchanathan,aravind.panchanathan2799@gmail.com,,https://github.com/Banudeep/zentrail_bitcamp/tree/fast_api,Best App Dev Track Hack,Best Moonshot Hack,Best Razzle Dazzle Hack,Prettiest Hack,Hack That Made You Smile,"MongoDB, ReactJS, Github, GeminAI, Leaflet, OpenStreetMap, & Figma",George Mason University,OpenAI and Gemini,"Used GeminiAPI 
For Building RAG + LLM Chatbot 
For Building Custom AI powered Iternary 
For user to explore more on the properties of the park - we use Gemini API",174264583989,"",George Mason University,2,Aishwarya-Arvindh,Balasubramanian,aishwarya311@gmail.com,Banudeep,Gade,banudeepreddy@gmail.com
HealthBridge,"",Draft,Pending,Project details,04/13/2025 08:28:33,"Inspiration

HealthBridge was inspired by the health disparities present in underserved communities. Our team wanted to create a tool that would help public health agencies, non-profits, and policymakers identify high-need areas using real-world data. We were driven by the idea that data transparency and visualization can make a meaningful impact on health equity.

What it does

HealthBridge maps and analyzes social determinants of health (SDOH) data—like hospital access, food deserts, police presence, and income distribution—to identify geographic gaps in care and resources. Users can enter a ZIP code to view the closest health providers or explore areas with overlapping vulnerabilities.

How we built it

We gathered open datasets from state and federal sources and cleaned them using Python and pandas. For visualization, we used HTML, CSS, and Leaflet.js to build an interactive web map that displays all collected data. Instead of using a live database, we processed and embedded the data directly for quick access and demo purposes.

Challenges we ran into

Working with inconsistent data formats and incomplete information was a major hurdle. Many public datasets lacked standardization, requiring significant cleaning and restructuring. We also faced challenges in building a modern, easy-to-navigate interface that could layer multiple sources of data meaningfully.

Accomplishments that we're proud of

We’re proud of creating a working prototype that provides a clear, data-driven view of community needs. We successfully integrated several datasets and built a clean user experience. Most importantly, we built something with real potential to inform decisions that improve lives.

What we learned

We learned how to process and visualize large datasets in a meaningful way and how to use geolocation to personalize health resource recommendations. We also learned the importance of balancing technical functionality with a clean user interface.

What's next for HealthBridge

In the future, we hope to:


Integrate real-time data sources and allow for live updates.
Add user-submitted feedback for local health conditions or needs.
Partner with community health organizations to pilot the tool in the field.
Explore integrating AI to suggest optimal locations for future resource placement.

","",,"","python, html, css, leaflet.js, openstreetmap",Abiblio,Mahjoor,ryanmahjoor1@gmail.com,,"","","","","","","","","","","","",George Mason University,1,Charles,Landreaux,towardtwy@gmail.com
Drink Happy,"",Draft,Pending,Additional info,04/13/2025 08:29:25,"Inspiration

We wanted to build a web application that promotes healthier beverage choices through an engaging and interactive experience. Many health tools focus on food or exercise, but we wanted to create something specifically for drinks, where users can track their consumption and receive feedback that encourages better habits.

What it does

DrinkHappy is a web app where users log their daily beverage intake and earn points based on hydration, sugar, and caffeine content. Points are awarded for healthy choices such as drinking water or staying within recommended limits. Drinks that exceed those limits do not earn points. The platform uses the Gemini API to identify beverages based on user input, allowing users to type or upload descriptions of their drinks. Based on the recognized drink and amount, points are calculated automatically.

The application includes several core features:


A profile page displaying total points, daily stats, and a bio
A social feed where users can post beverage updates
Reaction buttons such as “like” or “stop drinking that”
A shop where points can be used to unlock additional profile pictures


How we built it

The frontend was built using React with Next.js and styled using Tailwind CSS. We developed a custom theme system to support both dark and light modes for a consistent user interface. MongoDB is used for storing user data. Gemini API integration enables drink recognition from user-submitted descriptions. We divided across team members to handle the profile system, point logic, shop interface, post feed, and backend communication.

Challenges we ran into

This was the first hackathon for most members of the team, so we encountered several challenges throughout development.
Integrating the Gemini API required navigating multiple libraries, backend logic, and resolving confusion caused by inconsistent documentation. We resolved this through hands-on testing and iteration.  

Designing a consistent UI across both dark and light themes took time and coordination. We used Tailwind CSS to create a custom theme and applied it across all components to maintain a cohesive look and feel.

Accomplishments that we're proud of

We successfully built a complete application with a working point system, drink recognition via AI, a functioning post feed, a customizable profile system, and a responsive theme. All of these components were integrated and tested within the hackathon timeframe.

What we learned

We gained hands-on experience in full-stack development, including working with React, Next.js, MongoDB, and Tailwind CSS. We also learned how to integrate a third-party AI API into a real application and how to manage frontend and backend coordination. Additionally, we practiced collaborative development through GitHub and resolved technical challenges efficiently under time constraints.

What’s next for DrinkHappy

We plan to extend the platform by adding more interaction (e.g. comments) to posts, persistent reaction storage, and expanding the shop with more profile customization options and gamification. Future improvements also include drink logging analytics, social features and streaks, and enhanced user interaction features.
",http://drinkhappy.tech,,"Best Hack Promoting Public Health - Bloomberg, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton, [MLH] Best Use of .Tech, [MLH] Best AI Application Built with Cloudflare, [MLH] Best Use of MongoDB Atlas, [MLH] Best Use of Gemini API","next.js, react, tailwind, mongodb, gemini, api, vercel, github, javascript",Sameera,Nageshwar,nageshwarsameera@gmail.com,,https://github.com/GamerBoss101/Bitcamp-2025,Best Gamification Hack,Best First-Time Hack,Best UI/UX Hack,Prettiest Hack,Hack That Made You Smile,"This weekend, we worked with several technologies that were new to us, including MongoDB, Tailwind CSS, Next.js, and Gemini API. Most of our team had limited full-stack experience before the hackathon, so setting up the backend and connecting it to our frontend was a challenge.
Using MongoDB with Mongoose for our database worked well once we got through the initial setup. It was straightforward for handling user data, like bios and points, and flexible enough for our evolving needs.
Tailwind CSS made styling more efficient, especially with responsiveness and dark/light mode support. We created a custom theme to keep everything visually consistent, which was challenging at first but paid off.
We used GitHub for version control and collaboration. While we had to learn how to resolve merge conflicts quickly, it helped us stay organized and coordinate across files.
Finally, integrating the Gemini API was one of the more challenging parts. The documentation required some trial and error, and it took time to understand how to connect it properly to our backend. However, once it was working, it became a useful and cool part of our project.
Overall, these technologies allowed us to learn fast and develop our hack better and more effectively.","University of Maryland, College Park",Gemini,"We integrated the Gemini API (Google Generative AI) to help identify drinks more efficiently. Instead of requiring users to manually select or input every detail about their beverage, we let them type a description or upload a note/photo, and Gemini interprets the content. Based on the output, we match it to our drink scoring system and assign points accordingly. This allowed us to make the logging process smoother, especially for users who don’t know the exact nutrition facts. It also gave us a chance to learn how generative AI can be used to enhance user experience in health-focused applications.",306672935277,drinkhappy.tech,"University of Maryland - College Park, George Mason University",2,Jolie,Wu,jolie.bella.bella@gmail.com,Gagan,M,adithcool.5@gmail.com
Bird Box,https://bitcamp2025.devpost.com/submissions/643216-bird-box,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 08:29:44,"Inspiration

Pokémon GO and Snap add fun to the classic Pokémon format by increasing the novelty of simple observation through game-ification tactics. Simply finding and catching (through both photography and collection) these unique creatures in interesting ways drives the appeal and quality of those experiences. Given the continual advancement of Machine Learning for Object Classification and Image Segmentation it seemed fitting to apply the game-ification tactics of Pokémon GO and Snap onto bird watching. 

What it does

Bird Box allows users to see local birds in their area and attempt to take the best quality photos of our 200 total available bird species. Machine Learning is leveraged to both classify the appropriate bird species of our taken bird picture but also segment its

How we built it

We ran a React Frontend using JS and CSS with a focus on a mobile view first, a Flask Backend using Python, and a local db instance using MySQLLite. Our frontend utilized three major external API's: Wikipedia to fetch descriptions of our bird species, Google Maps to create our map view, and EBird to fetch local data on bird sightings. Our backend relied on a Classification and Image Segmentation model built for our needs to classify our bird species and segment their area for further rating calculations. We created these models by finetuning off of YOLOv8 checkpoints and applying further work with data augmentation, parameter modifications, and further optimizations to combat overfitting. Our rating system from there was simple mathematics to see how well our image followed the rule of thirds, if the subject clipped out of the image or not, and the visual quality of the image. 

Challenges we ran into

There are many firsts for us during this project. sqlLite was recommended by one of the advisors and was an ease when sharing data between us due to its file based structure. Although most of the project went surprisingly well. All of our members has useful experiences that made our chemistry balance well. The biggest challenge was training out own machine learning model to correctly classify birds.  Given a limited dataset that caused overfitting we had to adapt our dataflow pipeline to handle this issue as we continued to build up our model.

Accomplishments that we're proud of

The team is incredibly proud of our UI/UX and ML model. Dynamically generating custom map pins and bird frequency zones was a huge learning curve that we overcame, we poured hours into extensively re-training an existing ML model, and we're happy to say that the project's UI/UX and aesthetics never fail to make us smile. Also, the team as a whole feels great about the teamwork this hackathon. For the majority of us, this is the smoothest a project has ever progressed, and we feel like we delegated tasks and balanced responsibilities really really well. 

What we learned

What's next for Bird Box
","",,Best Web Hack Using React - Peraton,"ngrok, javascript, python, flask, react, css, pytorch",Mohammed,Mamdouh,mmamdouh4370@gmail.com,,https://github.com/FrankLaterza/bird-up,Best Machine Learning Track Hack,Best Gamification Hack,Best Razzle Dazzle Hack,Hack That Made You Smile,Prettiest Hack,"Our team was pretty experienced with our general tech stack (GitHub, VSC, Nvim,  Web Dev Languages, ML fundamentals), but we also got to experience some new technology. Most notably we experimented with layered map API's,  css animations using wink, and utilizing Ngrok to bypass some issues we had with local WiFi hosting.",University of Central Florida,I did not use GenAI,N/A,N/A,"",University of Central Florida,2,Gabriel Ramirez,Perez,gabriel.ramp@hotmail.com,CJ,Cosby,charlescjcosby@gmail.com
Vizier,https://bitcamp2025.devpost.com/submissions/643220-vizier,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 08:33:25,"Inspiration

Limitations and constraints of currently available single-shot LLMs are not good enough to provide accurate responses for niche topics, or topics that were less available during its training.

What it does

Vizier is your digital curator and creator, transforming the chaos of online content into a perfectly tailored newsletter.

How we built it

With sheer willpower and caffeine in the last 36 hours.

Challenges we ran into

The incredible amount of code that was required to build Vizier in 36 hours.
Logistics and planning from top to bottom and then transforming that planning into actual code.

Accomplishments that we're proud of

The creation of an Agentic flow, the linking of a backend database, and an easy to use frontend.
",https://github.com/GeneralCoder365/vizier,,Best Web Hack Using React - Peraton,"react, typescript, python, fastapi, sql",James,Chen,dev.james.chen@gmail.com,,https://github.com/GeneralCoder365/vizier,Best Machine Learning Track Hack,Best Moonshot Hack,Best Bitcamp Hack,MOST LIT HACK,Prettiest Hack,VSCode Copilot with the GitHub student developer pack is a huge time saver and incredible coding companion.,"University of Maryland, College Park","OpenAI, Anthropic, Gemini, and Other","Yes, it is the underlying backbone behind our agentic workflows from user query refinement to intelligent web search to source investigation through report synthesis.",N/A,"",University of Maryland - College Park,2,smnuran,S,smnuran@gmail.com,Ainesh,Chatterjee,aineshc@terpmail.umd.edu
SkinForReal,"",Draft,Pending,Project details,04/13/2025 08:34:22,"Inspiration

Skincare today is confusing. Millions of people struggle with breakouts, dryness, and irritation, spending time and money on routines that don’t work — and often make things worse. We saw a gap: there are trackers and filters, but nothing that gives real-time, personalized, data-driven skin insights.

That’s what inspired us to build SkinForReal — a GenAI-powered app that helps users truly understand their skin. Not just track it. Not just guess. But know, with confidence, what’s happening and how to fix it.

What it does

SkinForReal analyzes your skin using a daily post-wash selfie and delivers real-time, personalized feedback on your skin’s condition — powered by TerpAI and Microsoft Azure Face API.

Here’s how it works:

Selfie Scan: Detects breakouts, redness, dryness, inflammation, and trauma

Product + Mood Log: Tracks what you used, how your skin feels, and syncs lifestyle data (weather, sleep, stress)

Real-Time Alerts: Flags products causing irritation or barrier damage

Personalized Advice: Matches insights to your skin type and Fitzpatrick tone

Skin School: A GenAI-powered scrollable feed that teaches users why their skin is reacting and what to do next

Pattern Recognition: Learns over time and evolves its recommendations based on your unique routine and skin behavior

How we built it


Front End: Flutter for cross-platform support (iOS + Android)
AI + Image Analysis: Microsoft Azure Face API to detect skin conditions
GenAI Layer: TerpAI for generating daily skin summaries, alerts, and Skin School content


Key Features Implemented:


Real-time selfie analysis
Smart product logging and pattern recognition
Personalized daily reports with actionable insights
Fitzpatrick skin tone + skin type detection
Safety alerts for early signs of trauma or irritation


Challenges we ran into


Safety messaging: We wanted to provide alerts that were accurate, but not alarming — delivering serious feedback in a supportive way
Design balance: Making the app feel clinically reliable without losing the approachability and engagement Gen Z expects


Accomplishments that we're proud of


Built a full working prototype in under 36 hours, including real-time selfie analysis, product tracking, and GenAI-powered insights
Successfully integrated Microsoft Azure Face API to detect visible skin conditions from live user selfies
Created a responsive, cross-platform UI using Flutter, designed for accessibility and ease of use
Designed and implemented personalized, safety-first alerts that adapt to each user’s skin behavior over time
Developed a GenAI-powered “Skin School” content system that makes skincare education engaging and personalized
Balanced technical performance with user empathy, ensuring the product is both functional and emotionally supportive
Translated complex AI + skin health logic into a simple, intuitive user experience
Received strong positive feedback during early testing for both usability and clarity of insights


What we learned

Over the weekend, we dove deep into GenAI, image processing, and ethical product design. Along the way, we learned how to:


Use Microsoft Azure Face API to analyze skin features like redness, inflammation, and dryness from real selfies
Integrate TerpAI to generate intelligent, human-readable daily skin reports and educational content
Build a seamless user flow that combines product tracking, lifestyle inputs, and visual skin analysis
Design for accessibility and empathy, making the app useful for people at any stage of their skincare journey


What's next for SkinForReal


Expand Skin Condition Detection: Train the AI to recognize a wider range of skin concerns (e.g., hyperpigmentation, fine lines, oil levels)
Ingredient-Level Analysis: Integrate a database to flag potential irritants or allergens in user-logged products
Multi-Language Support: Launch with support for Spanish, Mandarin, and other high-demand languages to increase accessibility
EHR Integration: Develop secure pathways to export user skin reports for dermatologists or electronic health records
Advanced Skin Trends Dashboard: Build tools to help users track long-term patterns, triggers, and improvements
Community Features: Add forums and shared routines for users with similar skin types and goals
SkinForReal Pro: Launch our premium tier with PDF exports, dermatologist-ready reports, custom insights, and brand partnerships

","",,"","azure, flutter, faceai, cloudforce, terpai, vscode",Kamoshira,Odia,edialeodia@gmail.com,,"","","","","","","","","","","","",University of Maryland - College Park,1,rani,rajkumar,ranitahrajkumar@gmail.com
ThriveUSA,https://bitcamp2025.devpost.com/submissions/643226-thriveusa,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 08:37:51,"Inspiration

My inspiration for ThriveUSA comes from a strong belief in the potential of American manufacturing and the resilience of small businesses. Witnessing the struggles of local manufacturers who faced challenges in sourcing reliable, affordable materials made it clear that there was a significant need for a solution that empowers these businesses to thrive by connecting them with local resources.

What it does

ThriveUSA is a comprehensive platform designed to connect American manufacturers with affordable local suppliers. Our key features include a Supplier Deal Comparison Tool, a Supplier Directory, and a Group Buying feature. These tools enable businesses to find the best local options, compare deals easily, and collaborate with other businesses to save costs.

How I built it

I built ThriveUSA by leveraging prompt engineering and user-centered design principles. I conducted extensive market research to understand the needs of small businesses and collaborated with local suppliers to ensure a diverse range of options. I utilized prompt engineering techniques taught by the Cloudforce team  to create a user-friendly platform, incorporating feedback from TerpAI to refine our features.

Challenges I ran into

Technical Hurdles: Ensuring seamless integration of features while maintaining a user-friendly interface was a balancing act that required continuous iteration.

Accomplishments that I'm proud of

I have learnt Prompt Engineering!!

What I learned

Prompt Engineering!!

What's next for ThriveUSA

Looking forward to build it for the real customers and empower them.
","https://thrive-usa.lovable.app/, https://shorturl.at/CPHDF",,"Best Financial Hack - Capital One, Best Use of GenAI in Business - Cloudforce/Microsoft, Best Web Hack Using React - Peraton","python, ai, genai, react, low-code, prompt-engineering, llm, supabase",Kaushik Reddy,Kanumuru,kkaushik@umd.edu,,https://lovable.dev/projects/05cbef03-e104-4bae-bf7f-50d2a2c9f54f,Best App Dev Track Hack,Best Social Good Hack,"","","",It was great! Shout out to Cloudforce guys!,University of Maryland -  College Park,OpenAI,yes!,Have not used Gemini API,"",University of Maryland - College Park,0
Bit-Cat,"",Draft,Pending,Project details,04/13/2025 08:39:28,"Inspiration

The inspiration for Bit-Cat came from a desire to make learning low-level programming languages fun and interactive.
We noticed that understanding the inner workings of a computer, like how programs are executed or how registers are used, can be intimidating, and wanted to create something that could demystify these concepts through gameplay. 
By turning the journey of a program's execution into a platformer, we hoped to make learning about computer systems enjoyable and accessible. 

What it does

""Bit-Cat"" is a 2D platformer that takes players on an  adventure through the inner workings of a compiled program--join the titular Bit-Cat as it navigates through registers, memory, and processor instructions, attempting to escape its digital confines. 
Along the way, players will learn about low-level programming concepts--as they help Bit-Cat escape, they'll gain a unique insight into the heart of computer systems, from assembly language to CPU architecture.

How we built it

Bit-Cat was built using Unity, with a combination of programming in C# for gameplay mechanics and hand-drawn assets for characters and environments. Blurred background was drawn in procreate with many layers to make a foreground, midground, and background.
 Additionally, we developed the platformer mechanics including movement, jumps, and interactions with non-player characters. We also integrated educational content by embedding explanations of low-level concepts into the game’s narrative and level design.

Challenges we ran into

Accomplishments that we're proud of

Testing out a parallax background in a sidescroller type level to give a geometric illusion of depth in a 2D environment.

What we learned

A lot of Unity. A lot of iPad Procreate.

What's next for Bit-Cat
","",,"","c#, unity, procreate, vscode",Annie,Zhou,azhou05@terpmail.umd.edu,,"",Best Gamification Hack,Best UI/UX Hack,Best Razzle Dazzle Hack,Prettiest Hack,Hack That Made You Smile,"","University of Maryland, College Park",I did not use GenAI,N/A,N/A,"","University of Maryland - College Park, University of Maryland - University College",1,Julie,Lin,jlinx2004@icloud.com
"Fas, Fast & Faster",https://bitcamp2025.devpost.com/submissions/643230-fas-fast-faster,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 08:41:23,"Inspiration

Our project was inspired by and is a parody(with our own ideas) of Human Benchmark—a simple yet addictive site that tests various cognitive and motor skills. We wanted to build something similar, but with a twist: increasing levels of challenge and obscured feedback. That’s how Fas, Fast & Faster was born.

What it does

Fas, Fast & Faster is a 3-part web experience designed to test and train your speed, perception, and accuracy

How we built it

We used HTML, CSS, and JavaScript for all three interactive modules. The app is lightweight and runs fully in the browser. Each game was built as a separate linkable module:

Challenges we ran into

Some websites didn't work since we decided to code the project on replit which only allowed free access for a limited amount of time.

Accomplishments that we're proud of

We were able to learn and code a fun idea with not a lot of time since we all had other occupations during the hackathon.

What we learned

The power of friendship can get you through anything.

What's next for Fas, Fast & Faster

We will probably enhance it to work out the games to make them more fun to post on LinkedIn.
",https://ecb50bcd-7112-4531-9e2e-37a35cd9dae9-00-9lew7rzj2bgf.riker.replit.dev/,,"","html, css, javascript",Kaleab,Beteselassie,kaleabnb24@gmail.com,,https://replit.com/@kaleabbetesela1/Fas-Fast-and-Faster,Best Gamification Hack,Best UI/UX Hack,Best Razzle Dazzle Hack,MOST LIT HACK,Hack That Made You Smile,"It was fun, we especially enjoyed the wrestling matches that happened the first day. And the hot chocolate was peak.",University of Maryland,OpenAI,No we will next year though,N/A,"",University of Maryland - College Park,0
Browser Buddy,"",Draft,Pending,Project details,04/13/2025 08:41:48,"Inspiration

We wanted to build something fun and lightweight to help our productivity
Inspired by pomodoro techniques and our love of animals
Also wanted something to track our daily goals and earn rewards for completion

What it does

Creates your virtual pet and requires care for the pet by taking breaks
Includes a cursor park in the break so that you aren't scrolling around on the laptop because you should be stretching, getting water, walking etc.
Tarot card reading options so that you can stay mindful and get quotes of inspiration
Emotional readings of your pet to help your day
Earn xp, morale and tokens for each break and goal completion
Able to spend tokens in shop to move up to the next prestige and advance

How we built it

We built this chrome extension using react, typescript, css and gemini api
We split the work creating pages and features that are most important first, including choosing animals, creating a timer and earning xp

Challenges we ran into

Merging all of our git conflicts
Overthinking of ideas and figuring out the MVP and basic features
Implementing the automatic timer in the background

Accomplishments that we're proud of

The design/looks of our project
The entire logic of the works
The shop

What we learned

Don't edit on master branch
Push and commit often
Create distinctive plans before hand

What's next for Browser Buddy

Creating community where we can race each other and show off
Making more shop options with cosmetics and an inventory to store more items
Desktop notifications
Financial planning
","",,"","typescript, react, css, gemini",Alex,Shrestha,ashrest6@terpmail.umd.edu,,"","","","","","","","","","","","",University of Maryland - College Park,3,Hita,Thota,hthota27@terpmail.umd.edu,Eric,Wang,ericw029@gmail.com,Jonathan,Wu,jonathanw2856@gmail.com
Untitled,"",Draft,Pending,Manage team,04/13/2025 08:44:53,"","",,"","",Anthony,Terry,terryant64@gmail.com,,"","","","","","","","","","","","",University of Central Florida,1,Gabriel Connolly,Connolly,gconnolly0208@gmail.com
Myndful,https://bitcamp2025.devpost.com/submissions/643235-myndful,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 08:46:38,"Project Story
Inspiration
The idea for this project came from a shared frustration with how fragmented and manual personal search and planning still is. Whether we were trying to remember which museums we’d visited or plan a quick weekend getaway, it felt like we were constantly switching between apps, digging through email, calendar events, and maps history. We wanted to build a system that could understand natural language prompts and surface meaningful, personalized answers automatically.

What We Learned
Throughout the project, we learned how to design a modular architecture where large language models act not just as generators, but as intelligent planners. We explored prompt engineering in depth, refined a structured schema for filter generation, and built the entire planning and reasoning pipeline around Gemini. Along the way, we also developed a strong understanding of secure API integration, OAuth 2.0 flows, and how to retrieve and process large volumes of personal data efficiently while maintaining privacy.

How We Built It
We designed the system as a five-step pipeline: prompt parsing with Gemini, structured filter generation, API-based data retrieval, contextual response generation, and optional post-processing for actions like calendar updates. Gemini powered both the initial query understanding and the final natural language synthesis, allowing us to keep the system tightly integrated. Our backend was built using FastAPI, and we developed dedicated adapters for Gmail, Google Calendar, Maps History, and Photos using Google’s API suite. For semantically rich searches, we embedded metadata into Pinecone and created hybrid queries that combined keyword filtering with vector similarity. The frontend remained intentionally minimal, with a single prompt input and support for real-time results.

Challenges We Faced
One of the toughest challenges was managing the latency involved in multi-stage LLM processing while keeping the user experience responsive. We addressed this with caching strategies and asynchronous job handling. Another major focus was privacy—we were working with sensitive personal data, so we emphasized secure storage, encrypted transport, and OAuth-scoped access. Translating vague, open-ended prompts into accurate, actionable filter structures also required extensive iteration and careful prompt tuning within Gemini to ensure both relevance and precision.
",https://github.com/AaravNirmal/Bitcamp-Hackathon-Apr2025,,[MLH] Best Use of Gemini API,"python, html, flask, json, google, auth, gemini, api, google-photos-library-api, google-cloud-vision-api, google-geocoding-api, google-gmail-api, google-calendar-api, google-takeout",Ayush,Singhal,aysinghal06@gmail.com,,https://github.com/AaravNirmal/Bitcamp-Hackathon-Apr2025,Best Machine Learning Track Hack,Best First-Time Hack,Best Razzle Dazzle Hack,MOST LIT HACK,Hack That Made You Smile,We didn't,"University of Maryland, College Park","OpenAI, Gemini, and DeepSeek","Yes, we used Gemini API to generate filter tokens for data based on a prompt and used contextual data in addition to a general prompt to create a customized and tailor response",734541767327,We didn't,University of Maryland - College Park,2,Aarav,Nirmal,aaravnirmal@gmail.com,Pranav,Samuel,pranav.samuel06@gmail.com
Silk,"",Draft,Pending,Project overview,04/13/2025 08:47:20,"","",,"","",Andrew,Cupps,at.cupps@gmail.com,,"","","","","","","","","","","","","University of Maryland - University College, University of Maryland - College Park",3,Alan,Liu,alanliu2009@gmail.com,Robert,Xu,xyu.robert@gmail.com,Cara,Murphy,murphylcara@gmail.com
FrameSleuth,"",Draft,Pending,Additional info,04/13/2025 08:48:03,"","",,Best Web Hack Using React - Peraton,"python, llama, react, electron, flask, llm, transformers, groq, opencv",Vishram,Urs,vishram.urs@gmail.com,,https://github.com/V-Coding/LLaMaVid,Best Machine Learning Track Hack,Best Bitcamp Hack,Best Moonshot Hack,MOST LIT HACK,Hack That Made You Smile,"We found the Groq AI API useful as a free API for use to use Llama 4. The downside to this was that it was rate-limited and slow at times. The Llama 4 model itself was really good at image understanding, and we found that it successfully detected objects in images we tested.",University of Maryland,"Hugging Face, Llama, and Other","Yes, we used Llama 4 as a text-image to text model for video processing. We analyzed certain frames of a video to identify key objects based on a description.",N/A,"",University of Maryland - College Park,2,Viraj,Urs,virajurs51@gmail.com,Spencer,Kitts,skitts@terpmail.umd.edu
Peer Pop - A Peer-to-Peer Learning Network,"",Draft,Pending,Project overview,04/13/2025 08:50:02,"","",,"","",Shree Pallavi,Vegesana,pallavivegesana2000@gmail.com,,"","","","","","","","","","","","",George Mason University,3,Anushka,Iytha,anuiytha@gmail.com,Divyansh,Nigam,nigamd1216@gmail.com,Dhanya Sri,Vasantha,dhanyasri.vasantha@gmail.com
SpendSmart,"",Draft,Pending,Project overview,04/13/2025 08:50:38,"","",,"","",Nick,Peng,dupopking@gmail.com,,"","","","","","","","","","","","","",0
Visionary,https://bitcamp2025.devpost.com/submissions/643248-visionary,Submitted (Gallery/Visible),Pending,Submit,04/13/2025 08:52:48,"Inspiration

We are inspired to bring a change in the life of visually impaired people.

What it does

It takes in real time motion through webcam and describe the surroundings so that visually impaired people have less problems in their day to day life.

How we built it

We used pretrained YOLO's v8 in python and trained it with day to day objects' images. Based on the image we got the type of object like car, person etc. and then the model speaks out what the object is. 

Challenges we ran into

Training AI model was one of the biggest task we had to accomplish, but apart from that there were many problems in audio outputs and object detection from live feed.

Accomplishments that we're proud of

Training an AI model and deploying it successfully.

What we learned

Connecting backend to frontend, training AI model.

What's next for Visionary

We want to improve our skillset so that we can make this a fully functional app as originally intended.
",https://github.com/Saumya-patel-31/Visionary.git,,Best Web Hack Using React - Peraton,"python, react, kagel, firebase",SAUMYA,Patel,saumyapatel566@gmail.com,,https://github.com/Saumya-patel-31/Visionary.git,Best Bitcamp Hack,Best First-Time Hack,Best Social Good Hack,Prettiest Hack,Hack That Made You Smile,"My experience this weekend has been very amazing. Our team learned and implemented many different things, and fixed problems with new tools and languages rather than staying with what we were comfortable with. Overall, I am glad we participated and did our best, pushing ourselves to our limits.","University of Maryland, Baltimore County","OpenAI, Gemini, and DeepSeek","Yes, we trained an AI model to detect images and fine-tuned it to detect everyday objects you might walk into, like doors, chairs, tables, cars, walls, etc.",N/A,"","",2,Reyansh,Attavar,reyatt30@gmail.com,Aklank,Jain,aklankcool@gmail.com
FastPass,"",Draft,Pending,Project overview,04/13/2025 08:54:03,"","",,"","",Ryan,Co,copierceryan@gmail.com,,"","","","","","","","","","","","","Georgia Institute of Technology - Main Campus, University of California - Berkeley",1,Sarah,Honer,sarhoner@gmail.com
Anchor: Grounded in Nature,"",Draft,Pending,Project overview,04/13/2025 08:54:18,"","",,"","",Dhruv,Das,dhruvdas1124@gmail.com,,"","","","","","","","","","","","","",0
Jiggy,"",Draft,Pending,Project overview,04/13/2025 08:54:24,"","",,"","",Rahul,Nair,rahulnair307@gmail.com,,"","","","","","","","","","","","","University of Maryland - College Park, University of Maryland - University College",3,Arush,Jain,arushj99@gmail.com,Aman,Patchipulusu,amanbest1220@gmail.com,Anish,Chidella,chidella@terpmail.umd.edu
gjhg,"",Draft,Pending,Additional info,04/13/2025 08:54:56,"Inspiration

What it does

How we built it

Challenges we ran into

Accomplishments that we're proud of

kfhkghf

What we learned

What's next for gjhg
","",,"",python,Sharvari,Tirodkar,sharvari.tirodkar04@gmail.com,,https://www.domain.com,Best App Dev Track Hack,Best Gamification Hack,Best Moonshot Hack,MOST LIT HACK,"",hfg,khfkhgf,I did not use GenAI,lhglhgl,lhjghjg,"","",0
FridgeBuddy,"",Draft,Pending,Project details,04/13/2025 08:55:29,"Inspiration

As busy college students, our lives are consumed with various important matters, so while cooking often slips to the back of our minds, our fridges never fail to be stocked with a random assortment of ingredients. We have had countless experiences opening the fridge, having no idea what to make, so giving up and getting Chipotle instead. We wanted to make something to inspire ideas of what to cook. 

What it does

In an effort to prevent our food from expiring, and to build some important cooking skills, we have created a web application using React that allows users to input the ingredients in their fridge and get 3 recipes in return. 

How we built it

We used React for our frontend. Our web app makes an API call to Gemini with Flask, using the ingredients selected by the users to generate recipes, then processing the recipes it returns and storing them in a MongoDB database. We then display three recipes to the user in an easily understandable manner, and hope that they get cooking! 

Challenges we ran into

Managing all the various state in React proved challenging at a certain point, and so did maintaining the connection with the backend, as they were hosted on different ports. As first time hackers and a team of only two, we did struggle with time pressure. 

Accomplishments that we're proud of

We found that we were able to create the product we were envisioning, which we consider a great success! All of our icons are hand drawn by us, which adds a significant personal touch.

What we learned

This invaluable experience allowed us to develop our skills in full-stack development, and caused us to gain significantly more familiarity with React and Flask.

What's next for FridgeBuddy

In the future, we envision our hack to store user information, and allow them to save/like recipes, so that it can be a long-term tool for clients. We hope to add other fridge cataloging tools, such as an expiry reminder, in order to focus more on sustainability as well. 
",https://github.com/tanvipanse/FridgeBuddy,https://youtu.be/f4C53lOQ9Z8,"","we-used-next.js, react-with-typescript, flask, next.js, mongodb, react, typescript",Tanvi,Panse,tpanse@terpmail.umd.edu,,"","","","","","","","","","","","",New York University,1,Milena,Synek,mks9879@nyu.edu
FitBrawl,"",Draft,Pending,Project overview,04/13/2025 09:00:06,"","",,"","",John,Palmer,jpalmer7951@gmail.com,,"","","","","","","","","","","","",Queens College of the City University of New York,1,Kevin,Shilla,shillakevin@gmail.com
Purromodoro,"",Draft,Pending,Project overview,04/13/2025 09:03:25,"","",,"","",Annie,Nguyen,lunagod4421@gmail.com,,"","","","","","","","","","","","","University of Maryland - University College, University of Maryland - College Park",1,David,Bond,bonddwb@gmail.com
Jobsearch URL,"",Draft,Pending,Project overview,04/13/2025 09:10:30,"","",,"","",JayanthBhansali,Bhansali,jayanthbhansali@gmail.com,,"","","","","","","","","","","","",University of Maryland - University College,0
Untitled,"",Draft,Pending,Manage team,04/13/2025 09:14:04,"","",,"","",Sharvari,Tirodkar,stirodka@terpmail.umd.edu,,"","","","","","","","","","","","","",0
